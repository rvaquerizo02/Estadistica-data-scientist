[["index.html", "Introducción a la Estadística para Científicos de Datos con R Capítulo 1 Introducción 1.1 Objetivos 1.2 Entorno de trabajo", " Introducción a la Estadística para Científicos de Datos con R Raul Vaquerizo 2023-02-19 Capítulo 1 Introducción 1.1 Objetivos Hay muchos y mejores pero quiero mi propio ensayo de introducción a la estadística en la Ciencia de Datos, además quiero que sirva para crear historias sobre la Estadística. Se emplea un título comercial donde aparece Ciencia de datos personalmente no creo que el oficio consista en hacer ciencia con los datos, en el día a día se aplican análisis estadísticos que nos permiten describir problemas, sugerir soluciones a esos problemas y mediante diseño de experimentos pilotar o medir la eficiencia de esas soluciones. De este modo, la estadística es uno de los pilares sobre los que se fundamenta eso que se denomina Ciencia de Datos y como os se ha comentado con anterioridad hay más y mejores trabajos y por ahí vamos a comenzar, la referencias empleadas para realizar este trabajo son: Libro de Carlos Gil Introducction to Stadistical Learning R for Data Science Un Recorrido por los Métodos Cuantitativos en Ciencias Sociales a bordo de R Analizando datos, visualizando información, contando historias de Javier Alvarez Liébana . Esta serie monográfica será similar a la formación que llevo impartiendo en empresas y postgrados universitarios desde hace algunos años. Los capítulos de los que tendrá este ensayo serán: Datos Estadística descriptiva Probabilidad y distribuciones que describen un problema Estadística descriptiva bivariable Muestreo e inferencia de aquella manera Regresión lineal Que nos cuenta el parámetro de un modelo Diseño de experimentos Modelos GLM Visión completamente práctica y desde el punto de vista frecuentista que será distinta a lo que podéis encontrar en otros foros. Cada tema tendrá una sección descriptiva y una sección práctica con R. 1.2 Entorno de trabajo Este trabajo está escrito en Markdown desde RStudio, el primer paso es disponer de RStudio e instalar las librerías necesarias para crear Markdown. Los paquetes que se van a emplear a lo largo del curso serán: readxl tidyverse lubridate sqldf raster DataExplorer formattable MASS car Estas librerías de R se emplearán en casos prácticos pero no son necesarias: h2o e1071 vcd "],["datos.html", "Capítulo 2 Datos 2.1 Qué son los datos 2.2 Tipos de datos 2.3 Almacenamiento de datos 2.4 Todo es álgebra lineal 2.5 Como se disponen los datos 2.6 Tipos de Variables 2.7 El rol de las variables en los datos", " Capítulo 2 Datos 2.1 Qué son los datos Los datos están en boca de todos, la nueva revolución industrial, el nuevo petróleo, el nuevo plutonio, pero ¿qué son los datos? Si nos dirigimos al diccionario de la Real Academia tenemos: Información sobre algo concreto que permite su conocimiento exacto o sirve para deducir las consecuencias derivadas de un hecho. A este problema le faltan datos numéricos. Documento, testimonio, fundamento. Inform. Información dispuesta de manera adecuada para su tratamiento por una computadora. Base de datos: Conjunto de datos organizado de tal modo que permita obtener con rapidez diversos tipos de información. Parece que la tercera acepción es la correcta pero los datos no son información, nos van a permitir obtener información, ésta se obtiene mediante un análisis estadístico. Disponer datos de forma organizada no es información. Es la estadística a través de un análisis descriptivo, a través de un modelo o a través de un algoritmo la que obtiene la información. Un científico de datos es un gestor, coge datos y los transforma en información a través de análisis estadísticos más o menos complejos. 2.2 Tipos de datos El científico de datos accederá a datos informatizados y, atendiendo a su origen, una posible clasificación sería: 2.2.1 Datos internos Son los datos que recoge la propia organización fruto de su actividad, ejemplos en esta línea son: Información sociodemográfica Información sobre la operativa del cliente Contratos activos (y no activos) Contactos con el cliente Encuestas Datos de la red comercial  La lista es larga y depende del tipo de actividad realizada. En este ámbito se está mejorando con la inclusión de sistemas informacionales y la puesta en valor del uso de los datos, en este sentido es necesario señalar la importancia que toman en las organizaciones los equipos humanos de Tecnologías de la Información. Los datos internos pueden estar organizados o no pero son propiedad de la organización que pretende explotarlos. 2.2.2 Datos externos: Datos libres o generados por empresas especializadas y que pueden mejorar los análisis estadísticos. Ejemplos: Datos GIS de un instituto geográfico Análisis de competencia (promociones, puntos de venta, ) Estadísticas del INE, Registro de la Propiedad o BOE. Cualquier estadística que publique un estado o una organización de estados Bureaux de crédito BBDD externas  Este tipo de datos está tomando más peso y aparecen expresiones como escrapear (del inglés scrapping) nos permite obtener datos de sitios web, un ejemplo de fuente de información son las RRSS. Por ejemplo, ¿tendría el mismo precio de un seguro de motos aquel cliente que pone en Instagram fotos de paisajes en sus rutas en moto o aquel cliente que publica acrobacias en su historia? 2.3 Almacenamiento de datos Es un tema complejo pero el científico de datos debe conocer los entornos que suministran datos y de ese modo hacer un mejor aprovechamiento de ellos. Una visión sintetizada y simplista sería. 2.3.1 Sistemas operacionales Son los sistemas con los que se gestiona una organización y con los que trabajan aquellos que, en muchas ocasiones, no les preocupa lo más mínimo la ciencia de datos. Aquí existen sistemas como mainframes, AS400 o CRMs, estos sistemas tienen información exhaustiva sobre operaciones y sobre la actividad, pero esa información no está (estaba) pensada para explotarse, muchos carecen de visión cliente y están más orientados a productos y a la propia actividad empresarial. Pueden suministrar información en tiempo real pero el modo en el que se almacenan estos datos puede dificultar su explotación estadística. 2.3.2 Sistemas informacionales tradicionales Al albor de la importancia de los datos nacieron los primeros sistemas informacionales como el data warehouse, lo principal en ellos es la estructura y la distinción entre productos, contratos y clientes. Estos sistemas empiezan a tener campos, variables, pensadas para su explotación estadística y la modelización. Además, toma relevancia la tabulación de los datos con una visión histórica. Estos sistemas informacionales son caros, requieren recursos para su actualización y provienen de procesos intermedios que, en ocasiones, no recogen toda la problemática del operacional pero gracias a ellos las organizaciones han empezado a conocer la importancia de gestionar los datos, la importancia de la inteligencia de negocio, el bussines intelligence. 2.3.3 Data Lake Es el nuevo paradigma al amparo de la aparición de equipos informáticos más potentes, se accede de un modo muy rápido, permite disponer de un gran volumen sin estructura por lo que la información puede ser muy exhaustiva, es más complejo distinguir la visión cliente y esa falta de estructura hace que el acceso a estos sistemas requiera de software y hardware específico además de equipos humanos más especializados. No están pensados para el análisis estadístico tradicional pero es factible. Hay una menor integridad en la información disponible pero se puede acceder y explotar un volumen de información que antes era más complicado por lo que se abren nuevas formas de explotación estadística de datos. En este trabajo no se va a trabajar con el acceso a estos sistemas, la información vendrá tabulada directamente pero es necesario reiterar el científico de datos tiene que tener claro donde están los datos y como acceder a ellos. 2.4 Todo es álgebra lineal Esta figura es clave y aparecerá en más ocasiones para señalar la importancia de las estructuras algebraicas en el análisis y la modelización estadística. Es relevante disponer de ese conocimiento matemático porque todo el trabajo que realiza el científico de datos es algebra lineal, menos el trabajo administrativo el resto es álgebra. De hecho, la ciencia de datos ha evolucionado a la vez que los equipos informáticos han podido manejar estructuras algebraicas más complejas. La estructura más sencilla podría ser un número o una sucesión de números que forma un vector, para analizar vectores haríamos estadística univariable. Si tenemos varias variables y ponemos esos vectores en forma matricial ya estamos en disposición de ver muchas variables a la vez. Si a esas matrices le añadimos funciones que puedan ser derivadas podremos sintetizar su información mediante gradientes y por último si añadimos más dimensiones y complejidad a estas estructuras estamos ante tensores matemáticos. Visto como una secuencia: Vectores -&gt; análisis univariable como tablas de frecuencia, sumarización estadística o gráficos, al trabajo con estructuras vectoriales se le puede denominar business intelligence. Matrices -&gt; análisis multivariable como modelos lineales, componentes principales,.. cuando se trabaja con cálculo matricial estamos haciendo modelización estadística. Gradientes -&gt; análisis multivariable basado en modelos de aprendizaje automático, tenemos funciones multivariables que nos permiten derivar matrices de forma parcial para buscar máximos y mínimos locales, permiten realizar modelos de machine learning. Tensores matemáticos -&gt; aprendizaje profundo con redes neuronales que imitan el comportamiento humano, inteligencia artificial. Es evidente el paralelismo entre la evolución matemática y la evolución informática. Mayor capacidad de computación ha implicado poder manejar estructuras algebraicas más complejas y esto ha permitido una evolución en el análisis estadístico. 2.5 Como se disponen los datos Para poder llevar a cabo un análisis estadístico los datos no sólo han de estar disponibles, además deben de tener una lógica y una estructura para ser utilizados. Habitualmente, los datos estarán dispuestos en tablas donde, de forma horizontal, tenemos registros y de forma vertical tenemos columnas. Existen diversas dialécticas en ciencia de datos para hacer referencia a esas columnas y a esos registros, en general dependerá del software que empleemos en nuestro trabajo. En este ensayo se va a emplear R y la librería tidyverse de este modo la estructura de datos principal será el data frame. library(tidyverse) data(&quot;iris&quot;) str(iris) ## &#39;data.frame&#39;: 150 obs. of 5 variables: ## $ Sepal.Length: num 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... ## $ Sepal.Width : num 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ... ## $ Petal.Length: num 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... ## $ Petal.Width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... ## $ Species : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... head(iris, 5) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa La función str nos permite describir las características de un objeto de R, en este caso se trata de un data frame donde cada fila será un registro, una observación, que podrá ser un individuo, un contrato, una compañía, una especie de flor, un pingüino, depende de la población que esté en estudio. De esa población se pueden extraer subconjuntos de registros que llamaremos muestras que deberían ser representativos de la población y permitirían, con menos medios, estudiar los problemas que deseamos analizar en la población. Esta muestra permitiría inferir aspectos de interés en la población. Cada columna del data frame será denominada variable que define una característica de la observación, esta característica puede ser cualitativa (factor) o cuantitativa (número). El estudio de esas características será el que nos lleve a describir o estimar el problema al que nos enfrentamos. 2.6 Tipos de Variables Cuando se afrontan proyectos de ciencia de datos la mejor solución es dividir las variables en cualidades o cantidades, con sus matices. 2.6.1 Variables cuantitativas Son numéricas y pueden tener un número muy grande (en algunos casos infinito) de valores y describen una cantidad numéricamente. En sucesivos capítulos se verá que estas variables serán descritas con estadísticos descriptivos, en análisis visuales se emplearán histogramas, gráficos de densidades o gráficos de cajas (boxplot). Los problemas que plantean estas variables son: Aparición de valores modales que desvirtúan la forma o la distribución de los valores como por ejemplo el valor 0 en una variable que recoja saldo en fondos de inversión, ¿eliminamos una variable porque el 80% de las veces toma un valor 0? Valores atípicos o outlier también pueden desvirtuar los análisis estadísticos. Si una observación tiene una variable numérica que toma un valor anormalmente alto o bajo, ¿debemos incluirla en el análisis? Valores perdidos o valores missing en las variables cuantitativas supone un problema, debemos tener clara nuestra estrategia con los valores perdidos antes de analizar la información que nos suministran los datos. ¿Qué hacemos con esas observaciones que no tienen datos para determinadas variables? ¿Tiene sentido que no tengan datos? 2.6.2 Variables cualitativas. Factores Toman un número finito (en ocasiones muy extenso) de valores y describen una cualidad. En el conjunto de datos pueden ser tanto numéricas como cadenas de caracteres, da igual el formato pero es necesario saber que representa una cualidad. Esta cualidad puede estar ordenada como puede ser un ranking bueno/medio/malo o simplemente describir un valor como es el sexo Femenino/Masculino. Siguiendo con la dialéctica de R a estas variables las vamos a denominar factores y a cada valor que pueda tomar el factor le denominamos nivel. Para describir estas variables emplearemos tablas de frecuencia donde se pueden contar el total de observaciones para cada nivel del factor o bien relativizar esas cantidades y calcular frecuencias como porcentajes sobre el total de observaciones. Gráficamente se sugiere usar gráficos de barras o de tarta que se verán posteriormente. Los problemas que presentan son análogos a los vistos con las variables cuantitativas: Valores modales, si un factor presenta un nivel con un gran número de observaciones, ¿merece esfuerzo incluir ese factor? Valores missing, si el valor no está disponible ¿podemos crear otro nivel que sea N/D? Gran número de niveles de un factor. En ocasiones tendremos factores con miles de niveles como un código postal, es evidente que tenemos que agrupar los niveles de un factor, ¿qué estrategias de agrupamiento podemos seguir? 2.7 El rol de las variables en los datos Una vez es conocido el como se estructuran los datos y como se puede aproximar a ellos el científico de datos es necesario remarcar que las variables no tienen todas el mismo rol, en un conjunto de datos cada variable juega un papel distinto. Hay variables que identifican registros, hay fechas, medidas, campos calculados, factores reclasificados Se tiene claro que todas estas variables siempre serán factores o variables cuantitativas, pueden almacenarse como una cadena de caracteres o como un número, pero dentro del conjunto de datos hay que tener muy claro que función desempeñan. Una clasificación global del rol de las variables en los datos es: Variable Target: Es la variable más importante porque describe el objetivo sobre el que realizamos el análisis. Es la variable encargada de dar respuesta al problema que planteamos a los datos. Variable input: Esta variable servirá para explicar el comportamiento de nuestra variable target. ID: las observaciones suelen ir identificados por un campo del conjunto de datos. Puede ser un DNI, un número de contrato, una codificación administrativa, Raw (variable en bruto): serán las variables que nos ayudarán a crear cualquiera de los tipos anteriores. Por ejemplo, una fecha en sí misma no debe ser un dato de entrada en un modelo de clasificación, ha de ser transformada para crear una variable objetivo o bien una variable input que nos ayude a predecir o explicar nuestro el comportamiento de la variable target. En determinados análisis el científico de datos no dispone de esa variable target que dirige el análisis. La disponibilidad de la variable target también define el tipo de análisis estadístico a plantear. Durante todo este ensayo se van a emplear ejemplos con variable target definida, todos los análisis estadísticos serán dirigidos. "],["manejo-de-datos-con-r.html", "Capítulo 3 Manejo de datos con R 3.1 Universo tidyverse 3.2 Selección de registros 3.3 Selección de columnas 3.4 Creación o actualización de variables 3.5 Ordenar datos 3.6 Sumarizar valores 3.7 Combinación de acciones", " Capítulo 3 Manejo de datos con R 3.1 Universo tidyverse En el capítulo anterior se iniciaba el método dialéctico a utilizar en el resto del ensayo fundamentado en la idea de transformar datos en información mediante análisis estadístico con el software R. Los datos a explotar tienen una estructura tabular que llamamos data frame. Esta estructura contiene observaciones y variables sobre las que se fundamenta la labor del científico de datos ya que son ellas las que plantean y resuelven problemas. Para trabajar con data frames en R disponemos de un entorno conocido como tidyverse que engloba un gran número de librerías de R. Nota: En la instalación de paquetes R emplea install.packages(\"librería\") y sólo será necesario ejecutarlo una vez. Para iniciarse en el manejo de datos se destaca el uso de la librería dplyr que se encuentra dentro de este universo tidyverse y será sobre la que verse este capítulo. Como norma general cuando se trabaje con data frames y dplyr manipulando datos se realizarán acciones separadas con la expresión clave %&gt;% pipe de modo: df_final &lt;- df_inicial %&gt;% ACCION1 %&gt;% ACCION2 %&gt;%  %&gt;% ACCION_N Cada acción tiene su correspondiente verbo: Selección de columnas -&gt; select Selección de registros -&gt; filter Creación/modificación de variables -&gt; mutate Sumarizar por variables valores -&gt; group_by %&gt;% summarize Ordenar dataframes -&gt; arrange Estos 5 elementos deben servir al científico de datos para realizar el 80% del manejo de datos habitual con R. Por ese motivo se va a ilustrar su uso mediante ejemplos: # Recordar que sólo es necesario instalar los paquetes una vez # install.packages(&#39;tidyverse&#39;) library(tidyverse) # Se cargan los datos de ejemplo data(iris) head(iris, 5) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa library(tidyverse) permite disponer de las librerías del universo tidyverse, los ejemplos se van a realziar con el conjunto de datos iris al que se accede con data, head nos permite ver los 5 primeros registros del data frame. 3.2 Selección de registros Al seleccionar registros se está seleccionando una muestra de observaciones en base a un criterio. El verbo de dplyra emplear será filter. setosa &lt;- iris %&gt;% filter(Species==&quot;setosa&quot;) head(setosa, 5) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa Puede ser útil emplear una lista para una condición. En ese caso se ha de realizar. species_to_select = c(&quot;setosa&quot;,&quot;virginica&quot;) species &lt;- iris %&gt;% filter(Species %in% species_to_select) table(species$Species) ## ## setosa versicolor virginica ## 50 0 50 La función table realiza tablas de frecuencia (cuenta registros), en este caso se tienen los tres posibles valores que toma la variable Species con el número de registros que tiene cada elemento de la variable, el número de registros que tiene cada nivel del factor, hay un factor que no tiene registros, no desaparece el nivel, simplemente aparece con 0 observaciones. Nota: Cuando se trabajen con datos es recomendable emplear sumarizaciones, visualizaciones o salidas de tablas para contrastar que todo el trabajo que se está realizando sea correcto. Esas acciones evitarán posteriores errores. 3.3 Selección de columnas Para esta tarea se emplea el verbo select que aparece en otros paquetes de R, cuando una función se emplea por más de un paquete se recomienda emplear la estructura librería::función en este caso dplyr::select y de ese modo se indica a R que la función select es específicamente de la librería dplyr. Hecho el apunte se presentan los ejemplos. # Librería para mejorar las visualizaciones de tablas library(DT) two.columns &lt;- iris %&gt;% select(Sepal.Length,Sepal.Width) datatable(two.columns) En el ejemplo anterior se introduce el uso de la librería DT para mejorar la visualización del data frame resultante de la selección. Del mismo modo que ocurría con las observaciones es posible realizar una selección mediante lista de variables: columns = c(&#39;Sepal.Length&#39;,&#39;Sepal.Width&#39;) two.columns &lt;- iris %&gt;% dplyr::select(columns) 3.4 Creación o actualización de variables El verbo para generar nuevas variables es mutate. iris2 &lt;- iris %&gt;% mutate(Sepal.Length.6 = ifelse(Sepal.Length &gt;=6, &#39;GE 6&#39;, &#39;LT 6&#39;)) %&gt;% mutate(Sepal.Length.rela = Sepal.Length/mean(Sepal.Length)) Se crea una variable cualitativa mediante una condición con la función ifelse, también se ilustra el ejemplo con la creación de una variable numérica que es la operación matemática de dividir Sepal.Lentgh por su propia media. Muy habitual en el trabajo diario de un científico de datos a la hora de crear variables es la necesidad de anidar condiciones, para ello se puede emplear la función case_when. Un ejemplo de uso es. iris2 &lt;- iris %&gt;% mutate(Sepal.Length.agrupado = case_when( Sepal.Length &lt;= 5 ~ &#39;1. Menor de 5&#39;, Sepal.Length &lt;= 6 ~ &#39;2. Entre 5 y 6&#39;, TRUE ~ &#39;3. Mayor de 6&#39;)) table(iris2$Sepal.Length.agrupado) ## ## 1. Menor de 5 2. Entre 5 y 6 3. Mayor de 6 ## 32 57 61 Las condiciones son excluyentes en función del orden y se recomienda especificar la condición final y si no mediante TRUE, facilita el control de errores. A lo largo de todo el ensayo será una función que aparezca de forma recurrente. 3.5 Ordenar datos Las ordenaciones requieren siempre de arrange. Ordenación ascendente: order1 &lt;- iris %&gt;% arrange(Sepal.Length) Ordenación descendente: order2 &lt;- iris %&gt;% arrange(desc(Sepal.Length)) Separando por , es posible poner más de una variable en la ordenación. 3.6 Sumarizar valores En este caso se emplea una combinación de acciones, primero group_by indica el campo por el que se desea agrupar, el campo a sumarizar. Definido el campo, mediante summarise se indica la operación matemática y la variable sobre la que se realiza. iris %&gt;% group_by(Species) %&gt;% summarize(mean.Sepal.Length = mean(Sepal.Length), sd.Sepal.Length = sd(Sepal.Length), rows = n()) ## # A tibble: 3 x 4 ## Species mean.Sepal.Length sd.Sepal.Length rows ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 setosa 5.01 0.352 50 ## 2 versicolor 5.94 0.516 50 ## 3 virginica 6.59 0.636 50 Las posibles agregaciones a realizar con summarise son amplias, se disponen de funciones de agrupación, rango o dispersión. 3.7 Combinación de acciones Se realizan combinaciones de acciones con dplyr por lo que es posible que, separando por el pipe %&gt;%, podamos combinar distintas acciones. iris %&gt;% group_by(Species) %&gt;% summarize(mean.Sepal.Length = mean(Sepal.Length), sd.Sepal.Length = sd(Sepal.Length), rows = n()) %&gt;% filter(mean.Sepal.Length&gt;=5.7) %&gt;% arrange(desc(mean.Sepal.Length)) ## # A tibble: 2 x 4 ## Species mean.Sepal.Length sd.Sepal.Length rows ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 virginica 6.59 0.636 50 ## 2 versicolor 5.94 0.516 50 Siempre se respeta el orden de ejecución por lo que variables creadas o acciones utilizadas en un paso anterior pueden ser empleadas en un paso posterior. Nota: desde la versión 4.1.0 es posible emplear como pipe |&gt;. El autor de este libro tiene configurado su equipo con el anterior y se siente más cómodo con él, pero cualquiera de las opciones es válida. "],["uniones-de-tablas-con-r.html", "Capítulo 4 Uniones de tablas con R 4.1 Uniones verticales 4.2 Uniones horizontales o join 4.3 Duplicidades en las uniones de tablas 4.4 Referencias", " Capítulo 4 Uniones de tablas con R Además de manejar los datos de un data frame en ocasiones es necesario realizar uniones entre conjuntos de datos para crear o añadir nuevas variables a un data frame que es una base de observaciones inicial. Se pueden establecer 2 tipos de uniones fundamentales, uniones verticales de tablas y uniones horizontales. Las uniones verticales serán las concatenaciones de data frames, poner una estructura de datos encima de otra y las uniones horizontales serán las que se denominarán join. Se emplea una estructura de datos sencilla para ejemplificar el funcionamiento. library(kableExtra) library(tidyverse) df1 &lt;- data.frame(anio = c(2018, 2019, 2020, 2021), variable1=c(10, 20, 30, 40), variable2=c(1000,2000,3000,4000)) df2 &lt;- data.frame(anio = c(2017, 2018, 2019, 2020), variable1=c(50, 60, 70, 80), variable3=c(5000,6000,7000,8000)) df1 %&gt;% kable() anio variable1 variable2 2018 10 1000 2019 20 2000 2020 30 3000 2021 40 4000 df2 %&gt;% kable() anio variable1 variable3 2017 50 5000 2018 60 6000 2019 70 7000 2020 80 8000 Se puede observar como se han creado manualmente 2 data frames con lo que trabajaremos y el uso de tidyverse y kable para la visualización de tablas en R. Veamos los principales tipos de uniones. 4.1 Uniones verticales El siguiente código emplea la función rbind.data.frame para concatenar datos, para poner una tabla encima de otra y generaría un error: df &lt;- rbind.data.frame(df1, df2) Error in match.names(clabs, names(xi)) : names do not match previous names Significa que ambos conjuntos de datos no tienen las mismas variables. En las uniones verticales con la función rbind.data.frame se han de unir las mismas estructuras. En los datos de trabajo no se dispone de la misma estructura por lo que se torna necesario saber que deseamos unir verticalmente, saber que deseamos concatenar. Si deseamos realizar una unión de todos los datos ambas tablas requieren de las mismas variables: df1$variable3 &lt;- NA df2$variable2 &lt;- NA df &lt;- rbind.data.frame(df1, df2) df %&gt;% kable() anio variable1 variable2 variable3 2018 10 1000 NA 2019 20 2000 NA 2020 30 3000 NA 2021 40 4000 NA 2017 50 NA 5000 2018 60 NA 6000 2019 70 NA 7000 2020 80 NA 8000 Se han creado las variables 3 y 2 donde ha sido necesario y ya se está en disposición de concatenar ambos data frames. Observemos como queda el data frame resultante. Es importante puntualizar que se están produciendo duplicidades por la variable anio, cabe preguntarse ¿son necesarias esas duplicidades? Cuando se trabaje con datos es muy importante disponer de un campo identificativo del registro y determinar si existen duplicidades por ese campo. En cualquier caso, con el paquete dplyr se pueden concatenar data frames mediante la función bind_rows. df &lt;- bind_rows(df1, df2) df %&gt;% kable() anio variable1 variable2 variable3 2018 10 1000 NA 2019 20 2000 NA 2020 30 3000 NA 2021 40 4000 NA 2017 50 NA 5000 2018 60 NA 6000 2019 70 NA 7000 2020 80 NA 8000 El empleo de esta función no es sensible a la necesidad de que ambos conjuntos de datos tengan los mismos nombres de las variables, si eso no ocurre se emplean valores perdidos representados en R como NA para aquellas ocasiones en las que no coincida. 4.2 Uniones horizontales o join Esta conocida figura recoge en SQL todos los tipos de join: No se considera ver todos los ejemplos, se estudiarán las uniones más habituales en el trabajo diario. 4.2.1 Inner join Es la intersección de dos conjuntos de datos. Usamos la función inner_join de dplyr. df1 &lt;- data.frame(anio = c(2018, 2019, 2020, 2021), variable1=c(10, 20, 30, 40), variable2=c(1000,2000,3000,4000)) df2 &lt;- data.frame(anio = c(2017, 2018, 2019, 2020), variable1=c(50, 60, 70, 80), variable3=c(5000,6000,7000,8000)) df &lt;- inner_join(df1,df2, by=&#39;anio&#39;) # Equivale a df &lt;- df1 %&gt;% inner_join(df2, by=&#39;anio&#39;) df %&gt;% kable() anio variable1.x variable2 variable1.y variable3 2018 10 1000 60 6000 2019 20 2000 70 7000 2020 30 3000 80 8000 La unión de ambas estructuras tiene una variable variable1 en común, dplyr entiende que es necesario preservar las variables del conjunto de datos de la derecha, con el sufijo .x, y las variables del conjunto de datos de la izquierda, con el sufijo .y por este motivo es muy relevante determinar que se quiere unir. En los datos de trabajo podríamos saber cuales de los datos de la izquierda coinciden por año con los de la derecha y unir la variable 3. df2 &lt;- df2 %&gt;% select(-variable1) df &lt;- inner_join(df1,df2) ## Joining, by = &quot;anio&quot; df %&gt;% kable() anio variable1 variable2 variable3 2018 10 1000 6000 2019 20 2000 7000 2020 30 3000 8000 Se ha eliminado la variable1 del df2 como paso previo, es la que ambos conjuntos de datos tienen en común, se realiza la unión y en este caso se ha obviado el campo de unión porque dplyr busca la unión natural, el campo en común que es anio en este caso y no es necesario especificar by= con lo que podemos ahorrar código. En el trabajo diario del científico de datos es necesario realizar múltiples uniones de conjuntos de datos por un camo identificativo (roles de las variables), es buena práctica que este campo identificativo tenga el mismo nombre para todos los conjuntos de datos de trabajo. 4.2.2 Left join Quizá una de las uniones más habituales en el trabajo diario de un científico de datos. Se parte de un conjunto de datos de base y se le añaden nuevas variables por la derecha respetando las observaciones de la izquierda. La función de dplyr usada es left_join. df1 &lt;- data.frame(anio = c(2018, 2019, 2020, 2021), variable1=c(10, 20, 30, 40)) df2 &lt;- data.frame(anio = c(2017, 2018, 2019, 2020), variable3=c(5000,6000,7000,8000)) df1 &lt;- df1 %&gt;% left_join(df2) ## Joining, by = &quot;anio&quot; df1 %&gt;% kable() anio variable1 variable3 2018 10 6000 2019 20 7000 2020 30 8000 2021 40 NA Se ha añadido por la derecha la variable3 al df1, añadimos una nueva variable a un data frame de base. 4.2.3 Anti join Vamos se van a seleccionar aquellos registros de una tabla base que no están en otra tabla de cruce. df1 &lt;- data.frame(anio = c(2018, 2019, 2020, 2021), variable1=c(10, 20, 30, 40)) df2 &lt;- data.frame(anio = c(2017, 2018, 2019, 2020), variable3=c(5000,6000,7000,8000)) df &lt;- df1 %&gt;% anti_join(df2) ## Joining, by = &quot;anio&quot; df %&gt;% kable() anio variable1 2021 40 Se observa que no se ha unido ninguna variable, solo se ha seleccionado el registro de df1 que no cruza con df2. 4.2.4 Librería sqldf Como científicos de datos es importante saber SQL como lenguaje de consulta, si sabemos SQL tenemos la librería sqldf para utilizar directamente SQL sobre data frames de R. library(sqldf) df1 &lt;- data.frame(anio = c(2018, 2019, 2020, 2021), variable1=c(10, 20, 30, 40), variable2=c(1000,2000,3000,4000)) df2 &lt;- data.frame(anio = c(2017, 2018, 2019, 2020), variable1=c(50, 60, 70, 80), variable3=c(5000,6000,7000,8000)) # Inner Join df &lt;- sqldf(&quot;select a.anio, a.variable1, variable3 from df1 a, df2 b where a.anio = b.anio&quot;) df %&gt;% kable() anio variable1 variable3 2018 10 6000 2019 20 7000 2020 30 8000 # Left join df &lt;- sqldf(&quot;select a.anio, a.variable1, variable3 from df1 a left join df2 b on a.anio = b.anio&quot;) df %&gt;% kable() anio variable1 variable3 2018 10 6000 2019 20 7000 2020 30 8000 2021 40 NA # Anti Join df &lt;- sqldf(&quot;select * from df1 where anio not in (select anio from df2)&quot;) df %&gt;% kable() anio variable1 variable2 2021 40 4000 4.3 Duplicidades en las uniones de tablas Otra situación habitual que se va a encontrar el científico de datos es la aparición de registros duplicados por el campo identificador (ID), es necesario controlar su existencia porque pueden distorsionar el resultado de un análisis. df1 &lt;- data.frame(anio = c(2018, 2019, 2020, 2021), variable1=c(10, 20, 30, 40)) df2 &lt;- data.frame(anio = c(2017, 2018, 2019, 2020, 2020), variable3=c(5000,6000,7000,8000, 1000)) df &lt;- df1 %&gt;% left_join(df2) ## Joining, by = &quot;anio&quot; df %&gt;% kable() anio variable1 variable3 2018 10 6000 2019 20 7000 2020 30 8000 2020 30 1000 2021 40 NA En este burdo ejemplo df2 tiene duplicado el año 2020 por lo que una left join con ese conjunto de datos por ese campo provocará duplicidades. Una forma de controlarlo será contabilizar por el campo identificativo. df %&gt;% group_by(anio) %&gt;% summarise(registros=n()) %&gt;% filter(registros&gt;1) %&gt;% kable() anio registros 2020 2 En el capítulo anterior ya se anotó la importancia de establecer mecanismos de control cuando se trabajen con datos, bien sea visualizaciones de datos agrupaciones, tablas de frecuencia o estadísticos básicos que veremos en posteriores capítulos. 4.4 Referencias Javier Alvarez Liébana "],["basicos-ggplot.html", "Capítulo 5 Representación gráfica básica con ggplot 5.1 Histogramas 5.2 Gráficos de densidad 5.3 Gráficos de caja (boxplot) 5.4 Gráficos de tarta 5.5 Gráficos de barras 5.6 Gráficos de líneas 5.7 Gráfico de dispersión 5.8 Múltiples gráficos con gridExtra", " Capítulo 5 Representación gráfica básica con ggplot Además del manejo de datos es necesario tener conocimientos de representación de datos. En este trabajo se va a emplear la librería de R ggplot y se trabajarán las representaciones gráficas básicas que ha de manejar un científico de datos. Se pueden explorar las múltiples posibilidades que ofrece esta librería en la web Statistical tools for high-throughput data analysis que dispone de un gran número de recursos para R entre los que destaca el uso del paquete ggplot. Iniciamos el proceso cargando la librería tidyverse donde podemos encontrar ggplot como podemos en los mensajes que nos ofrece cuando la cargamos. library(tidyverse) data(iris) Ya está disponible esta librería gráfica, se puede observar en el mensaje Attaching packages. A modo introductorio, cuando se emplea ggplot siempre se requiere: data conjunto de datos que tiene la información a representar, siempre será un data frame. aes Aesthetics es la parte del código donde pondremos lo que deseamos visualizar por ejes además de otras características como color, grupos, geom_ Geometry define el tipo de visualización que deseamos, también podremos establecer las variables a visualizar y otras opciones de visualización. La representación gráfica dependerá del tipo de variable que deseamos estudiar, recordando capítulos anteriores que hay dos grandes dos tipos, variables cuantitativas y variables cualitativas que, en la dialéctica de R, se denominan factores . Para estudiar variables cuantitativas emplearemos: Histogramas Gráficos de densidad Boxplot En variables cualitativas: Gráficas de tarta Gráficos de barra También podremos realizar visualizaciones que contengan dos variables, en ese caso se combinan tipos de variables. Gráficos de líneas Gráficos de puntos 5.1 Histogramas Es una representación gráfica para variables numéricas en forma de barras donde el alto de la barra representa la frecuencia de los valores numéricos agrupados. Nos permite visualizar la forma y la distribución de una variable numérica. ggplot(data = iris, aes(x=Sepal.Length)) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Se observa la sintaxis más básica como ggplot(data = data.frame , aes(x=var.cuantitativa)) + geom_histogram() que siempre se empleará con ggplot() y si se añaden opciones al gráfico lo haremos con +. En este caso, solo se representa una variable que estará en el eje x de menor a mayor valor y unas barras que en función de su anchura representan un tramo que divide la variable cuantitativa en tramos de igual tamaño y la altura de las barras recoge el número de observaciones que tiene cada tramo. Se puede establecer el número de tramos que por defecto está establecido en 30 como nos dice el comentario que ofrece R tras ejecutar el código. Para ello se juega con la opción bins= ggplot(data = iris, aes(x=Sepal.Length)) + geom_histogram(bins = 10) Esa anchura del tramo puede hacer que la variable tome una u otra forma, por ese motivo es recomendable ver ese histograma como una función continua, como un gráfico de densidad. 5.2 Gráficos de densidad Es una variación del histograma que permite ver la distribución de una variable mediante una línea continua. ggplot(iris, aes(x=Sepal.Length)) + geom_density() Ver la distribución de ese modo limita el problema que supone la anchura de las barras del histograma. Este gráfico representa lo mismo que el histograma pero se distribuye de una forma continua. En el eje x se representa el % de observaciones de forma que el la función que representa el gráfico de densidad deja el 100% de los datos bajo ella. El valor de lo que queda debajo de la función es 1. 5.3 Gráficos de caja (boxplot) Gráfico que recoge información relevante sobre la distribución, las medidas de dispersión y centralidad de una variable numérica. ggplot(iris,aes(y=Sepal.Length)) + geom_boxplot() Este análisis permite recoger mucha información sobre la que volveremos posteriormente. Es habitual categorizar este tipo de gráficos para estudiar visualmente diferencias en la distribución: ggplot(iris,aes(x=Species, y=Sepal.Length)) + geom_boxplot() En este caso, al representar dos variables en aes se especifican tanto la x como la y que crea grupos, en realidad este gráfico no deja de ser la combinación de dos variables. 5.4 Gráficos de tarta Gráficos empleados para mostrar proporciones en variables categóricas, el circulo representa el total, el 100% y cada porción una parte de ese 100%. resumen &lt;- iris %&gt;% group_by(Species) %&gt;% summarise(conteo=n()) ggplot(resumen, aes(x=&#39;&#39;, y=conteo, fill=Species)) + geom_bar(stat=&quot;identity&quot;, width=1) + coord_polar(&quot;y&quot;, start=0) Una práctica recomendable a la hora de realizar gráficos con factores y ggplot, la creación de un data frame previo con la agrupación por el factor en análisis y el cálculo de la variable a representar, en este caso se desea ver la proporción de cada especie en los datos. No hay variable x, la medida es esa sumarización que se ha llamado conteo y cada grupo será una especie. Es un factor, cuando representemos factores en el 80% de las ocasiones aparecerá geom_bar y si ya hemos creado la variable a representar, como es el caso porque hemos hecho una sumarización previa entonces se usará stat = 'identity'. Para especificar que sea un gráfico de tarta añadiremos coord_polar 5.5 Gráficos de barras Gráfico empleado para representar frecuencias de variables categóricas (factores) en un eje se pondrá el factor y en el otro la frecuencia total o frecuencia relativa del factor ggplot(iris, aes(x=Species, y=Sepal.Width)) + geom_bar(stat=&quot;identity&quot;) También es habitual la combinación de 2 factores en este tipo de gráfico. Creamos un nuevo factor como resultado de la comparación de la longitud del sépalo. iris &lt;- iris %&gt;% mutate(Sepal.Length.factor = case_when( Sepal.Length&gt;=5 ~ &#39;Mayor 5 mm&#39;, TRUE ~ &#39;Menor de 5 mm&#39;)) Se emplea la función case_when para crear ese factor, cada condición tiene un valor y por último con TRUE tenemos los restantes registros. Vamos a representar 3 barras, una por especie, y dentro de cada una de ellas vemos el número de registros separada por ese factor que se ha creado con anterioridad. resumen &lt;- iris %&gt;% group_by(Species, Sepal.Length.factor) %&gt;% summarise(registros = n()) ## `summarise()` has grouped output by &#39;Species&#39;. You can override using the `.groups` argument. ggplot(resumen, aes(x=Species, y=registros, fill=Sepal.Length.factor, color=Sepal.Length.factor)) + geom_bar(stat=&quot;identity&quot;) Se puede ver como este gráfico de barras distingue el número de registros por especie y de forma apilada, uno encima de otro, tenemos distintos colores en las barras que nos identifican aquellos registros que tienen la longitud del sépalo mayor de 5 mm. Este tipo de gráfica de barras son barras apiladas. Además de crear barras apiladas se pueden crear barras agrupadas: ggplot(resumen, aes(x=Species, y=registros, fill=Sepal.Length.factor, color=Sepal.Length.factor)) + geom_bar(stat=&quot;identity&quot;, position = &quot;dodge&quot;) Para modificar el tipo de gráfico sólo tenemos que emplear la opción position = \"dodge\". 5.6 Gráficos de líneas Esta representación gráfica muestra dos variables como una secuencia de datos unidos por una línea, muy habitual en las series temporales. En los datos utilizados no se dispone de datos temporales pero se puede emplear para sumarizar la media de la anchura del sépalo por especie. resumen &lt;- iris %&gt;% group_by(Species) %&gt;% summarise(media=mean(Sepal.Width)) ggplot(resumen, aes(x=Species, y=media, group=1)) + geom_line() El eje x es la especie y en el eje y tenemos la media de la anchura del sépalo, como en casos anteriores se sugiere realizar la sumarización previa creando un conjunto de datos de resumen. Destacar la necesidad de poner la opción group = 1 ya que es necesario especificar el número de cortes, de este modo especificamos que el 100 de las observaciones están en un grupo. 5.7 Gráfico de dispersión Gráfico empleado para visualizar en 2 ejes variables numéricas, muy útil para estudiar relaciones entre variables. ggplot(iris, aes(x=Sepal.Length, y=Sepal.Width)) + geom_point() Un gráfico sobre el que se volverá de forma reiterada a lo largo del ensayo cuando se trabaje la regresión lineal. 5.8 Múltiples gráficos con gridExtra Para incluir múltiples gráficos con ggplot se dispone de la librería gridExtra, la sintaxis es sencilla ya que sólo es necesario crear un objeto con el gráfico ggplot y emplear la función grid.arrange para representar esos objetos. library(gridExtra) resumen &lt;- iris %&gt;% group_by(Species, Sepal.Length.factor) %&gt;% summarise(registros = n()) p1 = ggplot(resumen, aes(x=Species, y=registros, fill=Sepal.Length.factor, color=Sepal.Length.factor)) + geom_bar(stat=&quot;identity&quot;) p2 = ggplot(iris, aes(x=Sepal.Length, y=Sepal.Width)) + geom_point() grid.arrange(p1, p2, nrow=1) grid.arrange(p1, p2, ncol=1) "],["descripcion-numerica-variables.html", "Capítulo 6 Descripción numérica de variables 6.1 Transformar datos en información 6.2 Caso práctico. Campaña de venta cruzada 6.3 El rol de las variables en el conjunto de datos 6.4 Análisis descriptivos de los datos", " Capítulo 6 Descripción numérica de variables Se comienza con la recopilación de datos, la tabulación de los mismos y el establecimiento de la tipología y el rol que juegan éstos en el conjunto de datos. Establecido ese marco de es necesario describir datos, recordemos que por si mismos los datos no dicen nada, no resuelven nada. Esa información la suministra un análisis. 6.1 Transformar datos en información Recordando lo tratado en el capítulo 2, el álgebra lineal define el análisis estadístico, la estructura más sencilla es el vector donde aplicaría el análisis univariable, el inicio de todo. Si se dispone de más de una variable ya podemos disponer esa serie de datos en forma matricial, buscar estructuras dentro de esas matrices nos produce el análisis multivariable. Conforme ha mejorado la capacidad de computación se han podido crear sistemas estadísticos capaces de aprender de los propios datos, al conjunto de análsis basados en estos sistemas se le denomina machine learning. Actualmente, se está avanzando más, hay entornos más sofisticados capaces de trabajar con tensores matemáticos, estructuras algebraicas multidimensionales que permiten implementar algoritmos que imitan los procesos de aprendizaje humano, este conjunto de técnicas y algoritmos se recogen dentro del ámbito de la inteligencia artificial. El presente trabajo se centra en el análisis univariable y servirá de introducción al análisis multivariable. Este capítulo, para ilustrustar como realizar el análisis univariable empleará un caso práctico orientado al marketing analítico. 6.2 Caso práctico. Campaña de venta cruzada Dentro del mundo del marketing es muy habitual emplear análisis estadísticos para mejorar los resultados de las acciones comerciales. En este caso, una aseguradora española que opera en el ramo de salud desea realizar una campaña de venta cruzada a sus clientes y ofrecer producto de Autos. El ejemplo se obtiene de una competición de Kaggle, se recomienda descargar y guardar en el equipo local los datos para poder replicar el código, aunque los datos estarán subidos al repositorio. El análisis univariable nos sirve cuando tenemos cuestiones del tipo: Estudiar la calidad de la información. Descripción inicial de las variables presentes en el conjunto de datos. En el caso práctico nos permite conocer la cartera de clientes encuestados. Identificar que características de nuestros datos que pueden ser eficaces cuando tenemos que plantear análisis. Que características de nuestra cartera pueden ser relevantes para ofrecer una acción comercial. Todas estas cuestiones están vinculadas a la estadística y en primer término a la estadística sobre una sola variable. Comencemos el trabajo con el caso práctico. Del conjunto de datos de trabajo train.csv nos han pasado la siguiente información: id Unique ID for the customer Gender Gender of the customer Age Age of the customer Driving_License 0 : Customer does not have DL, 1 : Customer already has DL Region_Code Unique code for the region of the customer Previously_Insured 1 : Customer already has Vehicle Insurance, 0 : Customer doesnt have Vehicle Insurance Vehicle_Age Age of the Vehicle Vehicle_Damage 1 : Customer got his/her vehicle damaged in the past. 0 : Customer didnt get his/her vehicle damaged in the past. Annual_Premium The amount customer needs to pay as premium in the year PolicySalesChannel Anonymized Code for the channel of outreaching to the customer ie. Different Agents, Over Mail, Over Phone, In Person, etc. Vintage Number of Days, Customer has been associated with the company Response 1 : Customer is interested, 0 : Customer is not interested Se comienza el proceso de análisis. 6.3 El rol de las variables en el conjunto de datos Como se comentó en el capítulo 2 dentro de los datos cada variable tiene una función distinta y esta función define lo que se desea hacer con los datos. En este caso, se dispone de un conjunto de datos suministrado por una aseguradora para ofrecer un seguro de automóviles a sus asegurados de Salud. La variable más relevante será nuestra variable respuesta o target, por la propia definición de los datos es sencillo, ese papel lo realiza el campo Response. Para identificar cada registro, cada cliente, se dispone de un campo id el rol de esta variable será directamente el de ID. El resto de variables se consideran variables de entrada, variables input. Es práctica habitual cuando se trabaja con datos nombrar los campos de las tablas de tal forma que sea más sencillo identificar cual es el papel de cada variable en el conjunto de datos. En el caso concreto que se está estudiando recordaos, response es target e ides ID. El nombre del resto de las variables solo las define y todas ellas serán variables de entrada o variables input. Puede ser recomendable incluir en el nombre de la variable, además de una breve descripción, un prefijo que nos definiera el rol dentro del conjunto de datos. En este caso práctico se tiene un número bajo de variables, pero es posible encontrarse situaciones en las que sea necesario analizar cientos de variables y esas prácticas facilitan los análisis. Conocida la función de cada variable en el conjunto de datos se comienza a describir los elementos del conjunto de datos. 6.4 Análisis descriptivos de los datos Recuperando, de nuevo, el capítulo 2 allí se dividieron las variables en 2 tipos, variables cuantitativas y variables cualitativas que llamamos factores. En base a esta división se planteaba una posible descripción numérica y una posible descripción gráfica para variables de entrada o input, aquellas variables ID o variables en bruto raw no tiene sentido que se estudien porque no deberían aportar nada en nuestro análisis. Con estas premisas, el primer paso es determinar que tipo de variable es cada una de las que tenemos en el conjunto de datos. Se comienza el trabajo con datos: library(tidyverse) train &lt;- read.csv(&quot;./data/train.csv&quot;) head(train,5) ## id Gender Age Driving_License Region_Code Previously_Insured Vehicle_Age Vehicle_Damage Annual_Premium ## 1 1 Male 44 1 28 0 &gt; 2 Years Yes 40454 ## 2 2 Male 76 1 3 0 1-2 Year No 33536 ## 3 3 Male 47 1 28 0 &gt; 2 Years Yes 38294 ## 4 4 Male 21 1 11 1 &lt; 1 Year No 28619 ## 5 5 Female 29 1 41 1 &lt; 1 Year No 27496 ## Policy_Sales_Channel Vintage Response ## 1 26 217 1 ## 2 26 183 0 ## 3 26 27 1 ## 4 152 203 0 ## 5 152 39 0 El conjunto de datos de trabajo es un archivo csv que se llama train y que previamente se ha descargado (como se indica con anterioridad), la función read.csv permite importar el csv de trabajo y crear un data frame en la sesión de R. Mediante la función str es posible ver el tipo de variables que tiene el data frame: str(train) ## &#39;data.frame&#39;: 381109 obs. of 12 variables: ## $ id : int 1 2 3 4 5 6 7 8 9 10 ... ## $ Gender : chr &quot;Male&quot; &quot;Male&quot; &quot;Male&quot; &quot;Male&quot; ... ## $ Age : int 44 76 47 21 29 24 23 56 24 32 ... ## $ Driving_License : int 1 1 1 1 1 1 1 1 1 1 ... ## $ Region_Code : num 28 3 28 11 41 33 11 28 3 6 ... ## $ Previously_Insured : int 0 0 0 1 1 0 0 0 1 1 ... ## $ Vehicle_Age : chr &quot;&gt; 2 Years&quot; &quot;1-2 Year&quot; &quot;&gt; 2 Years&quot; &quot;&lt; 1 Year&quot; ... ## $ Vehicle_Damage : chr &quot;Yes&quot; &quot;No&quot; &quot;Yes&quot; &quot;No&quot; ... ## $ Annual_Premium : num 40454 33536 38294 28619 27496 ... ## $ Policy_Sales_Channel: num 26 26 26 152 152 160 152 26 152 152 ... ## $ Vintage : int 217 183 27 203 39 176 249 72 28 80 ... ## $ Response : int 1 0 1 0 0 0 0 1 0 0 ... Numéricas o carácter, como se indicó, pero una variable numérica no tiene necesariamente un comportamiento numérico, por ese motivo siempre es mejor disponer las variables en Variables cuantitativas Variables cualitativas o factores No todas las variables numéricas serán variables cuantitativas, pero si todas las variables cualitativas serán factores, aunque pueden tener un orden. Nota: Nunca se debe tratar una cualidad como un número, si el sexo viene codificado 1-mujer 2-hombre, no debemos tratar esa variable como cuantitativa. 6.4.1 Descritivos en variables cualitativas (factores) En los datos hay variables que se pueden cuantificar y otras que definen cualidades de los datos. Una cualidad puede ser una característica (género, canal de venta,) o puede tener un orden como es una variable cualitativa ordinal (antigüedad de cliente, nivel de satisfacción,) en ambos casos, para describir su comportamiento de forma numérica se emplearán tablas de frecuencias. Estas tablas presentan cada valor de la variable cualitativa, o lo que es lo mismo, cada nivel del factor y contabilizan los registros que tienen esa característica. A la hora de contabilizar se tienen las frecuencias absolutas que contabiliza el número de registros para cada nivel del factor o las frecuencias relativas que contabiliza el porcentaje de individuos en cada nivel del factor y permiten relativizar esa cantidad. En el ejemplo de trabajo se dispone de diversas variables cualitativas. Se ilustran ejemplos de tablas de frecuencia mediante la librería dplyr. knitr::kable(train %&gt;% group_by(Gender) %&gt;% summarise(`Frecuencia absoluta` = n())) Gender Frecuencia absoluta Female 175020 Male 206089 Señalar la importancia de relativizar los datos absolutos, de obtener porcentajes. knitr::kable(train %&gt;% group_by(Gender) %&gt;% summarise(`Frecuencia relativa` = n()/nrow(train))) Gender Frecuencia relativa Female 0.4592387 Male 0.5407613 La variable género no tiene ningún tipo de orden, pero una variable cualitativa puede requerir un orden. knitr::kable(train %&gt;% group_by(Vehicle_Age) %&gt;% summarise(`Frecuencia relativa` = n()/nrow(train))) Vehicle_Age Frecuencia relativa &lt; 1 Year 0.4323855 &gt; 2 Years 0.0420011 1-2 Year 0.5256134 Por defecto R siempre presenta en las tablas de frecuencias el orden léxico gráfico, no el orden que tiene la variable, en este caso se tienen vehículos &lt; 1, 1 - 2 y &gt; 2, pero no es el orden que presenta el factor, es necesaria una ordenación. Creación y ordenación del factor. table(train$Vehicle_Age) ## ## &lt; 1 Year &gt; 2 Years 1-2 Year ## 164786 16007 200316 train$Vehicle_Age = factor(train$Vehicle_Age, levels=c(&#39;&lt; 1 Year&#39;,&#39;1-2 Year&#39;, &#39;&gt; 2 Years&#39;)) table(train$Vehicle_Age) ## ## &lt; 1 Year 1-2 Year &gt; 2 Years ## 164786 200316 16007 La función table permite realizar rápidas tablas de frecuencias, cuando trabajemos con variables categóricas es importante realizar esas rápidas comprobaciones sobre el correcto tratamiento de los datos. Nueva variable con el factor reclasificado mediante tidyverse. train &lt;- train %&gt;% mutate(fr_vehicle_age = case_when( Vehicle_Age==&#39;&lt; 1 Year&#39; ~ &#39;01 menor 1 año&#39;, Vehicle_Age==&#39;1-2 Year&#39; ~ &#39;02 Entre 1 y 2 años&#39;, TRUE ~ &#39;03 más de 2 años&#39;)) table(train$fr_vehicle_age) ## ## 01 menor 1 año 02 Entre 1 y 2 años 03 más de 2 años ## 164786 200316 16007 Usando cualquiera de los dos métodos para clasificar correctamente factores ordinales si es recomendable emplear un sufijo para determinar aquellas variables que han de ser analizadas, en este caso, se emplea fr_ indicando factor_reclasificado. Práctica muy útil cuando se manejen grandes cantidades de variables y poder distinguir variables input de variables raw (en bruto). 6.4.2 Descritivos en variables cuantitativas Las variables cuantitativas pueden tomar valores finitos (ejemplo la edad, antigüedad de cliente,) o valores infinitos (ejemplo prima de un seguro, salario,) pero en ambas situaciones se emplearán los mismos análisis descriptivos: Estadísticos descriptivos Representación gráfica que describa la forma y los posibles valores que toma la variable Y se pondrá especial cuidado en detectar algunos de estos problemas: Detección de valores modales Detección de outliers Detección de valores missing 6.4.2.1 Medidas de posición Los estadísticos descriptivos permiten conocer valores relevantes que toman las variables de trabajo. Cuando se quiere conocer sobre que valores se sitúa la variable estamos analizando medidas de posición entre las que destacan: Media Mediana Percentiles Moda 6.4.2.1.1 Media de una variable Es el valor sobre el que se sitúan los datos se obtendría sumando todos los valores de la variable y dividiendo por el número de registros, en los datos de trabajo, para la variable Age se tiene: train %&gt;% summarise(sum(Age)/nrow(train)) ## sum(Age)/nrow(train) ## 1 38.82258 Evidentemente R dispone de una función que realiza este cálculo y podemos hacerlo de diversas formas: train %&gt;% summarise(mean(Age)) ## mean(Age) ## 1 38.82258 mean(train$Age) ## [1] 38.82258 mean(train$Annual_Premium) ## [1] 30564.39 mean(train$Vintage) ## [1] 154.3474 Para describir correctamente los datos es necesario calcular la media para todas las variables cuantitativas, en R, mediante la librería kable es posible realizar salidas de datos formateadas: knitr::kable(train %&gt;% summarise(`Media edad` = mean(Age), `Media de prima` = mean(Annual_Premium), `Media de antigüedad (meses)` = mean(Vintage))) Media edad Media de prima Media de antigüedad (meses) 38.82258 30564.39 154.3474 Los valores de las variables se sitúan en el entorno de la media pero este valor está muy influenciado por la escala, al definirse como la suma de los valores de la variable entre el número de registros si uno de esos valores es muy alto es posible que la media tienda a perder representatividad sólo por un dato atípico. 6.4.2.1.2 Mediana de una variable Si se ordenan los datos de la variable en estudio de menor a mayor y establecemos un corte justo a la mitad tenemos otra medida de posición que nos permite conocer el punto que divide esa variable al 50% en valores inferiores y al 50% en valores superiores, esta medida es conocida como mediana. No es una programación exacta pero ilustrando el ejemplo con tidyverse se puede hacer. train %&gt;% select(Age) %&gt;% arrange(Age) %&gt;% filter(row_number() == round(nrow(train)/2,0)) ## Age ## 1 36 Se ha definido una función empleando tidyverse para aproximar el cálculo de la mediana, esta función tiene sus limitaciones pero sirve para establecer ese valor que deja a su derecha el 50% y a su izquierda el otro 50% de las observaciones. Evidentemente es mejor emplear la función específica de R: median(train$Age) ## [1] 36 6.4.2.1.3 Percentiles de una variable Además de esa separación 50% de valores a la izquierda - 50% de valores a la derecha para conocer como es una variable numéricamente podemos desear 5% - 95% o 75% - 25% a esos valores que dejan un X% de valores por la derecha se les denomina percentiles: quantile(train$Age, probs = c(0, 0.25, 0.5, 0.75, 1)) ## 0% 25% 50% 75% 100% ## 20 25 36 49 85 Para obtener el percentil aparece la función quantile que es cuantil en español, es el cuantil el que toma los valores de la variable a intervalos regulares si deseamos dividir en % estamos ante percentiles. Estos valores tienen unos puntos que caracterizan los datos, el percentil 0 es el mínimo de los valores de nuestra variable el valor 100 es el máximo de la variable y, por supuesto, el percentil 50 es la mediana de la variable. Una variable edad es cuantitativa ordinal, en este caso se tienen valores finitos entre 20 y 85. Si se replica el código para la variable prima: quantile(train$Annual_Premium, probs = c(0, 0.25, 0.5, 0.75, 1)) ## 0% 25% 50% 75% 100% ## 2630 24405 31669 39400 540165 En este caso la variable toma muchos valores, infinitos, entre 2630 y 540165 eso ya ofrece una impresión de la mayor complejidad para estudiar la concentración de valores. 6.4.2.1.4 Moda Otra medida importante para conocer como se posicionan los valores de una variable es la moda, se define como el valor más repetido. Puede parecer más relevante para variables cuantitativas finitas, sin embargo puede ser importante en variables cuantitativas infinitas porque hay valores que se repiten en la distribución y pueden representar un comportamiento. En R hay paquetes que calculan la moda pero podemos crear nuestra propia función: Mode &lt;- function(x) { fx &lt;- unique(x) fx[which.max(tabulate(match(x, fx)))]} # Moda para Age Mode(train$Age) ## [1] 24 # Moda para Annual_Premium Mode(train$Annual_Premium) ## [1] 2630 En los datos de trabajo la edad más repetida es Mode(train$Age) y hay un valor en la prima muy repetido que es el 2630 6.4.2.2 Medidas de dispersión Además de conocer sobre que valores se concentra la variable es necesario analizar como de dispersos están esos valores de esa medida de centralidad. Entre ellas se van a estudiar Rango intercuartílico Varianza y desviación típica Coeficiente de variación 6.4.2.2.1 Rango intercuartílico Como indica su nombre es un rango, un número inferior y un número superior donde esperamos tener el 50% de las observaciones, recuperando las definición del percentil, un rango que recogería ese porcentaje estaría entre el percentil 25 y el percentil 75, pues así se define el rango intercuartílico como la diferencia entre el percentil 75 - percentil 25 quantile(train$Age, probs = c(0.75)) - quantile(train$Age, probs = c(0.25)) ## 75% ## 24 IQR(train$Age) ## [1] 24 Esta medida establece una dispersión a partir de los percentiles, es una característica de las medidas de dispersión, siempre miden una diferencia con una medida de posición. 6.4.2.2.2 Varianza y desviación típica Para medir la dispersión una medida a emplear es la diferencia de cada observación a la media de todas las observaciones, pero es necesario eliminar el efecto del signo y elevar al cuadrado, si promediamos esa diferencia estaríamos ante la varianza. sum((train$Age - mean(train$Age))**2)/(nrow(train)-1) ## [1] 240.6101 var(train$Age) ## [1] 240.6101 La varianza no tiene una unidad de medida, para ello contamos con la desviación típica. sqrt(var(train$Age)) ## [1] 15.51161 sd(train$Age) ## [1] 15.51161 La desviación típica es la raiz de la varianza y está expresada en las mismas unidades que la variable lo que puede facilitar su interpretacion. 6.4.2.2.3 Coeficiente de variación Hilo de Twitter de AnaBayes para entender la importancia del coeficiente de variación Autor: AnaBayes Se define como la relación entre la desviación típica y la media y habitualmente se expresa en porcentaje. Es una medida de dispersión muy relevante porque no está en la unidad de la variable, es decir, no es lo mismo una dispersión de 200 grs. en una población de ranas que 200 grs. en una población de caballos, pero si podemos establecer una dispersión que sea el x% de la media. En este caso no es necesario disponer de una función. sd(train$Age) * 100/mean(train$Age) ## [1] 39.95512 Se puede decir que la variabilidad de la variable Age es un 40%. Cuando se analizan estadísticos descriptivos de cualquier tipo es muy importante relativizar porque empleando valores absolutos todas las conclusiones están afectadas por la unidad de medida. 6.4.2.3 Medidas de forma En el capítulo 4 se vieron los histogramas y los gráficos de densidad, en ellos se aprecia como es la forma, como se distribuye una variable en función del número de observaciones. La variable se ordena, se hacen tramos en los valores de la variable y se representan barras en función del número de observaciones que tiene cada tramo. Si se unen esas barras con una función continua se obtiene un gráfico de densidad que tiene la forma. Esa línea tiene dos características importantes lo apuntada que es y hacia que posición va ese apuntamiento, la curtosis mide ese apuntamiento y la asimetría mide la dirección de esa punta. No es habitual obtener estas medidas de posición pero si es importante conocer la forma que tiene una variable porque esa forma es posible que pertenezca a una familia de funciones conocidas. Para conocer la asimetría se emplea el paquete de R e1071 que contiene una serie de funciones estadísticas. library(e1071) skewness(train$Age) ## [1] 0.6725337 Un valor superior a 0 indica que es asimétrica a la izquierda por lo que la moda &lt; mediana &lt; media. Valores inferiores a 0 indican asimetría a la derecha por lo que media &lt; mediana &lt; moda, la simetría perfecta sería media = mediana = moda. La asimetría a la izquierda es propia de variables que indican precios como es Annual_Premium en los datos de trabajo. skewness(train$Annual_Premium) ## [1] 1.766073 En este caso se tiene una asimetría muy alta a la izquierda, hay importes muy altos de prima anual. Con la función skewness se puede calcular la asimetría por varios métodos, sería necesario modificar el parámetro type. La curtosis o apuntamiento también se calcula con la función kurtosis del paquete e1071. kurtosis(train$Age) ## [1] -0.5656762 kurtosis(train$Annual_Premium) ## [1] 34.00391 Una curtosis negativa indica una distribución más cuadrada y una curtosis positiva indica un apuntamiento, cuanto mayor sea ese apuntamiento mayor será la curtosis. En el ejemplo de trabajo la curtosis de la variable Age es negativa, es una forma de caja; sin embargo, para la variable Annual_Premium se tiene un apuntamiento muy alto, apuntamiento alto y asimetría hacia la izquierda. "],["descripcion-grafica-datos.html", "Capítulo 7 Descripción gráfica de datos 7.1 Descripción gráfica de factores 7.2 Descripción gráfica de variables numéricas", " Capítulo 7 Descripción gráfica de datos Los estadísticos son insuficientes para conocer una variable, la siguiente figura es muy conocida y presenta unas series de pares de datos X e Y con los mismos estadísticos que son completamente diferentes. Importancia de la visualización de datos Disponer los estadísticos es insuficiente para conocer como son los valores que toma una variable, como se distribuye. Se torna necesario describir mejor ese comportamiento mediante análisis gráficos. En capítulos anteriores se trabajó con las posibilidades que ofrece ggplot para visualizar datos. A continuación se desarrollan esas posibilidades y se estudia como describen nuestros datos esos gráficos. El primer paso es cargar los datos de trabajo, ya conocidos, y se da comienzo con el trabajo. library(tidyverse) train &lt;- read.csv(&quot;./data/train.csv&quot;) 7.1 Descripción gráfica de factores Si numéricamente tenemos frecuencias absolutas o frecuencias relativas los gráficos más sencillos para describir estas variables son los gráficos de barras y gráficos de sectores. resumen &lt;- train %&gt;% group_by(Gender) %&gt;% summarise(clientes = n()) ggplot(resumen, aes(x=&#39;&#39;, y = clientes, fill=Gender)) + geom_bar(stat=&quot;identity&quot;, width=1) + coord_polar(&quot;y&quot;, start=0) Con datos absolutos el gráfico de sectores pierde sentido, aunque es posible conocer como se distribuye la variable en la población es necesaria la relativización, la obtención de porcentajes. resumen &lt;- train %&gt;% group_by(Gender) %&gt;% summarise(porcen_clientes = n()/nrow(train)) ggplot(resumen, aes(x=&#39;&#39;, y = porcen_clientes, fill=Gender)) + geom_bar(stat=&quot;identity&quot;, width=1) + coord_polar(&quot;y&quot;, start=0) En el caso concreto de los gráficos de sectores puede ser más sencillo emplear los gráficos propios del módulo base, el corazón de R. En ese caso, se dispone de la función pie resumen &lt;- train %&gt;% group_by(Gender) %&gt;% summarise(porcen_clientes = n()/nrow(train)) pie(resumen$porcen_clientes, labels=paste0(resumen$Gender, &#39;: &#39;, round(resumen$porcen_clientes*100,1),&#39;%&#39;), main=&quot;Gráfico sectores por sexo&quot;) El código y el gráfico (aparentemente) es más sencillo de entender. Este tipo de representación está limitada cuando el factor a analizar tiene un gran número de niveles como sucede con la variable Region_Code. resumen &lt;- train %&gt;% group_by(Region_Code) %&gt;% summarise(porcen_clientes = n()/nrow(train)) pie(resumen$porcen_clientes, labels=paste0(resumen$Region_Code, &#39;: &#39;, round(resumen$porcen_clientes*100,1),&#39;%&#39;), main = &quot;Gráfico sectores por Región&quot;) La variable Region_Code tiene 0 posibles valores. Una puntualización, aunque en los datos de partida tiene un formato numérico estamos ante un factor, este ejemplo ilustra la importancia de conocer los datos que se están empleando. El conocimiento de las fuentes de información ha de ser previo al análisis descriptivo, o bien, este análisis nos puede servir para identificar estas incoherencias. Recuperando la defición de los datos anteriormente planteada. - Region_Code Unique code for the region of the customer Código único no expresa un valor numérico, expresa una codificación. Puede parecer un caso sencillo pero es importante tener en cuenta estas situaciones cuando se trabajan con un gran volumen de variables. Para factores como este, con un gran número de niveles, puede ser más apropiado el uso de gráficos de barras. train %&gt;% group_by(Region_Code) %&gt;% summarise(porcen_clientes = n()/nrow(train)) %&gt;% ggplot(aes(x=Region_Code, y=porcen_clientes)) + geom_bar(stat=&quot;identity&quot;) En este código no se genera el habitual data frame intermedio, directamente podemos aplicar ggplot sobre la sumarización, es otro modo de trabajar si no se quiere disponer de la sumarización. Se observa que el eje X interpreta la variable Region_Code como numérica, establece rangos númericos, no aparece cada valor. se recomienda empezar a describirla como un factor. Esta es una situación habitual que se da en los datos importados, hay valores cualitativos que se almacenan en vriables cuantitativas y viceversa. Se trata de un problema habitual que se encuentra el científico de datos. train %&gt;% group_by(Region_Code = as.factor(Region_Code)) %&gt;% summarise(`Porcentaje de clientes` = round(n()*100/nrow(train),2)) %&gt;% ggplot(aes(x=Region_Code, y = `Porcentaje de clientes`)) + geom_bar(stat=&quot;identity&quot;) + ggtitle(&quot;Porcentaje de clientes por región&quot;) En esta nueva propuesta el eje x es distinto, ya no hay intervalos, cada región tiene su correspondiente código, si la variable es numérica se disponen intervalos. En cualquier caso, se aprecia como se complica la lectura de esos datos porque el factor Region_code tiene un gran número de niveles. Es importante destacar la importancia descriptiva de los gráficos de barras ya que nos permiten conocer la composición de los datos en función de los niveles de un factor. Además, se está repitiendo el mismo código en cada uno de los gráficos propuestos. Cuando estamos en estas situaciones es útil crear funciones. #Función para describir factores describe_factor &lt;- function(df, fct){ df %&gt;% group_by(factor_analisis = as.factor(!!as.name(fct))) %&gt;% summarise(`Porcentaje de clientes` = n()/nrow(train)) %&gt;% ggplot(aes(x=factor_analisis, y = `Porcentaje de clientes`)) + geom_bar(stat=&quot;identity&quot;) + ggtitle(paste0(&quot;Porcentaje de clientes por &quot;,fct)) } describe_factor(train, &#39;Gender&#39;) describe_factor(train, &#39;Vehicle_Age&#39;) describe_factor(train, &#39;Vehicle_Damage&#39;) El código que describe el factor es repetitivo, para simplificar la programación en R se emplea una función, no es el objeto de esta formación aprender a realizar funciones con R pero se van a ilustrar algunos ejemplos de uso. Además, cuando se usa dplyr facilita el uso de funciones trabajar con símbolos que es una forma de referir valores de un objeto de R, en este caso los nombres y por ello se emplea as.name() con el único paramétro fct que recibe la función. En este caso la función describe_factor crea una sumarización previa por el factor que describe el parámetro de la función fct y posteriormente se genera el gráfico de barras más sencillo posible con ggplot, sólo se parametriza el título del gráfico uniendo un texto con el nombre de la variable. Esta función puede describir también variables cuantitativas. describe_factor(train, &#39;Age&#39;) Se recuerda que una variable cuantitativa puede tener un número finito de elementos lo que da pie a ser tratada como un factor, paradigma de esta situación es la variable edad en prácticamente todos los conjuntos de datos. 7.2 Descripción gráfica de variables numéricas Vistos con anterioridad los estadísticos de posición, variación y forma para las variables cuantitativas es imprescindible la descripción gráfica, en el capítulo introductorio a ggplot se pusieron sobre el papel diversos análisis gráficos que servirán para realizar esta descripción. Se parte con los gráficos más sencillos, realizados con el código más sencillo de ggplot y paulatinamente se irá sofisticando ese código a lo largo del curso. Se comienza con los histogramas y los gráficos de densidad. # Histogramas ggplot(data = train, aes(x = Age)) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. # Gráficos de densidades ggplot(data = train, aes(x = Age)) + geom_density() # Ambos ggplot(data= train,aes(x= Age))+ geom_histogram(binwidth = 5)+ geom_density(aes(y=5 * ..count..), color=&#39;Blue&#39;) Tanto histograma como gráfico de densidad representan todos los posibles valores que puede tomar la variable cuantitativa. Nunca se representará un valor que no aparece y nunca se dejará de representar algún valor existente. En el caso del gráfico de densidad la línea continua deja una superficie por debajo de la curva de tamaño 1, es decir, el 100% de las observaciones están en algún punto por debajo de esa curva y de izquierda a derecha se pueden ir acumulando porcentaje de observaciones, se puede ir acumulando densidad. Volviendo con los principales estadísticos descriptivos se puede buscar como aparecen reflejados en el histograma. library(e1071) # Para la moda se crea una función Mode &lt;- function(x) { fx &lt;- unique(x) fx[which.max(tabulate(match(x, fx)))]} knitr::kable(train %&gt;% summarise(`Media edad` = mean(Age), `Mediana edad` = median(Age), `Moda de edad` = Mode(Age), `Desviación de edad` = sd(Age))) Media edad Mediana edad Moda de edad Desviación de edad 38.82258 36 24 15.51161 ggplot(data = train, aes(x = Age)) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. La moda se sitúa en la barra más alta, es el valor que más se repite, la forma de la variable parece que tiene a su vez como dos formas distintas, la encuesta se ha realizado a clientes jóvenes y a clientes en el rango entre 40 y 50 años, como mucho se ha preguntado a clientes de 90 años y en ningún caso a clientes menores de edad. En este caso los estadísticos descriptivos poco o nada podían aportar. En cuanto al código empleado hay particularidades en el gráfico que une histograma y densidad. Al ser dos valores en distinta escala es necesario realizar un ajuste para que ambos gráficos se puedan ver en uno y por eso se modifican los valores del gráfico de densidades mediante ..count.. ggplot tiene un código propio que puede hacer labores básicas, es importante saber que disponemos de esa herramienta. Podemos representar algunas de esas medidas en el gráfico. media_c = mean(train$Age) ggplot(data = train, aes(x = Age)) + geom_histogram() + geom_vline(xintercept = media_c, color=&#39;blue&#39;) + geom_text(aes(media_c, 20000, label=paste(&quot;Media = &quot;, round(media_c,1))), size=5, color=&#39;blue&#39;, data = data.frame()) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Con geom_vline añadimos una barra vertical y con geom_text añadimos texto a nuestro gráfico de ggplot. En cuanto a la asimetría teníamos un 0.67 un valor muy positivo indica asimetría a la izquierda, algo muy evidente viendo como se distribuye la variable a lo largo del gráfico de densidad. La kurtosis poco o nada puede aportar con una forma tan peculiar del gráfico de densidad donde parece que hay dos formas diferenciadas dentro de los datos. A continuación, se analiza la variable Annual_premium con un comportamiento sustancialmente distinto. knitr::kable(train %&gt;% summarise(`Media prima` = mean(Annual_Premium), `Mediana prima` = median(Annual_Premium), `Moda de prima` = Mode(Annual_Premium), `Desviación de prima` = sd(Annual_Premium))) Media prima Mediana prima Moda de prima Desviación de prima 30564.39 31669 2630 17213.16 ggplot(data= train,aes(x = Annual_Premium))+ geom_histogram(bins = 100) + geom_density(aes(y= 10000 * ..count..), color=&#39;Blue&#39;) ggtitle(&#39;Distribución de la variable prima anual&#39;) ## $title ## [1] &quot;Distribución de la variable prima anual&quot; ## ## attr(,&quot;class&quot;) ## [1] &quot;labels&quot; Cuando se trabajen con variables como importes se puede complicar el histograma y el gráfico de densidades es recomendable jugar con el número de grupos, anchura de los mismos, eliminar densidades, etc. En este caso concreto se aprecia que la forma, la distribución de los valores de la variable tiene un punto alto en el 0, un valor modal en el 0, no se deberían tener clientes sin prima si se trabaja con una base de clientes de un ramo de salud. Posteriormente tiene una forma muy asimétrica hacia la izquierda debido a que hay valores de prima muy altos. Esos valores 0 hacen que la media sea más baja que la mediana, la desviación típica es muy alta. Como se vio con anterioridad estábamos con valores muy altos de asimetría, la distribución es asimétrica por la izquierda y con valores muy altos de curtosis, hay un apuntamiento muy alto, muy alto a la izquierda. Este tipo de forma, este tipo de distribución es muy típica de importes, ninguno es menor que 0 y hay importes muy altos. Además de los histogramas y los gráficos de densidad en la descripción de las posibilidades gráficas se dio importancia a los boxplot, estos gráficos contienen mucha y muy valiosa información sobre como es una variable cuantitativa. ggplot(train , aes(y = Age)) + geom_boxplot() + coord_flip() Mediante coord_flip() se ha rotado la figura generada con ggplot. Esta representación gráfica está muy ligada a los estadísticos de posición y a algunos estadísticos de dispersión. summary(train$Age) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 20.00 25.00 36.00 38.82 49.00 85.00 p &lt;- ggplot(train , aes(y = Age)) + geom_boxplot() + coord_flip() p + annotate(geom=&quot;text&quot;, x=-0.4, y=summary(train$Age)[1], label=summary(train$Age)[1],color=&quot;red&quot;) + annotate(geom=&quot;text&quot;, x=-0.4, y=summary(train$Age)[2], label=summary(train$Age)[2],color=&quot;red&quot;) + annotate(geom=&quot;text&quot;, x=-0.4, y=summary(train$Age)[3], label=summary(train$Age)[3],color=&quot;red&quot;) + annotate(geom=&quot;text&quot;, x=-0.4, y=summary(train$Age)[5], label=summary(train$Age)[5],color=&quot;red&quot;) + annotate(geom=&quot;text&quot;, x=-0.4, y=summary(train$Age)[6], label=summary(train$Age)[6],color=&quot;red&quot;) En este ejemplo ggplot no genera directamente el gráfico, genera un objeto lista con los elementos del gráfico, a esos elementos les ponemos un texto en este caso con la función annotate que se corresponde con los percentiles que nos arroja la función summaryde R por lo que el gráfico de cajas, el boxplot describe gráficamente donde están el 50% de las observaciones entre que valores está comprendida la variable y donde se sitúa la mediana. Además, también toma mucha relevancia obtener gráficos de cajas que dicen menos. summary(train$Annual_Premium) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 2630 24405 31669 30564 39400 540165 ggplot(train , aes(y = Annual_Premium)) + geom_boxplot() + coord_flip() Ya se vio con el gráfico de densidad que la prima anual toma unos valores más complejos. En este caso de la caja parece muy estrecha y aparecen una serie de puntos por la derecha que dificultan la descripción de los datos. Cuando aparecen esos puntos en estos gráficos los posibles valores que toma la variable tienen algún valor atípico, la distribución de la variable tiene outliers. Es uno de los posibles problemas que pueden plantear los datos numéricos y que se desarrollarán en capítulos posteriores. "],["problemas-datos.html", "Capítulo 8 Problemas con los datos 8.1 Problemática con factores 8.2 Problemas con variables numéricas", " Capítulo 8 Problemas con los datos Siguiendo con el desarrollo se ha establecido una estructura de datos, fundamentalmente se está trabajando con data frames, que se componen de filas (registros) y columnas (variables). Tanto registros como variables pueden presentar problemas que dificulten la gestión de la información al científico de datos. En este capítulo se van a estudiar los problemas más comunes con los datos y se plantean posibles estrategias para resolver estos problemas. Aunque los datos pueden presentar problemas desde el punto de vista de los registros y desde el punto de vista de las variables en este caso se van a abordar análisis de variables que permitirán identificar tanto variables como registros que distorsionan el análisis. Para encontrar y describir estas situaciones se dispone tanto de análisis numéricos como análisis gráficos sencillos con los que se tomó contacto en los dos capítulos anteriores. Para ilustrar esta situaciones se continúa con el caso práctico de trabajo visto en el apartado 6.2 de este libro. Como en capítulos anteriores se cargan los datos: library(tidyverse) train &lt;- read.csv(&quot;./data/train.csv&quot;) head(train,5) ## id Gender Age Driving_License Region_Code Previously_Insured Vehicle_Age Vehicle_Damage Annual_Premium ## 1 1 Male 44 1 28 0 &gt; 2 Years Yes 40454 ## 2 2 Male 76 1 3 0 1-2 Year No 33536 ## 3 3 Male 47 1 28 0 &gt; 2 Years Yes 38294 ## 4 4 Male 21 1 11 1 &lt; 1 Year No 28619 ## 5 5 Female 29 1 41 1 &lt; 1 Year No 27496 ## Policy_Sales_Channel Vintage Response ## 1 26 217 1 ## 2 26 183 0 ## 3 26 27 1 ## 4 152 203 0 ## 5 152 39 0 A modo de recuerdo, son datos de una compañía de seguros que opera en el ramo de salud y que desea realizar una campaña de venta cruzada para ofrecer seguro de automóviles a sus asegurados de salud. Para ello ha elaborado una encuesta que le permite perfilar que clientes son los más propensos a comprar su producto de automóviles. 8.1 Problemática con factores 8.1.1 Factores con niveles de pocos registros Cuando se estudian características, cualidades, es posible que una cualidad esté presente en un porcentaje muy alto de observaciones. Cuando un factor presenta un nivel con un gran número de observaciones y el resto de niveles apenas tiene observaciones, ¿tiene sentido conservar ese factor? En la descripción de los datos se hace mención a esta variable Driving_License. El científico de datos va a emplear la estadística para conocer y depurar la calidad de los datos que maneja, esa estadística le va a permitir transformar esos datos en información pero antes de todo análisis tiene que apelar al sentido común y al conocimiento y la motivación de su trabajo. El problema que se quiere resolver con los datos a nuestra disposición es determinar que clientes del ramo de salud son más propensos a comprar una póliza del ramo de automóviles ¿tiene sentido vender a clientes sin carnet de conducir pólizas de automóviles? ¿Qué nos dice la variable? resumen &lt;- train %&gt;% group_by(Driving_License=as.factor(Driving_License)) %&gt;% summarise(porcen_clientes = n()/nrow(train)) ggplot(resumen, aes(x=Driving_License, y=porcen_clientes)) + geom_bar(stat=&quot;identity&quot;) + ggtitle(&quot;Distribución de variable Driving_License&quot;) La práctica totalidad de los clientes encuestados tiene carnet. Es una variable que no tiene sentido, si se va a ofrecer un seguro de automóviles será necesario que el cliente pueda optar a tener un automóvil. En esta situación se plantea una estrategia muy simple la variable ha de ser eliminada, bien es cierto que se puede plantear la eliminación de aquellos clientes que no tienen carnet, pero son muy pocos. Un análisis descriptivo de lo más sencillo está permitiendo detectar claros problemas de negocio. Cuando se trabaje con factores es necesario estudiar si hay algún nivel con un porcentaje desmesurado de observaciones, esa desmesura la debe establecer el científico de datos con sus análisis descriptivos, si fuera necesario fijar un umbral, por encima de un 95% de observaciones en un mismo nivel se sugiere trabajar con mayor detenimiento ese factor. 8.1.2 Factores que aparecen como variables numéricas Se sigue haciendo mención en múltiples ocasiones, el científico de datos debe conocer los datos con los que está trabajando. Habrá situaciones en las que se enfrente a cientos (miles de variables) y tenga que decidir cuales de ellas sirven en su tarea. Esto sucede cuando los tablones de trabajo se almacenan en estructuras como data lakes con un volumen ingente tanto de registros como de variables por eso el científico de datos establece como describir esas variables, si describir una variable numérica o describir un factor. En los datos de trabajo este problema se presenta en la variable Region_code. summary(train$Region_Code) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.00 15.00 28.00 26.39 35.00 52.00 ¿Tiene sentido esta sumarización? Evidentemente no, y este es uno de los problemas más comunes que aparece en el trabajo con datos. El formato de las variables no es el adecuado, lo podremos identificar conociendo los datos o bien asignando nombres a las variables que nos permitan intuir el tipo, en este caso se emplea el sufijo code que ya nos deja claro que tipo de variable es y como debemos tratarla resumen &lt;- train %&gt;% group_by(Region_Code=as.factor(Region_Code)) %&gt;% summarise(porcen_clientes = n()/nrow(train)) %&gt;% arrange(desc(porcen_clientes)) head(resumen) ## # A tibble: 6 x 2 ## Region_Code porcen_clientes ## &lt;fct&gt; &lt;dbl&gt; ## 1 28 0.279 ## 2 8 0.0889 ## 3 46 0.0518 ## 4 41 0.0479 ## 5 15 0.0349 ## 6 30 0.0320 ggplot(resumen, aes(x=Region_Code, y=porcen_clientes)) + geom_bar(stat=&quot;identity&quot;) + ggtitle(&quot;Distribución de variable Region_Code&quot;) Se ha indicado con anterioridad que son datos de una aseguradora española, si se conoce la distribución geográfica de España esta variable hace referencia a las provincias, donde el 28 es Madrid y el 8 es Barcelona. Es posible que este dato no se conozca, se insiste, es relevante asociar un dato numérico a una codificación, esta variable nunca habría de tratarse como numérica. En el caso que nos ocupa, conocida la estructura administrativa de España, aparece la provincia 0, como tal no es un valor perdido, es un valor desconocido que no ofrece información a la resolución del problema que se está planteando con los datos. IMPORTANTE: Nunca se deben dar análisis numéricos de factores, no se deben hacer medias de regiones, ni medias de sexo, ni percentiles de códigos. Hay que tener muy claros los datos que participan en los análisis. 8.1.3 Factores con un gran número de niveles Trabajar con códigos puede plantear un problema con el número de niveles en un factor, en el ejemplo de la variable Region_Code se representa el ranking de porcentaje de encuestas por provincia española: train %&gt;% group_by(Region_Code=as.factor(Region_Code)) %&gt;% summarise(porcen_clientes = n()/nrow(train)) %&gt;% ggplot(aes(x=reorder(Region_Code,porcen_clientes), y=porcen_clientes)) + geom_bar(stat=&quot;identity&quot;) + ggtitle(&quot;Distribución de variable Region_Code&quot;) + coord_flip() El código empleado en ggplot tiene aspectos interesantes y que permiten crear gráficos descriptivos más completos. Empezando por indicar que no se crea un data frame temporal intermedio, directamente se emplea el pipe (%&gt;%) para realizar la representación gráfica. Por otro lado, se emplea la función reorder(factor, orden) para ordenar el factor en análisis en función de otro campo, en este caso se pretenden ordenar los códigos de provincia en función del porcentaje de observaciones que contiene. Por otro lado, para realizar un ranking se emplea un gráfico de barras horizontal, para rotar el gráfico se emplea coord_flip(), se aprecia que el eje y conserva el nombre de la variable establecida en aes. Este trabajo es habitual cuando se quieren representar rankings de factores. Cabe preguntarse ¿Es necesario agrupar esta variable? Se puede responder con otra cuestión, ¿se debe dar la misma relevancia al código 28 (Madrid) que al código postal 51 (Ceuta)? La respuesta evidente es no, pero esta respuesta no tiene ningún sustento estadísto, al menos de momento. Cuando aparece esta situación será de utilidad agrupar los niveles del factor, posibles estrategias para esta labor: Agrupar los niveles en base al peso, en este caso al % de clientes. Agrupar los niveles en base a criterios de negocio. Agrupar los niveles en base a nuestro problema, en este caso, agrupar los códigos de provincia en base a una variable respuesta, agrupar en base al interés por el producto de automóviles. En la tercera de las estrategias se plantea estudiar una variable en función de otra lo que denominamos análisis bivariable, en estos momentos no se dispone de una herramienta para realizar este análisis por ello se sugiere la primera de las estrategias. train &lt;- train %&gt;% mutate(fr_zona = case_when( Region_Code == 28 ~ &#39;Madrid&#39;, Region_Code == 8 ~ &#39;Barcelona&#39;, TRUE ~ &#39;Resto&#39;)) train %&gt;% group_by(fr_zona) %&gt;% summarise(porcen_clientes = n()/nrow(train)) %&gt;% ggplot(aes(x=fr_zona, y=porcen_clientes)) + geom_bar(stat=&quot;identity&quot;) Se transforma un factor con 53 niveles en un nuevo factor reclasificado (prefijo fr_ en la variable) que tiene 3 niveles. Es buena práctica asignar prefijos o sufijos a variables que se codifiquen cuando se manejan datos. De este modo se puede determinar el rol que juega esa variable en el análisis planteado. A lo largo de este trabajo se insistirá en esa labor. 8.1.4 Factores con valores perdidos Otro de los problemas que aparece en los datos son los valores perdidos o missing. Es una problemática común con valores numéricos pero hay matices que el científico de datos debe conocer. El valor perdido no ha de ser necesariamente un valor NA en R si volvemos al ejemplo de la variable Region_Code: resumen &lt;- train %&gt;% group_by(Region_Code) %&gt;% summarise(porcen_clientes = n()/nrow(train)) %&gt;% arrange(Region_Code) head(resumen) ## # A tibble: 6 x 2 ## Region_Code porcen_clientes ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0 0.00530 ## 2 1 0.00264 ## 3 2 0.0106 ## 4 3 0.0243 ## 5 4 0.00473 ## 6 5 0.00336 Se ha indicado que Region_Code es una variable que recoge los códigos provinciales de España. En España no existe un código provincial 0, no es un valor perdido, es un valor desconocido. Puede parecer lo mismo pero no lo es, se volverá sobre esta situación cuando se trabajen los valores perdidos numéricos. Posibles estrategias ante esta situación: Imputar ese valor al nivel con mayor número de observaciones, al 28. Esta posibilidad no debería afectar al estudio, pero es cierto que el 0 tiene cierto peso. Al disponer de más de 300.000 observaciones se puede optar por eliminar esas observaciones, no deben afectar al estudio. No hacer nada. Pocas observaciones, no deberían existir clientes sin provincia. Se puede dejar ese nivel de esa observación y estudiar si tiene un comportamiento atípico. 8.2 Problemas con variables numéricas 8.2.1 Valores modales Esta es una problemática que no se trata en muchas ocasiones pero tiene relevancia en las aproximaciones iniciales a los datos. En las variables numéricas hay valores que se repiten en múltiples ocasiones y pueden recoger comportamientos a tener en cuenta. Para las primeras aproximaciones a las variables numéricas se disponía tanto de los gráficos de densidad como de los histogramas. Se analiza la variable Annual_Premium ggplot(data= train,aes(x = Annual_Premium))+ geom_histogram(bins = 100) + geom_density(aes(y= 10000 * ..count..), color=&#39;Blue&#39;) ggtitle(&#39;Distribución de la variable prima&#39;) ## $title ## [1] &quot;Distribución de la variable prima&quot; ## ## attr(,&quot;class&quot;) ## [1] &quot;labels&quot; La prima es un importe, el científico de datos ya ha de familiarizarse con los gráficos de densidad, y en importes se suelen obtener formas asimétricas con mayor número de observaciones a la izquierda con colas muy largas. Es decir, hay una alta concentración de importes bajos o habituales y luego hay importes desmesurados, no suelen producirse importes negativos o importes 0. Sin embargo, en este caso la prima anual que es lo que paga el cliente por su seguro de Salud tiene un gran número de observaciones en un valor muy próximo a 0. Se puede considerar que la variable Annual_Premium tiene un valor modal que se va a identificar numéricamente: library(kableExtra) Mode &lt;- function(x) { fx &lt;- unique(x) fx[which.max(tabulate(match(x, fx)))]} moda &lt;- Mode(train$Annual_Premium) resumen &lt;- train %&gt;% group_by(Prima_modal = ifelse(Annual_Premium &lt;= moda, &quot;Si&quot;, &quot;No&quot;)) %&gt;% summarise(porcen_clientes = n() / nrow(train)) kable(resumen) Prima_modal porcen_clientes No 0.8297679 Si 0.1702321 Sirva este proceso de ejemplo de uso de la librería kableExtra para la presentación de data frames con formas vistosas en R. Se calcula la moda de la variable Annual_Premium y se determina cuantos registros están por debajo de ese umbral de 2630 de importe anual, se tiene 17% que es un número importante de observaciones para determinar con el equipo que suministra los datos que está sucediendo con esa variable. En esta situación una posible estrategia es discretizar la variable prima para dar ese sentido de negocio, es decir, crear un factor a partir de la variable numérica que nos permita aislar ese comportamiento. En este caso, se decide mantener la variable y posteriores análisis indicarán que hacer con ella pero parece clara la necesidad de discretizar. 8.2.2 Valores atípicos Un valor atípico es aquel que está fuera de un rango de valores donde esperamos encontrarnos un % muy alto de observaciones, estos valores se denominan outliers y pueden distorsionar tanto análisis numéricos como gráficos. Volviendo sobre la variable Annual_Premium y los gráficos boxplot vistos en el capítulo 7. ggplot(train , aes(y = Annual_Premium)) + geom_boxplot() + coord_flip() Este gráfico se realiza a partir de las medidas de posición y las medidas de dispersión de una variable. No existe una definición formal de outlier pero se usa un umbral sobre el que se volverá posteriormente, si un valor supera 1.5 veces el rango intercuartílico estamos ante un outlier: ggplot(train , aes(y = Annual_Premium)) + geom_boxplot(outlier.colour = &quot;red&quot;) + coord_flip() En este código mediante outlier.colour = \"red\" podemos cambiar el color a los outliers, podemos cambiar el color a aquellas observaciones que están +/- 1.5 veces por encima del rango intercuartílico. summary(train$Annual_Premium) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 2630 24405 31669 30564 39400 540165 # Límite inferior del rango intercuartílico lim1 = summary(train$Annual_Premium)[2] - 1.5 * IQR(train$Annual_Premium) sum(train$Annual_Premium&lt;=lim1) ## [1] 0 # Límite superior del rango intercuartílico lim2 = summary(train$Annual_Premium)[5] + 1.5 * IQR(train$Annual_Premium) lim2 ## 3rd Qu. ## 61892.5 sum(train$Annual_Premium&gt;=lim2) ## [1] 10320 Como indicaba el boxplot la variable Annual_Premium no tiene valores atípicos en la parte inferior de su distribución, pero en la parte superior, todo aquel valor que supere los 6.18925^{4} será considerado como un outlier. En este caso tenemos 10320 observaciones cuyo valor de prima está por encima de ese umbral, tenemos 10320 outliers. Pueden parecer muchas observaciones pero trabajamos con 381109 observaciones lo que supone un 2.7% del total de registros. En este caso, las estrategias para solventar este problema dependen del científico de datos, se puede optar: Eliminar las observaciones. Si no se tienen muchos registros se pueden considerar atípicos y no participar en el estudio. Asignación de un valor máximo de modo que no sean atípicos. En este caso se vuelve a la idea de discretizar una variable numérica ya que si supera determinado umbral tendría siempre el mismo valor. 8.2.3 Variables numéricas como factores En apartados anteriores se está viendo la posibilidad de tratar variables numéricas como factores para solventar tanto la existencia de valores modales que puedan tener un significado de negocio como solventar la existencia de outliers y no tener que eliminar registros. Sin embargo, hay ocasiones en las que una variable numérica puede interesarnos que sea un factor sobre todo en procesos de modelización como se verá en capítulos posteriores. Sirva como ejemplo la variable Age: ggplot(data= train,aes(x= Age))+ geom_histogram(binwidth = 5)+ geom_density(aes(y=5 * ..count..), color=&#39;Blue&#39;) + ggtitle(&quot;Distribución variable Edad&quot;) La edad toma valores finitos con orden. Como variable numérica se aprecian dos formas en los datos, en edades jóvenes y edades maduras. 8.2.4 Valores perdidos en variables numéricas Esta problemática afecta tanto a variables numéricas como factores, las estrategias para resolver el problema son parecidas. Sin embargo, este problema tiene importancia en variables numéricas porque el científico de datos debe conocer como se preparan la información que está manejando ya que será habitual que el propio científico de datos realice ese tablón de datos esa tabla que es una base de observaciones a la que se pueden ir anexando columnas, a la que se va uniendo datos en forma de nuevas variables. En ese proceso de añadir datos la forma en la que los añadamos puede hacer que un valor perdido tenga sentido. Por ejemplo, una base de clientes de una entidad bancaria, un tablón de clientes, se van uniendo nuevos datos y es necesario crear una variable que indique si el cliente presente en ese tablón tiene saldo en fondo de inversión o no tiene. El tablón de inicio tiene los clientes de trabajo, por otro lado, se extraen los clientes con fondos de inversión y se cruzan ambas tablas, al realizar el cruce es posible que, aquellos que nunca han tenido fondos de inversión, no hayan cruzado y se queden con un valor vacío, en ese caso el valor perdido tiene un sentido -&gt; El cliente no ha tenido fondos de inversión En esta situación no se debe establecer un saldo de 0 en fondos de inversión porque es posible que un cliente que haya tenido fondos tenga saldo 0. Se tienen 3 posibles situaciones, saldo, saldo 0 o nunca ha tenido fondos. Esa situación la describe un valor perdido, ¿tiene sentido dar un valor 0 al valor perdido? Hay que tener clara la importancia de esa variable en el análisis porque la imputación de valores perdidos en variables numéricas se puede volver un proceso muy costoso. Se pueden imputar valores medios, realizar un modelo específico para imputar valores perdidos, asignar un valor modal teniendo en cuenta la problemática que puede surgir. Un gran número de valores perdidos también es un indicador de la calidad de la información y de las variables. En los datos de trabajo no se tienen valores perdidos pero en este caso la estrategia el clara es necesario imputar los valores perdidos pero es el científico de datos el que debe elegir el tipo de imputación. En R hay paquetes y muchas posibilidades de imputación pero siempre bajo un criterio de negocio que ampare su decisión. "],["analisis-exploratorio-datos-EDA.html", "Capítulo 9 Análisis Exploratorio de Datos. EDA", " Capítulo 9 Análisis Exploratorio de Datos. EDA Los capítulos 7 y 8 se resumen en éste mediante visualizaciones trabajadas en el capítulo 5. El análisis EDA (Exploratory Data Analysis) es el primer paso que ha de seguir un científico de datos y articula los temas tratados en el capítulo 2 ya que convertir datos en información implica que el científico de datos ha de preocuparse en saber como pueden estar estructurados sus datos, que tipo de variables los componen, el nivel al que se encuentran los registros, que problemas pueden presentar o como resumir información. Además, es necesario conocer los capítulos 3 y 4 para el manejo de variables y cruces de tablas y por este motivo el análisis EDA es la base, pero lo visto anteriormente son los cimientos. Para realizar este tipo de análisis R dispone de distintas librerías, algunas de ellas son: ExPanDaR dataMaid DataExplorer Se sugiere que sea el propio científico de datos quien debería tener sus propias herramientas descriptivas. En este capítulo del ensayo se empleará la libería DataExplorer por rapidez de realización, sencillez de uso y fácil interpretación de la salida que ofrece. En la línea habitual se trabaja con el caso práctico de la campaña de venta cruzada de una empresa aseguradora. library(tidyverse) train &lt;- read.csv(&quot;./data/train.csv&quot;) head(train,5) ## id Gender Age Driving_License Region_Code Previously_Insured Vehicle_Age Vehicle_Damage Annual_Premium ## 1 1 Male 44 1 28 0 &gt; 2 Years Yes 40454 ## 2 2 Male 76 1 3 0 1-2 Year No 33536 ## 3 3 Male 47 1 28 0 &gt; 2 Years Yes 38294 ## 4 4 Male 21 1 11 1 &lt; 1 Year No 28619 ## 5 5 Female 29 1 41 1 &lt; 1 Year No 27496 ## Policy_Sales_Channel Vintage Response ## 1 26 217 1 ## 2 26 183 0 ## 3 26 27 1 ## 4 152 203 0 ## 5 152 39 0 En el capítulo anterior ya aparecieron posibles problemas, no sólo de datos, también cuestiones de negocio a las que llegamos tras un análisis estadístico sencillo, por ese motivo se insiste en la idea de dar importancia al problema de negocio que se está abordando. El primer paso es conocer el número de observaciones y como son las variables del conjunto de datos, esto nos permite saber si es posible prescindir de algunas (observaciones y/o variables). Con DataExplorer se comenzará con la función introduce. library(DataExplorer) introduce(train) ## rows columns discrete_columns continuous_columns all_missing_columns total_missing_values complete_rows ## 1 381109 12 3 9 0 0 381109 ## total_observations memory_usage ## 1 4573308 27443752 Este análisis se puede acompañar de una visión gráfica. plot_intro(train) El conjunto de datos tiene 381109 observaciones y 12 variables donde un 25% son factores frente al 75% de numéricas, todos las columnas están completas y no hay observaciones sin datos por lo que no es necesaria una limpieza previa. Observemos a continuación que variables tiene el conjunto de datos. plot_str(train) Se observa que hay variables como Previously_Insured, Region_Code o Policy_Sales_Channel que son numéricas y serían susceptibles de ser factores. Los propios nombres de variables y sus descripciones tienen que ser de ayuda en estos primeros acercamientos. El siguiente paso es describir las variables en función del tipo. Si la variable es numérica se recomendaba la realización de histogramas, con plot_histogram se automatiza esta labor. plot_histogram(train, ncol = 3) Se disponen de 6 variables numéricas dispuestas en tres columnas con ncol=3 columnas. En este punto el científico de datos ya debe empezar a describir comportamientos buscando ese sentido de negocio. Veamos variable por variable: Age Es la edad del encuestado, la moda en 25 años aproximadamente con otro repunte en torno a los 45. Annual_Premium Prima anual concentrada en valores bajos con algún registro con valor superior a 40000 . Id Identificador de cliente poco o nada puede aportar a la resolución de un problema con los datos. Policy_Sales_Channel Canal de venta del contrato, no tiene sentido su histograma, ha de ser tratada como un factor. Region_Code Es un código de región ha de ser tratada como un factor. Vintage Antigüedad del contrato, no tiene una forma definida, de hecho sus posibles valores se parecen mucho al campo identificativo. El primer acercamiento va buscando ese sentido y pretende utilizar dialéctica propia de negocio estudiando las formas, los posibles valores que toman las observaciones y las variables. Sin embargo, en este primer paso ya encontramos acciones y modificaciones interesantes. train &lt;- train %&gt;% mutate(Policy_Sales_Channel = as.character(Policy_Sales_Channel), Region_Code = as.character(Region_Code)) Las siguientes variables a analizar serán los factores, en el capítulo 7 se sugirió el uso de gráficos de barras, en DataExplorer se emplea la función plot_bar. plot_bar(train) ## 2 columns ignored with more than 50 categories. ## Region_Code: 53 categories ## Policy_Sales_Channel: 155 categories En este caso DataExplorer nos arroja un warning que da una pista sobre un problema que tiene nuestro conjunto de datos de trabajo: 2 columns ignored with more than 50 categories. Region_Code: 53 categories Policy_Sales_Channel: 155 categories Hay dos columnas que se considera innecesario representar, justo los factores que se acaban de crear. Estas variables son susceptibles de ser agrupadas pero no han de ser tratadas como numéricas. En cuanto al resto de variables: Gender Describe el sexo del encuestado, es un factor con dos niveles. Vehicle_Age Esta variable define la antigüedad del vehículo, se ha agrupado en 3 niveles de un factor y por eso no es numérica pero es necesario observar el orden. Es un factor y el EDA ya nos advierte un posible problema, el orden, &lt;1 1-2 y &gt;2. En este caso toma el orden lexicográfico pone en primer lugar el nivel 1-2 Year y ha de estar en segundo lugar. Vehicle_Damage Indica si el vehículo tiene daños, a priori no parece presentar resultado anómalo. Driving_License Indica si se dispone o no de carnet, apenas hay observaciones sin carnet, ¿tiene sentido ofrecer un seguro de automóviles a clientes sin carnet? Esos registros son susceptibles de ser eliminados. Previously_Insured Es una variable numérica pero al tomar 2 valores se interpreta como factor. Response Será la variable más relevante de nuestro conjunto de datos, toma valores 1/0 Si/No. A continuación se resuelven algunos de los problemas planteados. Se empieza por la variable Vehicle_Age, para reordenar factores en R se cuenta con la función factor, se sugiere hacer: table(train$Vehicle_Age) ## ## &lt; 1 Year &gt; 2 Years 1-2 Year ## 164786 16007 200316 train &lt;- train %&gt;% mutate(Vehicle_Age=factor(Vehicle_Age, c(&#39;&lt; 1 Year&#39;,&#39;1-2 Year&#39;,&#39;&gt; 2 Years&#39;))) table(train$Vehicle_Age) ## ## &lt; 1 Year 1-2 Year &gt; 2 Years ## 164786 200316 16007 Sin embargo, en este punto se plantea una recomendación. Cuando se trabaja con datos en bruto y estos datos son analizados y clasificados es bueno crear una nueva variable y jugar con prefijos para determinar si esa variable está trabajada o no. En este caso, tratándose de un factor, se emplea el prefijo fr_ de factor reclasificado, en el ejemplo que se está trabajando: train &lt;- train %&gt;% mutate(fr_vehicle_age = factor(Vehicle_Age, c(&#39;&lt; 1 Year&#39;,&#39;1-2 Year&#39;,&#39;&gt; 2 Years&#39;))) train %&gt;% group_by(fr_vehicle_age) %&gt;% summarise(conteo=n()) %&gt;% ggplot(aes(x=fr_vehicle_age, y=conteo)) + geom_bar(stat = &#39;identity&#39;) Con la variable Driving_License se plantean dos posibles soluciones, eliminar esos pocos registros sin licencia de conducción o directamente ignorar la variable. De momento no se eliminan esos registros. El resto de factores que a priori no parecen presentar problema se renombran. train &lt;- train %&gt;% rename( fr_gender = Gender, fr_vehicle_damage = Vehicle_Damage, fr_previouly_insured = Previously_Insured) Quedan pendientes los dos factores con un gran número de niveles. En el caso de Region_Code se sabe que los datos provienen de una aseguradora española y se tienen 52 códigos, es evidente que hace mención a las provincias españolas: train %&gt;% group_by(Region_Code=as.factor(Region_Code)) %&gt;% summarise(conteo=n()) %&gt;% ggplot(aes(x=reorder(Region_Code,conteo), y =conteo)) + geom_bar(stat = &#39;Identity&#39;) + coord_flip() Se observa que los niveles 28 y 8 son los que más encuestados tienen, en el caso de las provincias españolas hacen mención a Madrid y Barcelona. En este caso se puede sugerir una agrupación con los 10 niveles más representativos y un resto. provincias &lt;- train %&gt;% group_by(Region_Code) %&gt;% summarise(conteo=n()) %&gt;% arrange(desc(conteo)) %&gt;% mutate(fr_region_code = case_when( row_number() &lt;= 10 ~ Region_Code, TRUE ~ &#39;Resto&#39;)) train &lt;- train %&gt;% left_join(provincias) ## Joining, by = &quot;Region_Code&quot; Otra posible agrupación tendría ese sentido de negocio provincial train &lt;- train %&gt;% mutate(fr_region_code2 = case_when( Region_Code == 28 ~ &#39;Madrid&#39;, Region_Code == 8 ~ &#39;Barcelona&#39;, TRUE ~ &#39;Resto&#39;)) Sin embargo, la mejor agrupación será aquella que una provincias con similar comportamiento ante un problema o ante la respuesta de un problema. En la misma situación está la variable Policy_Sales_Channel donde no tenemos ese conocimiento de negocio necesario para proponer una agrupación de niveles del factor. En capítulos posteriores se retomará este problema y se propondrá una solución. El análisis EDA permite al científico de datos empezar el trabajo con los datos, pero esos datos se quieren emplear para resolver un problema y de momento sólo se está en una fase descriptiva. En cuanto a la librería DataExplorer además de tener diversas funciones para realizar estos descriptivos tiene la posibilidad de automatizar un reporte con esa aproximación mediante la función create_report. En cualquier caso y reiterando comentarios anteriores, se recomienda que el analista disponga de sus propias herramientas. "],["probabilidad-distribuciones.html", "Capítulo 10 Probabilidad y distribuciones 10.1 Conceptos básicos de probabilidad 10.2 Distribución de un evento 10.3 Distribuciones discretas 10.4 Distribuciones continuas", " Capítulo 10 Probabilidad y distribuciones En este punto es necesario realizar un paréntesis en el hilo conductor del ensayo. Hasta el momento se han descrito variables, limpiado y depurado datos y se hizo mención al rol que desempeña cada variable en el conjunto de datos. Entre esas variables hay una de ellas que juega un rol fundamental para el científico de datos, la variable target o variable respuesta. Si no existe esa variable el científico de datos se enfrenta a un análisis no dirigido, no conoce como es el problema que representan las variables y las observaciones. Este trabajo se centra en el caso contrario, el conjunto de datos recoge la variable target o recoge las variables en bruto necesarias para crearla y esa variable dirige el tipo de análisis. ¿Por qué el científico de datos ha de tener conocimientos de probabilidad y distribuciones? Porque esa variable target define un evento que puede tener una probabilidad o función de probabilidad asociada y por lo tanto tiene una distribución. Ese evento ha de presentar unos valores que, representados gráficamente, tengan una forma conocida. ¿Por qué ha de ser una forma conocida? Porque la forma de la variable, los posibles valores que toma la variable respuesta, la distribución de la variable que da respuesta a mi problema define el tipo de análisis. La siguiente imagen resume algunos de los problemas/análisis/distribuciones con su correspondiente variable respuesta. Son problemas habituales a los que se puede enfrentar un científico de datos. Si no hay variable respuesta el análisis no está dirigido o supervisado por ninguna variable target. Si un evento no tiene una forma o una distribución conocida el científico de datos puede transformar los valores para que lo sea. Es habitual trabajar con eventos Si/No - 1/0. Puede resultar más práctico y más sencillo analizar qué clientes tienen saldo en un fondo de inversión que analizar el propio saldo. 10.1 Conceptos básicos de probabilidad A continuación se enumeran una serie de conceptos importantes que irán articulando el modo en el que se plantea un análisis. Un fenómeno aleatorio es aquel que no se puede predecir pero que sin embargo conocemos todos los posibles resultados (Ej: Lotería, lanzar un dado,). Este suceso es contrario a los fenómenos deterministas donde conocemos todos los factores del experimento y sabemos de antemano el resultado (Ej: experimentos en laboratorios). El espacio muestral es aquel conjunto que recoge todos los posibles resultados de un experimento aleatorio. (Lanzar un dado: 1,,6). Se llama evento a cualquier subconjunto de este espacio muestral. (Sacar un 5 al tirar el dado). Hay eventos generados por variables aleatorias que se consideran funciones matemáticas que denotadas con letras X, Y, etc. X puede ser (por ejemplo) el número de 5s al tirar el dado 5 veces, el número de siniestros que puede tener un riesgo en un periodo de tiempo o el pasivo estimado de un cliente fuera de nuestra entidad bancaria, en este último caso pueden interesarnos los clientes con un pasivo estimado superior a 30.000  fuera de la entidad para que el evento sólo tenga dos posibles valores si/no. Probabilidad consiste en asignar a cada evento del espacio muestral un número entre 0 y 1 (no un porcentaje), pero más importante que su definición son las características de la probabilidad: La probabilidad de cualquier evento siempre es mayor o igual a 0. La probabilidad de que se produzca al menos uno de los eventos del espacio muestral es 1. La probabilidad de los eventos es mutuamente excluyentes, lo que nos permite aseverar que la probabilidad de que suceda un evento u otro evento es la suma de las probabilidades. La probabilidad de sacar un 3 con un dado es 1/6, la probabilidad de sacar un 3 o un 5 es 2/6. Además hay ocasiones en las que la probabilidad de un evento está condicionada a que suceda otro evento a este hecho se le denomina probabilidad condicionada. En el caso de que un evento no influya en otro estamos ante sucesos independientes. A la probabilidad de que suceda un evento A cuando sucede un evento B se le denomina probabilidad de A condicionada a B. Cuando un suceso B no influye en la probabilidad de A se dice que A y B son sucesos independientes. 10.2 Distribución de un evento En pocas líneas se han planteado los conceptos básicos que van a describir la distribución de una variable. A lo largo del ensayo siempre se hace mención a la distribución como la forma que tienen los valores, formalmente esta distribución es una función matemática que asigna una probabilidad de ocurrencia a cada valor conocido que toma la variable. Si la variable es discreta las probabilidades serán función de la pertenencia a cada grupo de la variable discreta, en el caso de las variables continuas esa función de probabilidad se dispone de funciones de densidad y obtener la probabilidad de un suceso sería el área bajo la función de densidad. Esta diferencia se puede apreciar en dos variables del conjunto de datos de trabajo. library(tidyverse) if(!require(gridExtra)) install.packages(&quot;gridExtra&quot;) train &lt;- read.csv(&quot;./data/train.csv&quot;) c1 = train %&gt;% ggplot(aes(x=Vehicle_Age)) + geom_bar() +ggtitle(&quot;Valores discretos&quot;) c2 = train %&gt;% ggplot(aes(x=Age)) + geom_density() +ggtitle(&quot;Valores continuos&quot;) grid.arrange(c1, c2, ncol=2) En el caso de los valores discretos la probabilidad de pertenecer a alguno de los grupos de Vehicle_Age será: train %&gt;% group_by(Vehicle_Age) %&gt;% summarise(pct= round(n()/nrow(train),3)) ## # A tibble: 3 x 2 ## Vehicle_Age pct ## &lt;chr&gt; &lt;dbl&gt; ## 1 &lt; 1 Year 0.432 ## 2 &gt; 2 Years 0.042 ## 3 1-2 Year 0.526 En esta situación la probabilidad coincide con el porcentaje de observaciones, de ahí la tendencia a transformar una probabilidad en un porcentaje, en la medida de lo posible el científico de datos debe evitar esa transformación. De igual modo se puede plantear para la variable Age ya que es una variable peculiar donde la línea entre la cantidad o la cualidad con orden es muy fina. Si se plantea como numérica es necesario recordar que una integral calcula el área bajo una curva en un determinado intervalo, por lo que se puede obtener el área que hay por debajo de un valor hasta el siguiente. Por ejemplo, la probabilidad de tener 50 años o más en la población de trabajo, en la encuesta realizada, es: train %&gt;% ggplot(aes(x=Age)) + geom_density() + ggtitle(&quot;Probabilidad de tener más de 50 años en los datos&quot;) + geom_vline(xintercept = 50, color = &quot;red&quot;) + geom_segment(aes(x = 80, y = 0.025, xend = 64, yend = 0.005), arrow = arrow(length = unit(0.5, &quot;cm&quot;)), color=&#39;red&#39;) # Es necesario obtener la función de densidad densidad &lt;- density(train$Age) integrate(approxfun(densidad), lower=50, upper=max(train$Age)) ## 0.2412278 with absolute error &lt; 4.9e-06 Lo que queda por debajo de la curva de densidad, donde apunta la flecha creada con geom_segment, es la probabilidad de tener 50 años o más en los datos de la población en estudio. Se puede hilar un argumento del siguiente modo: 1. Una variable numérica se describe mediante un histograma o una gráfica de densidad que es una función continua 2. Esa función continua recoge el total de observaciones de la muestra 3. El área que deja por debajo de la función nos devuelve la probabilidad de un intervalo de valores de la muestra Si se obtiene la probabilidad de tener 44 años en el conjunto de datos de trabajo: integrate(approxfun(densidad), lower=44, upper=45) ## 0.02153075 with absolute error &lt; 3.6e-07 Esta probabilidad es muy similar a la probabilidad si se considera la variable como un factor: train %&gt;% group_by(Age) %&gt;% summarise(pct=round(n()/nrow(train),3)) %&gt;% filter(Age==44) ## # A tibble: 1 x 2 ## Age pct ## &lt;int&gt; &lt;dbl&gt; ## 1 44 0.022 En este caso la variable Age tiene una forma que describe su función de densidad, pero se ha hecho mención a que la forma de la variable ha de ser conocida para facilitar la labor al científico de datos. 10.2.1 Distribuciones conocidas En capítulos anteriores se distinguieron dos tipos de variables, variables discretas o variables numéricas por lo que las distribuciones conocidas son discretas o continuas. Dentro de las continuas se tienen distribuciones naturales o artificiales, estas últimas muy relevantes para la realización de modelos matemáticos y que se tratarán cuando se hable de inferencia. Hay múltiples distribuciones, en R se emplea help(\"Distributions\") para presentar que distribuciones disponibles en la base de R. De todas ellas se considera que el científico de datos debe conocer: Distribuciones discretas Bernoulli Binomial Poisson Distribuciones continuas Naturales Uniforme Normal Gamma Artificiales (se verán en capítulos posteriores) T-Student F-Snedecor Chi-Cuadrado Existen más distribuciones de especial relevancia, pero en una primera aproximación se consideran estas como las fundamentales. A continuación se analizan una a una y se emplea su descripción para introducir al lector a la simulación de datos con R, aunque no sea el objetivo del ensayo, puede ayudar a la comprensión de estas distribuciones. 10.2.2 Obtención de probabilidades con R R siempre trabaja con 3 elementos de cálculo: d obtiene la función de densidad de la distribución seleccionada p obtiene la función de probabilidad acumulada q obtiene el cuartil de la inversa de la función 10.2.3 Generación de números aleatorios con R Para generar números aleatorios que permitan crear simulaciones en R siempre se emplea el prefijo r seguido del nombre de la distribución, estas funciones requieren el número de eventos size y los parámetros asociados a cada distribución. El parámetro de una distribución de probabilidad define como son los valores, cada una de ellas tiene un parámetro que puede ser una medida de posición, dispersión o de forma. A continuación se describen las principales funciones, sus parámetros asociados y se ejemplifica la obtención de probabilidades y simulación de observaciones. 10.3 Distribuciones discretas 10.3.1 Distribución de Bernoulli Es el principio de todo, ejemplo: se tiene una pregunta tipo test y 3 posibles respuestas, sólo una es la correcta, si fuera necesario definir esa función se podría expresar: Bernoulli(Probabilidad=0.0.33) es la función más sencilla y sólo requiere un parámetro probabilidad_de_acertar. Sin embargo, lo más habitual cuando se realizan test es responder varias preguntas. 10.3.2 Distribución binomial Se realizan n pruebas de Bernoulli independientes, por ejemplo 10 preguntas tipo test con tres posibles respuestas donde sólo una es la correcta y se responden de forma aleatoria. El problema parte de una distribución binomial B con parámetros n = 10 preguntas, p = 0.33 de probabilidad de acertar (1 correcta de 3) estos posibles valores arrojan una distribución B(n=10, p=0.33) En ese punto comienzan las cuestiones a plantear sobre probabilidades. De 10 preguntas acertando en 5 se obtiene un aprobado, la función de densidad de una distribución B(10,0.33) se puede obtener en R con dbinom(x, size, prob) donde x es el evento, sacar un 5, size es el número de preguntas del test, en este caso 10 y prob es la probabilidad de éxito. Si se responde al azar en el test la probabilidad de sacar un 5 es: dbinom(x = 5, size = 10, prob = 0.33) ## [1] 0.1331509 Pero esta probabilidad de sacar un 5 no es la probabilidad de aprobar, el que responda y saque un 6 también aprueba se recuerda que son eventos mutuamente excluyentes por lo que la probabilidad de aprobar, la probabilidad de sacar 5 o mayor es: dbinom(x = 5, size = 10, prob = 0.33) + dbinom(x = 6, size = 10, prob = 0.33) + dbinom(x = 7, size = 10, prob = 0.33) + dbinom(x = 8, size = 10, prob = 0.33) + dbinom(x = 9, size = 10, prob = 0.33) + dbinom(x = 10, size = 10, prob = 0.33) ## [1] 0.2063514 Respondiendo al azar en un 20% de las ocasiones se aprobará. Se están acumulando resultados por ello es mejor emplear: sum(dbinom(x = 5:10, size = 10, prob = 0.33)) ## [1] 0.2063514 Mediante pbinom se obtiene el área que hay por debajo de la función de distribución acumulada hasta un valor q, en el ejemplo de trabajo, ¿cuál es la probabilidad de suspender o de sacar hasta un 4? pbinom(q = 4, size = 10, prob = 0.33) ## [1] 0.7936486 Como es la acumulación lo que quede a la derecha de esa función acumulada será la probabilidad de aprobar, el suceso contrario a suspender. 1 - pbinom(q = 4, size = 10, prob = 0.33) ## [1] 0.2063514 Se están planteando ejemplos con valores, pero es posible que interese conocer la puntuación obtenida en función de un percentil, para ello se emplea qbinom que ofrece la inversa de la función acumulada de densidad. qbinom(p = 0.7, size = 10, prob = 0.33) ## [1] 4 qbinom(p = 0.8, size = 10, prob = 0.33) ## [1] 5 A partir del percentil 80 ya empezarían a aparecer aprobados, pero hasta ese punto no. Sin embargo, puede resultar más sencillo para el científico de datos simular los resultados de forma que sean tangibles y poder obtener de ellos las probabilidades deseadas. Para la realización de simulaciones de una función binomial se emplea rbinom. Un test: set.seed(12) rbinom(n = 1, size = 10, prob = 0.33) ## [1] 1 Mediante la función set.seed(12) se garantiza que el resultado de la simulación siempre es el mismo porque se establece la semilla para la obtención de números aleatorios, en este caso se suspende el único test realizado. Realizando 1000 test y calculando la media o la proporción de casos en las que se supera el 5: numero_test = 1000 mean(rbinom(numero_test, 10, 0.33)&gt;=5) ## [1] 0.195 El resultado obtenido vía simulación es similar al que devuelve la función de densidad. Estos resultados se pueden ver gráficamente: numero_test = 10 data.frame(examen = 0:numero_test, prob = dbinom(x = 0:numero_test, size = 10, prob = 0.33)) %&gt;% mutate(resultado = ifelse(examen &gt;=5, &quot;Aprueba&quot;, &quot;Suspende&quot;)) %&gt;% ggplot(aes(x = factor(examen), y = prob, fill = resultado )) + geom_col() Es evidente que es mejor estudiar. Pero en el gráfico se observa que la media está sobre el 3 ya que en la distribución binomial tiene como media n * prob_exito y como varianza n * prob_exito * prob_fracaso. 10.3.3 Distribución de Poisson Los sucesos que se distribuyen según una distribución de Poisson aparecen cuando se cuentan eventos en un determinado espacio de tiempo. Ejemplos donde es posible encontrarse una distribución de poisson como variable respuesta pueden ser: Número de llamadas a un call center. Número de personas en espera. Número de siniestros en un seguro de autos en un año. En esta situación se tiene una distribución de poisson con parámetro  = media de eventos. Tiene una característica fundamental, la media es igual que la varianza. Hay un resultado interesante cuando se simulan valores que siguen una distribución de poisson porque se puede identificar el número de veces que se produce un fenómeno con una probabilidad baja de ocurrir. Para simular valores que siguen una distribución de poisson se emplea rpois(size, lambda) donde lamdaes tanto la media como la desviación típica: simulaciones = 1000 data.frame(resultado = rpois(simulaciones, lambda = 0.15)) %&gt;% ggplot(aes(x=as.factor(resultado))) + geom_bar() Un  = 0.15 hace que casi un 15% de las observaciones tengan un único evento, pero eso no exime de que se pueda producir más de uno, esta forma aparece, por ejemplo, cuando se trabajan con frecuencias siniestrales en el ámbito actuarial ya que se define como el número de eventos (siniestros) en un periodo de tiempo (365 días). Si el parámetro  se sitúa en un valores más altos: simulaciones = 1000 data.frame(resultado = rpois(simulaciones, 20)) %&gt;% ggplot(aes(x=as.factor(resultado))) + geom_bar() La forma de esta distribución es prácticamente triangular una característica que se produce cuando la media es igual que la varianza. En esta distribución, ¿cuál es la probabilidad de obtener un 10? dpois(x=10, lambda = 100 ) ## [1] 1.025153e-30 La probabilidad es ínfima como se puede comprobar. Esta distribución además de ser utilizada en el mundo actuarial es útil cuando se buscan observaciones extremas como se ha señalado con anterioridad: clientes &lt;- 100 situacion1 &lt;- data.frame(numero=rpois(clientes,1)); situacion1$situacion = &quot;1 cliente cada 30 minutos&quot; situacion2 &lt;- data.frame(numero=rpois(clientes,5)); situacion2$situacion = &quot;5 clientes cada 30 minutos&quot; situacion3 &lt;- data.frame(numero=rpois(clientes,10)); situacion3$situacion = &quot;10 clientes cada 30 minutos&quot; simulacion &lt;- rbind.data.frame(situacion1, situacion2, situacion3) ggplot(simulacion) + geom_bar(aes(x=numero, fill=situacion), position = position_dodge(preserve = &#39;single&#39;)) Si a un comercio acuden 10 clientes cada 30 minutos es posible que en algún momento se tengan hasta 20 clientes en el comercio, se plantean hipótesis en base al número de dependientes. Se aprecia que es relevante en teoría de colas o líneas de espera para dimensionar servicios de atención al cliente. 10.4 Distribuciones continuas 10.4.1 Distribución uniforme El resultado se distribuye de una forma uniforme cuando la probabilidad el evento es la misma para todos los resultados (Ej: probabilidad de un número en un dado es 1/6, probabilidad de un número de lotería 1/número de boletos a la venta). Estos modelos se aplican fundamentalmente en juegos de azar, en este caso la distribución no tiene un parámetro asociado que la defina pero son conocidos los posibles resultados del experimento. Un paseo aleatorio con runif(size,min,max) por fecha: longitud = 100 dia = seq(as.Date(&quot;2020/10/01&quot;), by = &quot;day&quot;, length.out = longitud) dato_aleatorio = runif(longitud, min = 0, max = 1) df &lt;- cbind.data.frame(dia, dato_aleatorio) ggplot(df, aes(x=dia,y=dato_aleatorio)) + geom_line() De 0 a 1 el resultado es equiprobable, luego la probabilidad de obtener un dato mayor de 0.2 en un paseo aleatorio será 0.2. longitud=1000 data.frame(dato_aleatorio = runif(longitud, min = 0, max = 1)) %&gt;% mutate(prob = ifelse(dato_aleatorio&gt;=0.2, &quot;si&quot; ,&quot;no&quot;)) %&gt;% ggplot(aes(x=dato_aleatorio, color= prob)) + geom_histogram(bins = 50) Dada la equiprobabilidad de la distribución uniforme sería posible simular pi si se tienen puntos distribuidos aleatoriamente en el espacio de dos dimensiones. x=runif(1000,-1,1) y=runif(1000,-1,1) simul &lt;- data.frame(cbind(x,y)) Todo valor que esté a una distancia mayor de 1 del centro estará fuera de la circunferencia: simul$dist=sqrt(x**2+y**2) simul$ok=as.factor((simul$dist&lt;=1)*1) ggplot(simul, aes(x=x, y=y, color=ok)) + geom_point() + theme_classic() La proporción de casos que caen dentro del círculo será el área, como se trata de un círculo de radio 2 se puede despejar \\(\\Pi\\) como el área por 4: (nrow(subset(simul,ok==&quot;1&quot;))/nrow(simul)) * 4 ## [1] 3.04 Como se ha comentado, la lotería sigue una distribución uniforme. En 1000 semanas jugando, ¿puede tocar el número 10976? semanas = 1000 sorteos &lt;- data.frame(numero=as.integer(runif(semanas,0,99999))) sum(sorteos$numero == 10976) ## [1] 0 Sin embargo, ¿qué sucede con el reintegro? sorteos$terminacion &lt;- sorteos$numero %% 10 sum(sorteos$terminacion == 6) ## [1] 97 De este modo es posible medir la inversión realizada en lotería: sum(sorteos$numero == 10976) * 20 * 1000 + sum(sorteos$terminacion == 6) * 20 - semanas * 20 ## [1] -18060 No parece una inversión muy rentable, con datos estadísticos no es posible saber el número de la lotería a priori, pero si se puede determinar las posibilidades de obtenerlo y transformar esas posibilidades en un resultado económico. 10.4.2 Distribución normal Se tiene una función continua con un punto medio muy probable y conforme se alejan los datos de ese punto medio tanto por la derecha como por la izquierda los eventos pasan a ser menos probables, en este caso se tiene una distribución normal o gaussiana. Es adecuada cuando hay muchas observaciones y cambios pequeños de forma aditiva (suman o restan), cambios lineales. Si los efectos son multiplicativos sería otro tipo de distribución. Su media se denomina µ y su desviación típica  y estos serán los parámetros que definan a la distribución normal. Tiene una serie de propiedades relevantes: Tiene una única moda, que coincide con su media y su mediana. La curva normal es asintótica al eje de abscisas, por ello, cualquier valor de los conocidos es teóricamente posible. El área total bajo la curva es igual a 1. Es simétrica con respecto a su media. Para este tipo de variables existe una probabilidad de un 50% de observar un dato mayor que la media, y un 50% de observar un dato menor. La distancia entre la línea trazada en la media y el punto de inflexión de la curva es igual a una desviación típica. Cuanto mayor sea, cuanto mayor dispersión haya, más aplanada será la curva de la densidad. El área bajo la curva comprendida entre los valores situados aproximadamente a dos desviaciones estándar de la media es igual a 0.95. En concreto, existe un 95% de posibilidades de observar un valor comprendido dentro de ese intervalo. Se simula una población de 1000 individuos con una altura media de 174 cm y una desviación de 7.6 cm, se agrupan las alturas simuladas en función del número de desviaciones y se contabilizan cuantos individuos de la población caen en cada grupo de desviaciones: altura_media &lt;- 174 desviacion &lt;- 7.6 alturas &lt;- data.frame(altura=rnorm(n = 1000, mean = altura_media , sd = desviacion)) alturas &lt;- alturas %&gt;% mutate(tipo = case_when( altura &gt;= altura_media - desviacion &amp; altura &lt;= altura_media + desviacion ~ &#39;1 desviación&#39;, altura &gt;= altura_media - desviacion*2 &amp; altura &lt;= altura_media + desviacion*2 ~ &#39;2 desviaciones&#39;, altura &gt;= altura_media - desviacion*3 &amp; altura &lt;= altura_media + desviacion*3 ~ &#39;3 desviaciones&#39;, TRUE ~ &#39;Más de 3 desviaciones&#39; )) alturas %&gt;% group_by(tipo) %&gt;% summarise(pct_personas = n()/nrow(alturas)) %&gt;% mutate(pct_acumulado = cumsum(pct_personas)) ## # A tibble: 4 x 3 ## tipo pct_personas pct_acumulado ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 desviación 0.664 0.664 ## 2 2 desviaciones 0.297 0.961 ## 3 3 desviaciones 0.037 0.998 ## 4 Más de 3 desviaciones 0.002 1 En el primer grupo de desviación están aproximadamente el 68% de las personas, entre las 2 desviaciones están el 95% de las personas (aproximadamente) y en 3 desviaciones típicas deben estar el 99.7%. Gráficamente. ggplot(alturas, aes(x = altura )) + geom_density() + geom_vline(xintercept=altura_media-desviacion, color = &#39;blue&#39;) + geom_vline(xintercept=altura_media+desviacion, color = &#39;blue&#39;) + geom_vline(xintercept=altura_media-desviacion*2, color = &#39;red&#39;)+ geom_vline(xintercept=altura_media+desviacion*2, color = &#39;red&#39;) Entre las barras azules hay una desviación típica de la población, entre las barras rojas hay 2 desviaciones de la población, el 95% de la población en estudio. Es decir, si algo se distribuye normalmente el 95% de las ocasiones su valor está entre +/- 2 desviaciones típicas, se puede construir un intervalo. Se volverá sobre los resultados que ofrece esta distribución cuando se trabaje la inferencia estadística. 10.4.3 Distribución gamma Es una distribución que viene definida por dos parámetros uno de forma y otro de escala y solo se estudian eventos con datos positivos &gt; 0 lo que suele implicar una forma asimétrica a la derecha. Será una distribución especialmente útil cuando nuestros datos tengan una longitud muy larga hacia la derecha, por ejemplo cuanto se estudien importes o duraciones. Cuando su parámetro de forma supere el 5 la distribución gamma tiene un gran parecido con la distribución normal. La distribución gamma tiene dos casos especiales la distribución exponencial y la distribución chi cuadrado. La segunda de ellas es una de las distribuciones que se han denominado artificiales y se analizará en capítulos posteriores. Para ver como se modifica la distribución en base a su parámetro se plantea la siguiente simulación: simulaciones = 10000 p1 = data.frame(gamma = rgamma(simulaciones, shape = 1, rate = 0.01)) %&gt;% ggplot(aes(x=gamma)) + geom_density() + ggtitle(&quot;Shape = 1 rate=0.01&quot;) p2 = data.frame(gamma = rgamma(simulaciones, shape = 1, rate = 0.1)) %&gt;% ggplot(aes(x=gamma)) + geom_density() + ggtitle(&quot;Shape = 1 rate=0.1&quot;) p3 = data.frame(gamma = rgamma(simulaciones, shape = 1, rate = 1)) %&gt;% ggplot(aes(x=gamma)) + geom_density() + ggtitle(&quot;Shape = 1 rate=1&quot;) p4 = data.frame(gamma = rgamma(simulaciones, shape = 2, rate = 0.1)) %&gt;% ggplot(aes(x=gamma)) + geom_density() + ggtitle(&quot;Shape = 2 rate=0.1&quot;) p5 = data.frame(gamma = rgamma(simulaciones, shape = 3, rate = 0.1)) %&gt;% ggplot(aes(x=gamma)) + geom_density() + ggtitle(&quot;Shape = 2 rate=0.1&quot;) p6 = data.frame(gamma = rgamma(simulaciones, shape = 6, rate = 0.1)) %&gt;% ggplot(aes(x=gamma)) + geom_density() + ggtitle(&quot;Shape = 6 rate=0.1&quot;) grid.arrange(p1,p2,p3,p4,p5,p6, ncol=2) Se observa como el parámetro de escala afecta al valor, no a la forma que permite mover ese pico de densidad hacia la derecha, cuando el parámetro de forma supera el 5 hay una aproximación a la distribución normal. Como se puede ver en el código anterior para simular datos con esta distribución está la función rgamma(size, shape, rate, scale=1/rate) Esta distribución aparece habitualmente cuando se tienen importes. El siguiente programa carga un data frame que se denomina moto y que contiene los datos de siniestralidad de una cartera de seguros de motocicletas. varib &lt;- c(edad = 2L, sexo = 1L, zona = 1L, clase_moto = 1L, antveh = 2L, bonus = 1L, exposicion = 8L, nsin = 4L, impsin = 8L) varib.classes &lt;- c(&quot;integer&quot;, rep(&quot;factor&quot;, 3), &quot;integer&quot;, &quot;factor&quot;, &quot;numeric&quot;, rep(&quot;integer&quot;, 2)) con &lt;- url(&quot;http://staff.math.su.se/esbj/GLMbook/mccase.txt&quot;) moto &lt;- read.fwf(con, widths = varib, header = FALSE, col.names = names(varib), colClasses = varib.classes, na.strings = NULL, comment.char = &quot;&quot;) ## Warning in readLines(file, n = thisblock): incomplete final line found on &#39;http://staff.math.su.se/esbj/ ## GLMbook/mccase.txt&#39; El campo impsin tiene el importe de los siniestros de la cartera, si se traza un gráfico de densidad: ggplot(moto, aes(x=impsin)) + geom_density() Se observa como es una distribución completamente asimétrica, demasiado, ya que el importe siniestral se aleja mucho en algún caso, en estas situaciones se recomienda realizar un corte para conocer mejor la forma de la distribución: moto &lt;- moto %&gt;% mutate(impsin2 = ifelse(impsin&gt;3000, 3000, impsin)) ggplot(moto, aes(x=impsin2)) + geom_density() Aunque se trabaje con un gran número de observaciones en el caso de realizar un proceso de modelización es mejor conocer que estos datos se distribuyen según una distribución gamma, no todos los eventos cuantitativos se distribuyen normalmente, esto es algo que ha te tener presente el científico de datos. "],["analisis-bivariable.html", "Capítulo 11 Análisis bivariable 11.1 Variable numérica frente a variable numérica 11.2 Factores frente a variables numéricas 11.3 Factores frente a factores 11.4 Propuesta de análisis gráfico para el análisis bivariable 11.5 Correlación entre factores. La V de Cramer 11.6 Agrupación de variables cuantitativas", " Capítulo 11 Análisis bivariable De nuevo se retoma el ejemplo que está sirviendo de hilo conductor para este ensayo, la campaña de marketing de venta cruzada en el sector asegurador que está disponible en Kaggle. Una aseguradora española que opera en múltiples ramos quiere ofrecer seguro de automóviles a sus clientes del ramo de salud. Para ello se realizó un cuestionario a los clientes de forma que se marcó quienes de ellos estarían interesados en el producto de automóviles y quienes no. Se identificaron posibles tareas: Describir la cartera de clientes. Identificar que características de nuestros clientes pueden ser eficaces a la hora de crear una campaña comercial. Sugerir unas reglas para la elaboración de la campaña. library(tidyverse) library(formattable) train &lt;- read.csv(&quot;./data/train.csv&quot;) formattable(head(train,5)) id Gender Age Driving_License Region_Code Previously_Insured Vehicle_Age Vehicle_Damage Annual_Premium Policy_Sales_Channel Vintage Response 1 Male 44 1 28 0 2 Years Yes 40454 26 217 1 2 Male 76 1 3 0 1-2 Year No 33536 26 183 0 3 Male 47 1 28 0 2 Years Yes 38294 26 27 1 4 Male 21 1 11 1 &lt; 1 Year No 28619 152 203 0 5 Female 29 1 41 1 &lt; 1 Year No 27496 152 39 0 Hasta el momento se ha descrito el conjunto de datos, se han determinado que roles juegan las variables dentro de ese conjunto de datos donde se estableció que la variable más relevante es Response ya que juega el rol de variable target. Esta variable toma valores 0 - no interesa 1 - interesa el producto, es decir, se distribuye según una binomial con parámetros n = 381.000 clientes y p = 0.1223 interesados, conocer la distribución ayuda a afrontar el análisis. formattable(train %&gt;% group_by(Response) %&gt;% summarise(clientes = n(),pct_clientes=n()/nrow(train))) Response clientes pct_clientes 0 334399 0.8774366 1 46710 0.1225634 En este capítulo el científico de datos comienza a estudiar la relación entre dos variables que pueden ser cuantitativas o factores. Esta categorización da lugar a 3 tipos de relaciones entre dos variables: Variable numérica frente a variable numérica Variable numérica frente a factor Factor frente a factor 11.1 Variable numérica frente a variable numérica El inicio de la relación entre dos variables es la correlación. Esta medida bivariable pretende medir como la variación de una variable cuantitativa influye en otra variable cuantitativa. La cuestión fundamental es, a medida que se incrementa x, ¿qué sucede con y? Por ejemplo, a medida que una persona crece en altura, ¿cómo se incrementa su peso? Para ilustrar se realiza una simulación y un gráfico de dispersión para estudiar como se distribuyen los pares de variables (peso, altura) personas &lt;- 100 altura &lt;- rnorm(personas, 170, 20) peso &lt;- altura/2.85 + rnorm(personas, 10, 5) data.frame(altura=altura, peso=peso) %&gt;% ggplot(aes(x = altura, y = peso)) + geom_point() Aunque sean datos simulados es evidente, por como se ha hecho la simulación, que a medida que aumenta la altura aumenta el peso. Hay una medida estadística para describir esta situación, el coeficiente de correlación que en R se calcula mediante la función cor. cor(peso, altura) ## [1] 0.8651548 Esta medida de 0.87 indica que peso y altura se relacionan positivamente, a medida que aumenta uno aumenta el otro pero no es una medida de graduación, sólo mide la fuerza de esa relación. El coeficiente de correlación es un valor que toma valores entre -1 y 1 donde -1 significa total correlación negativa (aumenta una implica que disminuye la otra variable), 0 significa que no existe ningún tipo de correlación y 1 la correlación positiva (aumenta una aumenta otra variable). Simulando y representando las distintas formas de correlación se tiene: library(gridExtra) # Correlación positiva observaciones &lt;- 1000 x &lt;- rnorm(observaciones, 0, 1) y &lt;- x/5 + rnorm(observaciones, 0, 0.1) p1 &lt;- data.frame(x,y) %&gt;% ggplot(aes(x , y)) + geom_point() + ggtitle(paste0(&quot;Correlación: &quot;, round(cor(x,y),3))) # Correlación negativa x &lt;- rnorm(observaciones, 0, 5) y &lt;- rpois(observaciones, 3) - x p2 &lt;- data.frame(x,y) %&gt;% ggplot(aes(x , y)) + geom_point() + ggtitle(paste0(&quot;Correlación: &quot;, round(cor(x,y),3))) # Sin correlación x &lt;- rnorm(observaciones, 0, 5) y &lt;- runif(observaciones) p3 &lt;- data.frame(x,y) %&gt;% ggplot(aes(x , y)) + geom_point() + ggtitle(paste0(&quot;Correlación: &quot;, round(cor(x,y),3))) # Aparentemente sin correlación x &lt;- rnorm(observaciones, 0, 1) y &lt;- x^2 + runif(observaciones, -1,1 ) p4 &lt;- data.frame(x,y) %&gt;% ggplot(aes(x , y)) + geom_point() + ggtitle(paste0(&quot;Correlación: &quot;, round(cor(x,y),3))) grid.arrange(p1,p2,p3,p4) Se simulan 4 tipos distintos de correlación, correlación positiva, negativa (vistas con anterioridad); en el tercer caso no hay correlación, cuando ésta no existe los gráficos de dispersión generan polígonos como cuadrados, rectángulos o circunferencias ya que la disposición de los pares de puntos se debe a un comportamiento azaroso (una variable no afecta a otra). Sin embargo, se ilustra otro tipo de situación, el cuarto gráfico, donde es evidente que hay una relación entre las variables pero ésta no es lineal y por ello el coeficiente de correlación no es suficiente para identificar esta situación. En el ejemplo de trabajo que hace de hilo conductor del ensayo no se realiza el análisis de correlaciones puesto que la variable principal para realizar un análisis bivariable es la variable target, la variable Response, que toma valores 0 si el cliente no está interesado en el seguro de automóviles y 1 si muestra interés en el producto. Por este motivo será más conveniente un análisis bivariable con un factor. 11.2 Factores frente a variables numéricas Una de las variables es un factor, cada nivel de ese factor representa una categoría, ya no es un punto. Será necesario estudiar como son los valores que toma la variable numérica para cada nivel del factor. En el capítulo 7, en la descripción gráfica de datos se estudiaban los posibles valores que toma una variable mediante histogramas, gráficos de densidades y boxplot. En este caso se puede hacer uno de esos análisis gráficos para cada nivel del factor y así comparar ambas variables. En el ejemplo de trabajo se tiene una variable target que toma valores 0 y 1, a continuación se estudia esa variable objetivo Response frente a la variable Age que indica la edad de los asegurados encuestados. train %&gt;% ggplot(aes(x=Age, group=as.factor(Response), fill=as.factor(Response))) + geom_histogram() Es necesario especificar que la variable es un factor, en este caso Response es numérica y a efectos prácticos se sigue manteniendo como numérica. Si no se especifica que es un factor se tendría la siguiente situación: train %&gt;% ggplot(aes(x=Age, group=Response, fill=Response)) + geom_histogram() No tiene sentido realizar una escala continua de un factor, es un problema habitual cuando se trabaja con factores en R. En cualquier caso, emplear un histograma de este tipo no sirve porque se están contando registros y si una de las categorías del factor tiene menos observaciones no se podrá comparar su comportamiento. Pero se dispone de los gráficos de densidad que permiten estudiar mediante una función continua la distribución de los valores. train %&gt;% ggplot(aes(x=Age, group=as.factor(Response), fill=as.factor(Response))) + geom_density() Se aprecian comportamientos distintos para la edad en la respuesta pero se sugiere jugar con la transparencia de los gráficos para conocer mejor las distribuciones, simplemente se añade el parámetro alpha. train %&gt;% ggplot(aes(x=Age, group=as.factor(Response), fill=as.factor(Response))) + geom_density(alpha = 0.3) Hay claramente dos distribuciones en función de la variable respuesta, estas distribuciones no son conocidas y tampoco es relevante porque es la distribución de la variable respuesta la que tiene importancia. La variable respuesta toma dos posibles valores y suponiendo cada cliente como independiente se tiene una distribución binomial como se ha indicado con anterioridad. Si el objetivo del ejercicio es establecer que características hacen que los clientes sean más propensos a adquirir un seguro de automóviles parece que la edad es una de esas características. A la vista del gráfico es más evidente que a mayor edad mayor interés, mayor probabilidad de adquirir un seguro de automóviles en la compañía. Cuando se tiene una variable respuesta es necesario estudiar esta variable frente a las variables que se consideren relevantes en su comportamiento, frente a todas las variables input. Por ejemplo, se repite el ejercicio contra Vintage que hace referencia a la antigüedad como cliente. train %&gt;% ggplot(aes(x=Vintage, group=as.factor(Response), fill=as.factor(Response))) + geom_density(alpha = 0.3) En este caso la forma de la variable tanto para los que afirman estar interesados en el seguro de automóviles como para aquellos que no lo están es prácticamente la misma. De forma gráfica no parece que la antigüedad como cliente esté afectando a la variable respuesta. Para este tipo de análisis también se puede emplear el boxplot. p1 &lt;- train %&gt;% ggplot(aes(x=Age, group=as.factor(Response), fill=as.factor(Response))) + geom_boxplot(alpha = 0.3) p2 &lt;- train %&gt;% ggplot(aes(x=Vintage, group=as.factor(Response), fill=as.factor(Response))) + geom_boxplot(alpha = 0.3) grid.arrange(p1,p2) Se aprecia como edades mayores muestran mayor propensión y como la antigüedad no está afectando al interés por el seguro de automóviles. Es importante reseñar que cualquier de los dos gráficos ofrece la forma, la distribución de las variables para cada nivel de la variable respuesta. Es necesario reseñar que en ningún momento aparece el número de observaciones ni se dispone de ningún mecanismo para identificar el peso que tiene cada grupo en estudio. Como se ha indicado este análisis es necesario llevarlo a cabo con todas las variables input pero no es posible emplearlo cuando se comparan factores frente a factores, en esa situación es necesario emplear otro análisis gráfico. 11.3 Factores frente a factores En el capítulo 7 los factores se estudiaban mediante gráficos de barras y en el análisis bivariable se puede seguir la misma tónica pero será necesario apilar en la barra los niveles del otro factor en estudio para poder comparar. Se recomienda apilar el factor que haga de variable respuesta. train %&gt;% group_by(Gender, target=as.factor(Response)) %&gt;% summarise(clientes=n()) %&gt;% ggplot(aes(x=Gender, y=clientes, fill=target)) + geom_bar(stat=&quot;identity&quot;) Es un gráfico apilado con un problema, no se puede determinar si el target es mayor para un sexo u otro, es necesario relativizar, es necesario estudiar los porcentajes de respuesta por nivel del factor input para no sacar conclusiones erróneas. train %&gt;% group_by(Gender, target=as.factor(Response)) %&gt;% summarise(clientes=n()) %&gt;% transmute(target, pct_interesados = clientes/sum(clientes)) %&gt;% ggplot(aes(x=Gender, y=pct_interesados, fill=target)) + geom_bar(stat=&quot;identity&quot;) + geom_text(aes(label=paste0(round(pct_interesados,4)*100,&#39;%&#39;)), vjust=0) Al emplear % para comparar se pierde el número de observaciones pero facilita la comparación. Se van añadiendo posibilidades gráficas a ggplot, en este caso se incluyen los valores porcentuales mediante geom_text el ejemplo de uso no se complica mucho el código. También tiene interés en el anterior código el uso de transmute que permite crear porcentajes de grupos como una nueva variable a partir de summarise: formattable(train %&gt;% group_by(Gender, target=as.factor(Response)) %&gt;% summarise(clientes=n()) %&gt;% transmute(target, pct_interesados = round(clientes*100/sum(clientes),2))) Gender target pct_interesados Female 0 89.61 Female 1 10.39 Male 0 86.16 Male 1 13.84 En el ejemplo de trabajo parece que los hombres encuestados muestran mayor interés por el producto de automóviles. Este mismo análisis se debe replicar para todos los factores input presentes en el conjunto de datos. Por ejemplo, la variable Region_Code train %&gt;% group_by(Region = as.factor(Region_Code), target=as.factor(Response)) %&gt;% summarise(clientes=n()) %&gt;% transmute(target, pct_interesados = clientes/sum(clientes)) %&gt;% ggplot(aes(x=Region, y=pct_interesados, fill=target)) + geom_bar(stat=&quot;identity&quot;) + geom_text(aes(label=paste0(round(pct_interesados,4)*100,&#39;%&#39;)), vjust=0) En este gráfico aparecen dos problemas que el científico de datos tiene que tener en cuenta cuando trabaje con factores. El factor tiene demasiados niveles El factor puede tener pocas observaciones y se están sacando conclusiones con pocos registros Al primer problema ya se hizo mención al inicio del ensayo, se sugiere realizar una agrupación de niveles para facilitar su interpretación. Además, es posible que se obtengan conclusiones sobre niveles del factor que puedan llevar al error debido a la escasa prevalencia. La prevalencia tiene diversas definiciones pero en este caso se define como el divisor en el cálculo del porcentaje dentro del nivel del factor. \\[pct-interesados_{nivel-i}= \\frac {interesados_{nivel-i}}{observaciones_{nivel-i}} = \\frac {interesados_{nivel-i}}{prevalencia_{nivel-i}}\\] Es relevante porque es sentido común no dar el mismo valor al % de la variable respuesta en una provincia donde se tiene una cartera de 80.000 clientes que en una provincia donde se tienen 80 clientes. El % de clientes interesados en un caso se calcula con un denominador de 80.000 y en otro caso con un denominador de 80 y en los gráficos presentados hasta el momento no es posible estudiar correctamente la prevalencia. Cuando el científico de datos se encuentre situaciones de baja prevalencia o alto número de niveles de un factor ha de pensar una estrategia para abordar el problema. Siempre se debe priorizar la realización de agrupaciones con sentido de negocio. En este caso, si se conoce la división territorial, agrupar por comunidades autónomas, por zonas, territoriales o separar las provincias más relevantes frente al resto. Por ejemplo: train &lt;- train %&gt;% mutate(fr_region = case_when( Region_Code == 8 ~ &#39;Barcelona&#39;, Region_Code == 28 ~ &#39;Madrid&#39;, TRUE ~ &#39;Resto&#39;)) train %&gt;% group_by(Region = as.factor(fr_region), target=as.factor(Response)) %&gt;% summarise(clientes=n()) %&gt;% transmute(target, pct_interesados = clientes/sum(clientes)) %&gt;% ggplot(aes(x=Region, y=pct_interesados, fill=target)) + geom_bar(stat=&quot;identity&quot;) + geom_text(aes(label=paste0(round(pct_interesados,4)*100,&#39;%&#39;)), vjust=0) Se sugiere que aquellas variables del conjunto de datos que estén analizadas o clasificadas empiecen por un prefijo o se las pueda distinguir de algún modo dentro del tablón de datos, en este caso y a lo largo de todo el trabajo, se emplea el prefijo fr_ para indicar factor reclasificado. Esto facilitará la automatización de análisis y los procesos de modelización como se verá con posterioridad. Parece más alto el interés en Madrid, sin embargo, el % de interesados en Barcelona y el resto de España es similar. No es una agrupación convincente pero puede interesar a un equipo de negocio. Otro mecanismo de unión de factores puede basarse en la propia variable respuesta, que sea ese % el que agrupe la variable. A continuación se plantea una agrupación en buenos, regulares y malos, agrupación de niveles de un factor simplista basada en el % de respuesta. res &lt;- train %&gt;% group_by(Region_Code) %&gt;% summarise(clientes=n(), pct_interesados = round(sum(Response)*100/n(),2)) %&gt;% arrange(desc(pct_interesados)) %&gt;% mutate(pct_agregado_clientes = cumsum(clientes)/nrow(train), pct_anterior_agregado=lag(pct_agregado_clientes, default = 0)) formattable(res, digits=2, list(pct_interesados=color_bar(&#39;grey&#39;))) Region_Code clientes pct_interesados pct_agregado_clientes pct_anterior_agregado 38 2026 19.2 0.0053 0.0000 28 106415 18.7 0.2845 0.0053 19 1535 16.3 0.2886 0.2845 4 1801 15.8 0.2933 0.2886 23 1960 15.3 0.2984 0.2933 51 183 15.3 0.2989 0.2984 24 2415 14.2 0.3053 0.2989 7 3279 13.2 0.3139 0.3053 18 5153 13.2 0.3274 0.3139 3 9251 12.8 0.3517 0.3274 35 6942 12.5 0.3699 0.3517 39 4644 12.4 0.3821 0.3699 52 267 12.4 0.3828 0.3821 29 11065 12.3 0.4118 0.3828 41 18263 12.2 0.4597 0.4118 40 1295 11.8 0.4631 0.4597 5 1279 11.6 0.4665 0.4631 20 1935 11.5 0.4715 0.4665 11 9232 11.3 0.4958 0.4715 45 5605 11.0 0.5105 0.4958 1 1008 10.8 0.5131 0.5105 46 19749 10.3 0.5649 0.5131 48 4681 10.2 0.5772 0.5649 31 1960 10.1 0.5824 0.5772 33 7654 9.9 0.6024 0.5824 12 3198 9.8 0.6108 0.6024 8 33877 9.6 0.6997 0.6108 43 2639 9.2 0.7067 0.6997 13 4036 9.0 0.7172 0.7067 14 4678 9.0 0.7295 0.7172 47 7436 8.8 0.7490 0.7295 0 2021 8.6 0.7543 0.7490 32 2787 8.6 0.7616 0.7543 9 3101 8.2 0.7698 0.7616 36 8797 8.0 0.7929 0.7698 37 5501 7.9 0.8073 0.7929 34 1664 7.6 0.8117 0.8073 49 1832 7.5 0.8165 0.8117 42 591 7.4 0.8180 0.8165 27 2823 7.4 0.8254 0.8180 30 12191 7.4 0.8574 0.8254 26 2587 7.2 0.8642 0.8574 15 13308 7.2 0.8991 0.8642 2 4038 7.1 0.9097 0.8991 21 4266 7.1 0.9209 0.9097 17 2617 7.0 0.9278 0.9209 6 6280 7.0 0.9443 0.9278 16 2007 6.7 0.9495 0.9443 22 1309 6.3 0.9530 0.9495 50 10243 6.3 0.9798 0.9530 10 4374 6.0 0.9913 0.9798 25 2503 4.3 0.9979 0.9913 44 808 4.1 1.0000 0.9979 La librería formattable además de permitir presentar de forma elegante data frames, puede añadir mayor vistosidad a las tablas como ilustra el ejemplo. Para facilitar el análisis se suma la variable Response, motivo por el cual no se transforma (aun) en factor, en ocasiones facilita el trabajo mantenerla como numérica. Se crea un agregado_clientes para determinar el % de clientes que se van agrupando. Mediante la función lag que permite extraer el anterior registro de un data frame se busca si la acumulación de clientes de la anterior región ya ha superado el 10% de las observaciones, de esa forma se crean grupos de provincias en función de la tasa de respuesta. La primera agrupación de provincias será aquella que supere el 10% de las observaciones. muy_buenos &lt;- res %&gt;% mutate(grupo = &quot;1 muy buenos&quot;) %&gt;% filter(pct_anterior_agregado&lt;0.1) Se vuelve a recalcular el agregado de clientes eliminando las provincias ya clasificadas y el siguiente corte se establece cuando se supere otro 10% de observaciones. buenos &lt;- res %&gt;% filter(!(Region_Code %in% unique(muy_buenos$Region_Code))) %&gt;% mutate(pct_agregado_clientes = cumsum(clientes)/nrow(train), pct_anterior_agregado=lag(pct_agregado_clientes, default = 0)) %&gt;% mutate(grupo = &quot;2 buenos&quot;) %&gt;% filter(pct_anterior_agregado&lt;0.1) Este proceso se plantea de forma iterativa en distintos data frames para entender el proceso y así se crean los grupos muy bueno, bueno, regular, malo, muy malo de provincias en base a la variable target. regulares &lt;- res %&gt;% filter(!(Region_Code %in% unique(muy_buenos$Region_Code)) &amp; !(Region_Code %in% unique(buenos$Region_Code))) %&gt;% mutate(pct_agregado_clientes = cumsum(clientes)/nrow(train), pct_anterior_agregado=lag(pct_agregado_clientes, default = 0)) %&gt;% mutate(grupo = &quot;3 regulares&quot;) %&gt;% filter(pct_anterior_agregado&lt;0.1) malos &lt;- res %&gt;% filter(!(Region_Code %in% unique(muy_buenos$Region_Code)) &amp; !(Region_Code %in% unique(buenos$Region_Code)) &amp; !(Region_Code %in% unique(regulares$Region_Code))) %&gt;% mutate(pct_agregado_clientes = cumsum(clientes)/nrow(train), pct_anterior_agregado=lag(pct_agregado_clientes, default = 0)) %&gt;% mutate(grupo = &quot;4 malos&quot;) %&gt;% filter(pct_anterior_agregado&lt;0.1) muy_malos &lt;- res %&gt;% filter(!(Region_Code %in% unique(muy_buenos$Region_Code)) &amp; !(Region_Code %in% unique(buenos$Region_Code)) &amp; !(Region_Code %in% unique(regulares$Region_Code)) &amp; !(Region_Code %in% unique(malos$Region_Code))) %&gt;% mutate(grupo = &quot;5 muy_malos&quot;) grupos_region &lt;- rbind.data.frame(muy_buenos, buenos, regulares, malos, muy_malos) %&gt;% mutate(fr_region2 = grupo) %&gt;% select(Region_Code, fr_region2) remove(res, muy_buenos, buenos, regulares, malos, muy_malos) Es útil ver paso a paso que regiones se van uniendo por si existe algún criterio de negocio o se encuentra algún indicador que pueda mejorar esta agrupación ya que este trabajo no tiene ningún sentido práctico más allá de unir regiones con similares tasas de respuesta, al científico de datos puede interesarle llevar estos datos a una hoja de cálculo y realizar sus propias agrupaciones. Si se trabaja con un gran número de variables existen librerías en R que realizan esta labor de forma automática tanto con variables numéricas como con factores pero se recomienda controlar como se producen estas agrupaciones. Ya se dispone de un data frame auxiliar que es necesario unir al tablón de trabajo. train &lt;- train %&gt;% left_join(grupos_region) ## Joining, by = &quot;Region_Code&quot; Se repite el gráfico de barras apiladas. train %&gt;% group_by(Region = as.factor(fr_region2), target=as.factor(Response)) %&gt;% summarise(clientes=n()) %&gt;% transmute(target, pct_interesados = clientes/sum(clientes)) %&gt;% ggplot(aes(x=Region, y=pct_interesados, fill=target)) + geom_bar(stat=&quot;identity&quot;) + geom_text(aes(label=paste0(round(pct_interesados,4)*100,&#39;%&#39;)), vjust=0) Para facilitar la labor y no trabajar con recodificación de factores se opta por numerar cada nivel del factor algo que ayuda en las visualizaciones, es necesario recordar que hay factores con orden (como es el caso). La agrupación de provincias garantiza que tiene en cada nivel al menos un 10% de observaciones, además parece que esa agrupación discrimina la variable respuesta, pero este análisis gráfico es claramente mejorable. 11.4 Propuesta de análisis gráfico para el análisis bivariable El científico de datos ha de elaborar sus propias herramientas y sus propias funciones gráficas para mostrar estos análisis iniciales sobre sus datos. Estas aproximaciones son fundamentales para el trabajo de documentación y modelización y en ocasiones el científico de datos tendrá que exponer estos resultados previos a equipos que no están familiarizados con la visualización de datos. Por este motivo, una buena o mala visualización no la establece la literatura sobre la realización de gráficos, la establece la capacidad de comprensión que tengan las áreas usuarias de las visualizaciones. En este caso se propone un análisis gráfico de uso en el ámbito actuarial donde la ciencia de datos tiene múltiples casos de éxito. Se trata de un gráfico de doble eje y donde el eje y de la izquierda representará el % de observaciones del nivel del factor (prevalencia) y permitirá dar un peso a la información que suministra ese nivel del factor. En el eje y de la derecha se pondrá la proporción, la tasa de respuesta del target en estudio. resumen &lt;- train %&gt;% group_by(factor_analisis = Age) %&gt;% summarise(pct_clientes = round(n()*100/nrow(train),1), pct_interesados = round(sum(Response)*100/n(),1), .groups=&#39;drop&#39;) ajuste = 1 g2 &lt;- ggplot(resumen, aes(x=factor_analisis)) + geom_line(aes(y=pct_interesados * ajuste), group=1, color=&quot;red&quot;) + geom_col(aes(y=pct_clientes),fill=&quot;yellow&quot;,alpha=0.5) + geom_text(aes(y=pct_interesados * ajuste, label = paste(pct_interesados,&#39; %&#39;)), color=&quot;red&quot;) + scale_y_continuous(sec.axis = sec_axis(~./ajuste, name=&quot;% interesados&quot;), name=&#39;% clientes&#39;) + theme_light() g2 + labs(title = &quot;Análisis de la variable edad&quot;) Tabularmente sería complejo de ver al tener un gran número de niveles, pero el gráfico permite estudiar como se distribuye la población para cada nivel del factor y como es su comportamiento frente a la variable target. En este caso ggplot representa en forma de línea continua la tasa de respuesta (geom_line) y en barra el % de observaciones (geom_col), se añade el valor del % de clientes interesados mediante geom_text mejorando el formato y es scale_y_continuous junto con sec_axis lo que permite crear un gráfico de doble eje a ggplot. Pero es necesario que ambos ejes estén en la misma escala, por eso se añade un ajuste en sec_axis. En el anterior gráfico se aprecia que pasa si se deja ese ajuste a 1, para mejorar la visualización se puede jugar con ese ajuste para el eje y de la derecha. resumen &lt;- train %&gt;% group_by(factor_analisis = Age) %&gt;% summarise(pct_clientes = round(n()*100/nrow(train),1), pct_interesados = round(sum(Response)*100/n(),1), .groups=&#39;drop&#39;) ajuste = 0.5 g2 &lt;- ggplot(resumen, aes(x=factor_analisis)) + geom_line(aes(y=pct_interesados * ajuste), group=1, color=&quot;red&quot;) + geom_col(aes(y=pct_clientes),fill=&quot;yellow&quot;,alpha=0.5) + geom_text(aes(y=pct_interesados * ajuste, label = paste(pct_interesados,&#39; %&#39;)), color=&quot;red&quot;) + scale_y_continuous(sec.axis = sec_axis(~./ajuste, name=&quot;% interesados&quot;), name=&#39;% clientes&#39;) + theme_light() g2 + labs(title = &quot;Análisis de la variable edad&quot;) Se aprecia como el eje y de la izquierda es la mitad del eje y de la derecha, se produce un ajuste del 0.5. Los clientes encuestados son mayoritariamente jóvenes entre 24-30 años que no muestran interés en el producto, sin embargo, hay una serie de clientes entre 35 y 50 años que sí tienen interés, a medida que se avanza en la edad ese mismo interés en el producto de automóviles va cayendo. Entendido este gráfico habría de ser replicado para todos los factores presentes en el análisis, cuando se repita un código en múltiples ocasiones es recomendable crear una función. bivariable &lt;- function(df, target, varib, ajuste=1){ target = as.symbol(target) fr_analisis = as.symbol(varib) g &lt;- df %&gt;% group_by(factor_analisis = as.factor(!!fr_analisis)) %&gt;% summarise(pct_clientes = round(n()*100/nrow(df),1), pct_interesados = round(sum(!!target)*100/n(),1), .groups=&#39;drop&#39;) %&gt;% ggplot(aes(x=factor_analisis)) + geom_line(aes(y=pct_interesados * ajuste), group=1, color=&quot;red&quot;) + geom_col(aes(y=pct_clientes),fill=&quot;yellow&quot;,alpha=0.5) + geom_text(size=3, aes(y=pct_interesados * ajuste, label = paste(pct_interesados,&#39; %&#39;)), color=&quot;red&quot;) + scale_y_continuous(sec.axis = sec_axis(~./ajuste, name=&quot;% interesados&quot;), name=&#39;% clientes&#39;) + theme_light() g + labs(title = paste0(&quot;Análisis de la variable &quot;,varib)) } bivariable_Age &lt;- bivariable(train, &#39;Response&#39;, &#39;Age&#39;, 0.5) bivariable_Age Este código automatiza el gráfico anterior y se puede aplicar para más variables, además sirve de ejemplo de uso de símbolos en funciones con dplyr ya que en funciones puede ser un problema pasar cadenas de caracteres. Se pueden emplear funciones como group_by_at para evitar esta situación pero en esta función se transforma en símbolo y se referencia mediante !! Como se puede comprobar esta función bivariable se puede replicar con todos los factores en estudio. grid.arrange(ncol=2, bivariable(train, &#39;Response&#39;, &#39;fr_region2&#39;, 1.5), bivariable(train, &#39;Response&#39;, &#39;Gender&#39;, 1), bivariable(train, &#39;Response&#39;, &#39;Driving_License&#39;, 1), bivariable(train, &#39;Response&#39;, &#39;Vehicle_Damage&#39;, 1), bivariable(train, &#39;Response&#39;, &#39;Policy_Sales_Channel&#39;, 1), bivariable(train, &#39;Response&#39;, &#39;Previously_Insured&#39;, 1), bivariable(train, &#39;Response&#39;, &#39;Vehicle_Age&#39;, 1)) En este punto se aprecian problemas ya sabidos e identificados en el análisis univariable como son: Baja prevalencia del nivel 0 de Driving License ya que no tiene sentido ofrecer seguro de automóviles a aquellos clientes sin carnet. El gran número de canales de venta (Policy_Sales_Channel), es un factor con un número inmanejable de niveles. El incorrecto orden del factor Vehicle_Age. Además surgen otras cuestiones propias del análisis bivariable. ¿Tiene sentido incluir aquellos clientes con en el factor Vehicle_Damage? ¿Tiene sentido incluir a los clientes que no tienen un seguro de daños cuando casi ninguno muestra interés por el producto? ¿Tiene sentido incluir a los clientes con un 1 en Previously_Insured? ¿Tiene sentido incluir clientes previamente asegurados en los análisis cuando su interés es tan bajo? El científico de datos debe emplear estas aproximaciones para transmitir a los equipos usuarios de los datos las situaciones que describen las variables. No se deben incluir variables sin sentido para mejorar los análisis o la capacidad predictiva de los modelos porque está sacando conclusiones que no tienen utilidad. Este análisis gráfico se puede emplear en factores pero no se puede usar con variables cuantitativas si éstas no han sido transformadas previamente a factores. 11.5 Correlación entre factores. La V de Cramer Es posible medir si dos factores están relacionados entre sí al igual que se hacía con la correlación lineal. Cuando se mide correlación en variables cuantitativas la idea la recoge el gráfico de dispersión y como se coloca cada observación ayuda a comprender el coeficiente de correlación. Sin embargo, en factores un análisis gráfico no recoge tendencia y se opta por medir la diferencia entre los conteos esperados y los conteos obtenidos. Se describe esta labor con un ejemplo sobre los datos de trabajo empleando la librería gmodels. library(gmodels) CrossTable(train$Response, train$Gender, prop.r=TRUE, prop.c=TRUE) ## ## ## Cell Contents ## |-------------------------| ## | N | ## | Chi-square contribution | ## | N / Row Total | ## | N / Col Total | ## | N / Table Total | ## |-------------------------| ## ## ## Total Observations in Table: 381109 ## ## ## | train$Gender ## train$Response | Female | Male | Row Total | ## ---------------|-----------|-----------|-----------| ## 0 | 156835 | 177564 | 334399 | ## | 69.461 | 58.989 | | ## | 0.469 | 0.531 | 0.877 | ## | 0.896 | 0.862 | | ## | 0.412 | 0.466 | | ## ---------------|-----------|-----------|-----------| ## 1 | 18185 | 28525 | 46710 | ## | 497.273 | 422.306 | | ## | 0.389 | 0.611 | 0.123 | ## | 0.104 | 0.138 | | ## | 0.048 | 0.075 | | ## ---------------|-----------|-----------|-----------| ## Column Total | 175020 | 206089 | 381109 | ## | 0.459 | 0.541 | | ## ---------------|-----------|-----------|-----------| ## ## Esta salida que se obtiene con la función CrossTable de la librería gmodels, se trata de una tabla de contingencia, muy empleada en el ámbito estadístico. Como dice la descripción previa para el cruce de los dos factores se tiene el número de observaciones y la Chi-square contribution que es: \\[contribucion- \\chi^2 = \\frac {(Esperado - Observado)^2}{Esperado}\\] Esta es la idea que se emplea para definir si hay dependencia entre factores, ¿qué es el valor esperado? Es el producto de las frecuencias marginales entre el total de observaciones. En el ejemplo se espera para el género Female no está interesado: 334399*175020/381109 = 153569 frente a 156835 que es lo observado, entonces (153569 - 156835)^2/153569 = 69.46 que es un número, no una medida. Además no sabemos la validez estadística que tiene ese valor pero sí es conocido que esta diferencia tiene una distribución asociada llamada Chi-cuadrado que se verá en el siguiente capítulo. Esta prueba da lugar al estadístico de la \\(\\chi^2\\) que está influenciado por el tamaño de la muestra y no da ninguna medida, por este motivo se le aplica una corrección por el número total de observaciones y las filas o columnas de la tabla de contingencia y ello permite dar una magnitud, esa corrección da lugar al estadístico V de Cramer. Para la obtención de la V de Cramer se emplea en este caso la librería vcd. library(vcd) ## Loading required package: grid tabla=ftable(train$Gender,train$Response) assocstats(tabla) ## X^2 df P(&gt; X^2) ## Likelihood Ratio 1058.1 1 0 ## Pearson 1048.0 1 0 ## ## Phi-Coefficient : 0.052 ## Contingency Coeff.: 0.052 ## Cramer&#39;s V : 0.052 En este caso la V de Cramer tiene un valor muy próximo a 0 lo que indica que hay poca relación entre los factores, valores próximos a 1 indican mucha relación. Cuando se trabaje con modelos lineales se verá la importancia que tiene esta medida para las variables input en los procesos de modelización. En cualquier caso, se ha planteado una descripción gráfica para factores y un análisis de correlación para factores, no es aplicable a variables numéricas. Un factor nunca debe ser tratado como una variable numérica, sin embargo, si es posible agrupar variables numéricas y que se comporten como factores y todos los análisis planteados pueden tener cabida. 11.6 Agrupación de variables cuantitativas Para poder emplear un estadístico como el planteado en el apartado anterior las variables cuantitativas pueden estar trameadas. Como en ocasiones anteriores se sugiere que esos tramos tengan el sentido de negocio, pero si se desconoce o nunca se ha abordado un análisis de ese tipo se puede empezar por dividir la variable cuantitativa en N tramos de igual tamaño y estudiar como se comporta la variable respuesta en esos N tramos creados. En el ejemplo de trabajo se estudia el comportamiento de la variable Annual_Premium dividida en 10 tramos con el mismo número de observaciones. # Se divide la variable en 10 tramos con el mismo número de observaciones grupos = 10 train &lt;- train %&gt;% arrange(Annual_Premium) %&gt;% mutate(fr_prima = as.factor(ceiling((row_number()/n()) * grupos))) table(train$fr_prima) ## ## 1 2 3 4 5 6 7 8 9 10 ## 38110 38111 38111 38111 38111 38111 38111 38111 38111 38111 Esa agrupación es la definición de percentil, se ordena la variable a tramificar de menor a mayor y cada registro dividido por el total si es multiplicado por el número de grupos va a dar un número entre 1 y el número de grupos especificado que tendrán el mismo número de registros y cuyo total recoge el total de los registros. Esa variable grupo se trata como un factor reclasificado por lo que es susceptible de aplicar la función de descripción bivariable. bivariable(train, &#39;Response&#39;, &#39;fr_prima&#39;, 1.5) Cada tramo de fr_prima tiene el 10% del total de observaciones, cada corte es un decil de Annual_Premium y, a la vista del gráfico, no parece una buena opción de tramificación pero acerca al científico de datos a conocer mejor como se comporta la variable respuesta frente a una variable cuantitativa. Existen técnicas para realizar la agrupación de variables como el Weight Of Evidence (WOE) pero la automatización pierden la perspectiva de conocer como se comportan las variables. Todos los análisis planteados comienzan a ofrecer impresiones sobre la variable target, permiten aproximar a la tarea identificar que características de nuestros clientes pueden ser eficaces a la hora de crear una campaña comercial. Pero no dejan de ser apreciaciones visuales y opiniones que no tienen ningún sustento estadístico. Es necesario determinar si esas impresiones son estadísticamente significativas y para ello el científico de datos debe tener nociones de muestreo e inferencia estadística. "],["muestreo-inferencia.html", "Capítulo 12 Muestreo e inferencia estadística 12.1 Muestreo 12.2 Inferencia estadística 12.3 Estimación de parámetros 12.4 Intervalos de confianza 12.5 Contrastes de hipótesis", " Capítulo 12 Muestreo e inferencia estadística En el capítulo anterior dedicado al análisis bivariable se crearon visualizaciones sencillas para describir la posible relación entre dos variables, pero más allá de impresiones visuales no es posible asegurar que esa relación tiene validez estadística. Para establecer esa validez es necesario disponer de cierta dialéctica, de cierta base teórica básica para entender como se comporta un contraste estadístico o un intervalo de confianza. El científico de datos tiende a considerar que toda esa base teórica está obsoleta y que existe un cambio en el paradigma, pero los problemas a resolver con análisis estadísticos avanzados son similares a los que resuelve la estadística clásica. El trabajo del científico de datos en muchas ocasiones consiste en separar la señal del ruido, separar lo aleatorio de lo estadísticamente significativo. En los capítulos anteriores se han ido estableciendo los cimientos para realizar esta labor. Todo problema y todo estudio parte de la población, en la RAE aparecen las siguientes definiciones de población: Acción y efecto de poblar. Conjunto de personas que habitan en un determinado lugar. Conjunto de edificios y espacios de una ciudad. Atravesó la población de una parte a otra. Conjunto de individuos de la misma especie que ocupan determinada área geográfica. Sociol. Conjunto de los elementos sometidos a una evaluación estadística mediante muestreo. Desarrollando esta quinta acepción se tiene que la población es el conjunto de elementos sobre el que se estudia una característica y es necesario tener claro como se compone porque es la herramienta a disposición del científico de datos para resolver el problema que le plantean los datos. Esa población no tiene porque ser individuos, en el ejemplo de trabajo que sirve de hilo conductor se tiene una población de clientes de una compañía de seguros pero se pueden tener empresas, contratos, acciones deportivas, Además, la población no tiene porque ser finita, de hecho para estudiar a la población será necesario obtener muestras mediante técnicas de muestreo. 12.1 Muestreo Comprende las técnicas para la selección de elementos de una población. En ocasiones, por recursos o por definición no se dispone del total de población para realizar análisis. En esos casos se trata de obtener una muestra representativa de la población, una muestra que tendrá las mismas características de la población y ello permite inferir información acerca de una característica de la población. Se distinguen dos grandes tipos de muestreo: Muestreo no probabilístico. Es un método de selección de elementos de la población donde el analista selecciona los elementos en base a su propia experiencia o necesidad. Ejemplos: seleccionar a clientes con una característica para realizar un análisis cualitativo. Muestreo probabilístico. Cada elemento de la población tiene una probabilidad conocida a priori de ser seleccionado para el estudio. El científico de datos trabaja habitualmente con muestreo probabilístico y dentro de este tipo de muestreo existen diversas técnicas encaminadas a garantizar la representatividad de la muestra, garantizan que la selección de elementos tenga el rigor requerido para el análisis. Para entender mejor como se comportan las distintas técnicas de muestreo se recomienda leer el siguiente enlace al blog de Anabel Forte donde se tratan poblaciones y muestras. Se considera que el científico de datos debe conocer los siguientes tipos de muestreo pero estas líneas son un atisbo sobre las posibilidades del muestreo estadístico, se pueden plantear diseños muy complejos. 12.1.1 Muestreo aleatorio simple Todos los elementos son elegidos al azar entre toda la población porque todos ellos tienen la misma probabilidad de ser elegidos. Será la técnica que más utilice el científico de datos porque es habitual separar grupos de entrenamiento, grupos de test y grupos de validación cuando se realizan procesos de modelización. En R hay múltiples formas de realizar una muestra aleatoria mediante muestreo aleatorio simple, con dplyr se puede realizar del siguiente modo: library(tidyverse) library(formattable) library(gridExtra) train &lt;- read.csv(&quot;./data/train.csv&quot;) set.seed(10) muestra_aleatoria1 &lt;- train %&gt;% sample_n(size = 100, replace = F) Con la función set.seed(10) se fija la semilla ya que en realidad R genera números pseudoaleatorios y al fijar la semilla se obtiene siempre la misma muestra aleatoria lo que garantiza que los trabajos puedan ser reproducibles, en este caso con sample_n se obtiene una muestra de tamaño size observaciones y la opción replace indica si se hace con reemplazamiento de individuos o no. Si se quiere obtener una muestra que sea una proporción: muestra_aleatoria2 &lt;- train %&gt;% sample_frac(size = 0.1, replace = F) En el ejemplo se obtiene una muestra aleatoria de un 10% de las observaciones mediante la instrucción sample_frac. Si el científico de datos desea dividir un data frame en dos partes es habitual el empleo de índices: set.seed(10) selecccionar &lt;- sample(seq(1:nrow(train)) , round(nrow(train) * 0.70)) datos_entrenamiento &lt;- train[selecccionar,]; datos_test &lt;- train[-selecccionar,]; # % de datos de entrenamiento nrow(datos_entrenamiento)/nrow(train) ## [1] 0.6999992 # % de datos de test nrow(datos_test)/nrow(train) ## [1] 0.3000008 De ese modo se han dividido los datos de partida en dos data frames disjuntos donde datos_entrenamiento tiene el 70% de las observaciones y datos_test tiene el 30% de observaciones restantes del conjunto de datos de partida. 12.1.2 Muestreo aleatorio estratificado Existe una población dividida en grupos homogéneos llamados estratos y se realiza un muestreo aleatorio dentro de cada uno de los estratos. Permite toma muestras de mayor tamaño donde así fuera necesario. El científico de datos empleará este tipo de técnica cuando tenga que balancear una muestra para un modelo (por ejemplo). En el ejemplo de trabajo se pretende balancear la muestra para equilibrar al 50% aquellos clientes que están interesados y los que no están interesados en el seguro de automóviles. Habría dos formas de hacer esta tarea, aumentar el número de interesados o disminuir el número de no interesados, siempre artificialmente, para ilustrar la situación: no_interesados &lt;- nrow(train[train$Response==0,]) interesados &lt;- nrow(train[train$Response==1,]) muestra_interesados &lt;- train %&gt;% filter(Response==1) %&gt;% sample_n(no_interesados,replace = T) muestra_aumentada &lt;- train %&gt;% filter(Response==0) %&gt;% bind_rows(muestra_interesados) muestra_disminuida &lt;- train %&gt;% filter(Response==1) %&gt;% bind_rows(sample_n(train %&gt;% filter(Response==0), size=interesados)) Muestra con número de interesados incrementados artificialmente: formattable(muestra_aumentada %&gt;% group_by(Response) %&gt;% summarise(conteo=n())) Response conteo 0 334399 1 334399 Muestra con número de interesados decrementados artificialmente: formattable(muestra_disminuida %&gt;% group_by(Response) %&gt;% summarise(conteo=n())) Response conteo 0 46710 1 46710 En ambos casos la proporción es del 50%. Esta situación se la encontrará el científico de datos cuando tenga baja proporción de casos a investigar, por ejemplo, si únicamente un 1% de los clientes estuviera interesado, cualquier modelo estadístico con asegurar que nadie está interesado acertaría el 99% de las ocasiones, desde un punto de vista teórico sería un buen modelo. Mediante el balanceo de la muestra se procura, que el modelo detecte esos patrones capaces de discriminar para que, en la práctica, el modelo cumpla su función. Estos ejemplos se ilustran con la librería dplyr pero se recomienda el uso de librerías específicas de muestreo como pueda ser sampling: library(sampling) muestra_balanceada &lt;- strata(train, stratanames = &quot;Response&quot;, size = c(10000,10000)) formattable(muestra_balanceada %&gt;% group_by(Response) %&gt;% summarise(conteo=n())) Response conteo 0 10000 1 10000 12.1.3 Muestreo por conglomerados Supone seleccionar al azar todos los elementos de un grupo o un conglomerado, es un tipo de muestreo aleatorio donde se seleccionan todos los elementos de un conglomerado. Si el científico de datos selecciona un colegio, una calle, una provincia, está realizando muestreo por conglomerados. En el ejemplo de trabajo se desean seleccionar 100 clientes de la provincia de Madrid y 100 clientes de la provincia de Barcelona. Barcelona &lt;- train %&gt;% filter(Region_Code==8) %&gt;% sample_n(100) Madrid &lt;- train %&gt;% filter(Region_Code==28) %&gt;% sample_n(100) El científico de datos ha de saber que en el momento de realizar una selección de observaciones de cualquier tipo que está haciendo muestreo y por ello debe conocer y argumentar los motivos que le han llevado a realizar esa selección. Como se señaló con anterioridad estos apuntes son un mínimo, el muestreo abarca técnicas y modelos más complejos. 12.2 Inferencia estadística Inferir significa extraer una conclusión a partir de hechos concretos a hechos generales. La inferencia estadística trata de extraer conclusiones sobre una característica, sobre un parámetro de la población a partir de una muestra de ésta. Los parámetros a analizar se denominan estadísticos muestrales, además si se conoce la distribución de dichos estadísticos (generalmente distribución normal) y se cumplen una serie de condiciones se trata de inferencia paramétrica. La estadística paramétrica clásica plantea tres tipos de problemas: Estimación puntual en la que se trata de dar un valor al parámetro a estimar (Ej: valor esperado de una media). Estimación por intervalos (buscar un intervalo de confianza). Contrastes de hipótesis donde se busca contrastar información acerca del parámetro. Se parte de un experimento, repetido varias veces y se obtiene una muestra con variables aleatorias independientes idénticamente distribuidas y función de distribución conocida. Por ejemplo, se pretende estimar la altura media de los varones españoles, se recogen las alturas de 30 individuos y su media es de 1,74 metros. Esa es una estimación puntual. Entonces, cualquier función de la muestra de 30 varones que no dependan del parámetro a estimar es un estadístico muestra y esa media obtenida que nos sirve para conocer la altura media de los varones españoles es un estimador del parámetro. Ejemplos de estadísticos son el total muestra, la media muestral, la varianza muestral, la cuasivarianza muestral y los estadísticos de orden que habitualmente se representan con letras griegas. 12.3 Estimación de parámetros Hay que determinar cual es el mejor estimador de un parámetro para una población a partir de una muestra. ¿Cuál será el estadístico muestral que mejor representa ese parámetro poblacional? El mejor estadístico será el más creíble, el más verosímil y por ello se denomina estimador de máxima verosimilitud pero tiene que cumplir una serie de condiciones: Será aquel que tiene menor sesgo. Esto se cumplirá cuando el promedio de las distintas estimaciones es análogo al parámetro poblacional. Será el más eficiente. Cuando la desviación de las distintas estimaciones es la más baja, se minimiza la varianza de esas estimaciones. Será el más consistente. Si la muestra crece también crece la probabilidad de que ese sea el estimador. Será suficiente. Ningún estadístico calculado sobre la muestra va a proporcionar información adicional sobre su valor. A la hora de estimar el parámetro se plantea un dilema que el científico de datos deberá abordar en múltiples ocasiones, se trata del dilema sesgo - varianza. Este dilema está presente siempre que se trabajan datos. Por ejemplo, aseverar que los inmigrantes cometen más delitos que los residentes de un país. Sin embargo, son más los hombres que cometen delitos que las mujeres. En ese caso la solución para vivir con mayor seguridad no sería una sociedad sin extranjeros, será una sociedad sin hombres. Para dar ambos datos se introduce sesgo, puede ser cierto que se acierte en mayor medida pero introduciendo condiciones que interesan al analista. El científico de datos se verá en esta situación, la mejor solución es argumentar, motivar y consensuar el sesgo con los usuarios de los datos. En el ejemplo de trabajo las aproximaciones iniciales a los datos ya están planteando este dilema. Hay observaciones que deben ser eliminadas debido a que no aportan a la resolución del problema, mejorarán la estimación del número de respuestas positivas pero, ¿aportan algo al análisis? bivariable &lt;- function(df, target, varib, ajuste=1){ target = as.symbol(target) fr_analisis = as.symbol(varib) g &lt;- df %&gt;% group_by(factor_analisis = as.factor(!!fr_analisis)) %&gt;% summarise(pct_clientes = round(n()*100/nrow(df),1), pct_interesados = round(sum(!!target)*100/n(),1), .groups=&#39;drop&#39;) %&gt;% ggplot(aes(x=factor_analisis)) + geom_line(aes(y=pct_interesados * ajuste), group=1, color=&quot;red&quot;) + geom_col(aes(y=pct_clientes),fill=&quot;yellow&quot;,alpha=0.5) + geom_text(size=3, aes(y=pct_interesados * ajuste, label = paste(pct_interesados,&#39; %&#39;)), color=&quot;red&quot;) + scale_y_continuous(sec.axis = sec_axis(~./ajuste, name=&quot;% interesados&quot;), name=&#39;% clientes&#39;) + theme_light() g + labs(title = paste0(&quot;Análisis de la variable &quot;,varib)) } grid.arrange(ncol=2, bivariable(train, &#39;Response&#39;, &#39;Driving_License&#39;, 1), bivariable(train, &#39;Response&#39;, &#39;Vehicle_Damage&#39;, 1), bivariable(train, &#39;Response&#39;, &#39;Previously_Insured&#39;, 1)) La estimación mejorará si se incluye la variable Vehicle_Damage y Previously_Insured porque los clientes sin cobertura de daños no van a contratar, igual que aquellos que ya han estado asegurados, no deben de ser reglas, deben ser condiciones a la hora de seleccionar clientes pero el analista debe tener claro, argumentar estas acciones y consensuar con los usuarios de los datos si son correctas las decisiones y los sesgos que está introduciendo en su análisis. 12.4 Intervalos de confianza El intervalo de confianza está presente en el lenguaje, si alguien pregunta sobre el precio de una vivienda no es posible dar un número, se da un mínimo y un máximo que recoja el mayor número de viviendas. El intervalo de confianza será un rango que recoja un x% de los posibles valores que toma una variable. Y su creación se sustenta en el Teorema central del límite. Éste dice, si se suman variables aleatorias, sin importar la distribución que éstas tienen, el resultado final será una variable con distribución normal si se cumplen ciertas condiciones: Las variables aleatorias tienen varianza finita Hay un número elevado de variables aleatorias Para entenderlo mejor, se lanza al aire una moneda 100 veces y se anota el número de caras, se repite el experimento 10 veces: set.seed(11) x &lt;- rbinom(10,100,0.5) hist(x) Si ese mismo experimento se repite 1000 veces: x &lt;- rbinom(1000,100,0.5) hist(x) ¿Qué forma empieza a tomar ese número de caras? Efectivamente, cuando hay un número elevado de variables la distribución empieza a asemejarse a una distribución normal. Desde una distribución binomial como es el lanzamiento de una moneda se ha llegado a una distribución normal. Llevando este teorema al ejemplo de trabajo: edad_40 &lt;- train %&gt;% filter(Age==40) medias_muestras &lt;- NULL for (i in seq(1:500)){ muestra &lt;- edad_40 %&gt;% sample_n(100, replace = T) medias_muestras &lt;- rbind.data.frame(medias_muestras, mean(muestra$Response)) } names(medias_muestras) &lt;- &quot;medias_response&quot; Se realizan 500 muestras con reemplazamiento de tamaño 100 clientes encuestados de 40 años de edad. Si se estudia la distribución de las medias de la variable respuesta en esas 500 muestras: medias_muestras %&gt;% ggplot(aes(x=medias_response)) + geom_density() Recuerda a la distribución normal con sólo 100 observaciones seleccionadas en cada muestra, en el capítulo 10 se demostró mediante simulación que en espacio de 2 desviaciones estaban el 95% de los posibles valores: medias_muestras %&gt;% ggplot(aes(x=medias_response)) + geom_density() + geom_vline(xintercept=mean(medias_muestras$medias_response)-sd(medias_muestras$medias_response)*2, color = &#39;red&#39;) + geom_vline(xintercept=mean(medias_muestras$medias_response)+sd(medias_muestras$medias_response)*2, color = &#39;red&#39;) El 95% de los posibles valores que va a tomar la media de la respuesta para los clientes de 40 años está dentro de ese intervalo (de confianza). Si se lleva este trabajo teórico al total de los grupos de edad para una confianza de 1 - \\(\\alpha\\) se produce un error que se define como: \\[Error = Z_{\\frac{\\alpha}{2}} \\frac{\\alpha}{\\sqrt{n}}\\] De lo que se deduce que la media estará en un intervalo definido por ese error: \\[MED-ESTIMADA - Z_{\\frac{\\alpha}{2}} \\frac{\\alpha}{\\sqrt{n}} &lt; MED-POBLACIONAL &lt; MED-ESTIMADA + Z_{\\frac{\\alpha}{2}} \\frac{\\alpha}{\\sqrt{n}} \\] Para entenderlo mejor se programa con R el intervalo sobre la variable Age del conjunto de datos de trabajo paso a paso. resumen &lt;- train %&gt;% group_by(factor_analisis = Age) %&gt;% summarise(porcen_clientes = round(n()*100/nrow(train),1), porcen_interesados = round(sum(Response)/n(),3), media_interesados = round(mean(Response),3), z = -qnorm(0.025,0,1), desviacion = sd(Response), n_grupo = n()) resumen[(1:10),] ## # A tibble: 10 x 7 ## factor_analisis porcen_clientes porcen_interesados media_interesados z desviacion n_grupo ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 20 1.6 0.027 0.027 1.96 0.163 6232 ## 2 21 4.3 0.035 0.035 1.96 0.183 16457 ## 3 22 5.5 0.036 0.036 1.96 0.187 20964 ## 4 23 6.4 0.037 0.037 1.96 0.188 24256 ## 5 24 6.8 0.035 0.035 1.96 0.184 25960 ## 6 25 5.4 0.036 0.036 1.96 0.185 20636 ## 7 26 3.6 0.036 0.036 1.96 0.186 13535 ## 8 27 2.8 0.044 0.044 1.96 0.205 10760 ## 9 28 2.4 0.069 0.069 1.96 0.253 8974 ## 10 29 1.9 0.102 0.102 1.96 0.302 7429 Para cada grupo de edad se tiene el % de clientes, el % de interesados, que es igual que la media de interesados porque en variables de respuesta binomial la media es la proporción, y el cuantil de una normal que deja tanto a derecha como a izquierda un 2.5% de forma que se pueda crear un intervalo del confianza que contenga el 95% de los posibles valores, además, es necesaria la desviación típica y el número de clientes para cada grupo de edad. Se calculan los límites del intervalo y se realiza una gráfica con los intervalos de confianza: resumen &lt;- resumen %&gt;% mutate(error_estimacion = z * (desviacion/sqrt(n_grupo)), lim_inf=media_interesados - error_estimacion, lim_sup=media_interesados + error_estimacion) ajuste = 200000 g2 &lt;- ggplot(resumen, aes(x=factor_analisis)) + geom_col(aes(y=n_grupo),fill=&quot;yellow&quot;,alpha=0.5) + geom_line(aes(y=media_interesados * ajuste), group=1, color=&quot;red&quot;) + scale_y_continuous(sec.axis = sec_axis(~./ajuste), name=&quot;&quot;) + geom_ribbon(aes(ymin=lim_inf*ajuste, ymax=lim_sup*ajuste), linetype=2, alpha=0.5) + theme_light() g2 + labs(title = &quot;Análisis de la variable edad&quot;) + ylab(&quot;% de clientes&quot;) + xlab(&quot;Edad clientes&quot;) Se observa que grupos con gran número de observaciones crean intervalos muy estrechos y grupos de edad con pocas observaciones crean intervalos enormes, incluso con posibles valores negativos que no se pueden dar. En la fórmula de cálculo del intervalo se tiene \\(\\sqrt{n}\\), la propia definición del intervalo está ponderando la definición del intervalo con el número de observaciones, además, una mayor desviación también hará incrementar el tamaño del intervalo. Una de las funciones de los intervalos de confianza reside en la utilidad a la hora de agrupar factores, algo que se trató en el capítulo 11 donde se describió la importancia de crear estas agrupaciones, a modo ilustrativo: El intervalo de confianza puede servir al científico de datos para agrupar los niveles de un factor. Hay que reseñar que en ggplot se puede emplear geom_ribbon sólo con variables numéricas si se emplean factores es necesario el uso de la función geom_errorbar: resumen &lt;- train %&gt;% group_by(factor_analisis = Gender) %&gt;% summarise(porcen_clientes = round(n()*100/nrow(train),1), porcen_interesados = round(sum(Response)*100/n(),1), media_interesados = mean(Response), z = -qnorm(0.05,0,1), desviacion = sd(Response), n_grupo = n(), error_estimacion = z * (desviacion/sqrt(n_grupo)), lim_inf=media_interesados - error_estimacion, lim_sup=media_interesados + error_estimacion) ajuste = 1000000 g2 &lt;- ggplot(resumen, aes(x=factor_analisis)) + geom_col(aes(y=n_grupo),fill=&quot;yellow&quot;,alpha=0.5) + geom_line(aes(y=media_interesados * ajuste), group=1, color=&quot;red&quot;) + scale_y_continuous(sec.axis = sec_axis(~./ajuste), name=&quot;&quot;) + geom_errorbar(aes(ymin=lim_inf* ajuste, ymax=lim_sup* ajuste), width = 0.2) + theme_light() g2 + labs(title = &quot;Análisis de la variable sexo&quot;) + ylab(&quot;% de clientes&quot;) + xlab(&quot;Género clientes&quot;) La estadística clásica es muy conservadora, se reitera que el intervalo es función de la raíz del tamaño del grupo, de este modo por sexo el intervalo es mínimo porque el tamaño de los grupos es de centenares de miles de clientes. 12.5 Contrastes de hipótesis Para realizar contrastes de hipótesis es necesario conocer 3 distribuciones artificiales asociadas a la distribución normal que no se trataron en capítulos anteriores. La chi-cuadrado (\\(\\chi^2\\)) es una función similar a la gamma y se define como una suma de distribuciones normales al cuadrado, el número de distribuciones normales sumadas son los grados de libertad de la \\(\\chi^2\\) df1=data.frame(x=rchisq(1:10000, df=1)) df1$grados_libertad = 1 df2=data.frame(x=rchisq(1:10000, df=2)) df2$grados_libertad = 2 df3=data.frame(x=rchisq(1:10000, df=3)) df3$grados_libertad = 3 df5=data.frame(x=rchisq(1:10000, df=5)) df5$grados_libertad = 5 df &lt;- rbind.data.frame(df1, df2, df3, df5) %&gt;% mutate(grados_libertad=as.factor(grados_libertad)) df %&gt;% ggplot(aes(x=x, group=grados_libertad, color=grados_libertad, fill=grados_libertad)) + geom_density(alpha=0.3) La t de Student se crea a partir de una normal (0,1) y una chi-cuadrado con n grados de libertad independientes. Una variable se distribuye bajo una t de Student si se puede definir como normal(0,1) dividido por la raíz cuadrada de una chi-cuadrado partida por sus grados de libertad. La F de Snedecor se crea a partir de dos chi-cuadrado independientes divididas por sus respectivos grados de libertad, así la F de Snedecor tiene dos parámetros que indican sus grados de libertad. ¿Por qué es necesario conocer estas distribuciones? Porque los principales contrastes de hipótesis bajo unos supuestos poblacionales (habitualmente que se distribuyen normalmente) se distribuyen según estas distribuciones artificiales. Y al igual que en el intervalo de confianza hay una región, una zona en la que tendríamos una seguridad de no equivocarnos a la hora de establecer que un valor inferido esté en esa región. Se tienen muchos supuestos, distribuciones y artificios estadísticos que dependen en gran medida del tamaño de la población. El científico de datos pretende trabajar en un entorno Big Data y con modelos de aprendizaje automático donde todos estos aspectos teóricos no tienen cabida y están obsoletos. Pero todos estos conceptos y el método de trabajo es imprescindible. El inicio de todo es la hipótesis, el pilar de una investigación, primero se establece y después se contrasta si es cierta o no. Se parte de una afirmación sobre un parámetro poblacional. ¿Es el parámetro \\(\\theta\\) un valor? Se establece la afirmación contraria \\(\\theta\\) no es ese valor. ¿Un factor es independiente de otro? ¿Es independiente la respuesta positiva a la encuesta del género del cliente encuestado? Este es un contraste de independencia y se realiza mediante un test de la chi cuadrado. bivariable(train, &#39;Response&#39;, &#39;Gender&#39;, 1) Gráficamente parece que hay diferencia, pero no se sabe si esa diferencia estadísticamente significativa. ¿Hay diferencia de medias entre dos grupos? ¿Es distinta la media de la antigüedad para la respuesta de los encuestados? Este es un contraste de igualdad de medias que se realiza con la distribución t de student. train %&gt;% ggplot(aes(x=Vintage, group=as.factor(Response), fill=as.factor(Response))) + geom_density(alpha = 0.3) ¿Dos poblaciones tienen la misma varianza? ¿Tiene sentido un modelo de regresión? Estos contrastes se realizan mediante un test con la F de Snedecor, se verá en sucesivos capítulos su uso. Hay infinidad de contrastes de hipótesis, en el este link quedan recogidos una gran parte y fundamentalmente como se clasifican: En general el contraste de hipótesis es una cuestión del tipo ¿Los datos de nuestras muestras respaldan las hipótesis de la población? y esa cuestión se resuelve del siguiente modo. Se parte de una hipótesis estadística (\\(H_0\\)) que es una proposición acerca de una característica de la población de estudio. Si esa hipótesis se realiza sobre un parámetro es una hipótesis paramétrica. Habitualmente las \\(H_0\\) se enuncian bajo el supuesto de que no hay efectos, por ejemplo, de que las muestras observadas pertenecen a las poblaciones definidas en las Hipótesis Nulas, no hay diferencias estadísticas entre las muestras comparadas, correlaciones nulas, Se establece un criterio de precisión que podemos controlar a priori con la probabilidad de rechazar esa hipótesis. Siempre hay dos hipótesis: Hipótesis de partida o nula (\\(H_0\\)) que supone que el parámetro toma un valor determinado, se supone cierta y se rechazará si no es compatible con la evidencia de la muestra. Se controla a priori el error de rechazarla con el nivel de significación del contraste (p-valor). Hipótesis alternativa (\\(H_1\\)) se formula como la Ho no es cierta. Evidentemente es posible errar a la hora de realizar el contraste de hipótesis, pero se trata de controlar ese error: Se define Error de tipo I  como una probabilidad de rechazar \\(H_0\\) siendo \\(H_0\\) cierta. Probabilidad de, siendo inocente, dejar libre y se puede controlar. El error de tipo II es  y es una probabilidad de aceptar \\(H_0\\) siendo \\(H_0\\) falsa. Probabilidad de, no ser inocente y entrar en la cárcel. No se puede controlar, define la potencia del contraste de hipótesis = 1- . Todos las aplicaciones estadísticas manejan el concepto p-valor que es una probabilidad fijada por el analista, define un umbral. El contraste de hipótesis sigue una distribución asociada a la normal y va a arrojar una probabilidad, si esa probabilidad está fuera de una región donde se rechaza la \\(H_0\\) y que fija ese umbral \\(\\alpha\\) (habitualmente 0.05) entonces se podrá rechazar la \\(H_0\\) bajo los supuestos establecidos. Como se vio en el capítulo 10 la función de densidad permite calcular probabilidades: aleatorio &lt;- data.frame(dx = rnorm(1000, mean=0, sd=1)) aleatorio %&gt;% ggplot(aes(dx)) + geom_density() + geom_vline(xintercept = 1.96, linetype=&quot;dashed&quot;, color=&#39;grey&#39;, size=2) + geom_vline(xintercept = 1, color = &#39;blue&#39;) + geom_vline(xintercept = 2.4, color = &#39;red&#39;) + annotate(&quot;text&quot;, x=1, y=0.082, label= &quot;Región de aceptación&quot;, color=&#39;blue&#39;) + annotate(&quot;text&quot;, x=2.5, y=0.060, label= &quot;Región de rechazo&quot;, color=&#39;red&#39;) Precisamente en función de esa región de rechazo se pueden clasificar los contrastes de hipótesis en: El contraste bilateral sitúa la región de rechazo en los dos extremos (colas) de la distribución muestral. Lanzar un dado y contrastar que la proporción de 3 es 1/6. El contraste unilateral sitúa la región de rechazo en uno de los dos extremos (colas) de la distribución muestral. Lanzar el dado y contrastar si la proporción de 3 es &lt; 1/6. Además de la región de rechazo se pueden clasificar en función del conocimiento de la distribución de la población. Inferencia paramétrica, se conoce la distribución de la población y se cumplen todos los supuestos generales de la prueba de contraste. Inferencia no paramétrica, no se conoce la distribución y no se cumplen los supuestos Retomando ejemplos anteriores, ¿es distinta la media de la antigüedad para la respuesta de los encuestados? Es necesario aplicar el test de la \\(\\chi^2\\) chisq.test(train$Response, train$Gender) ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: train$Response and train$Gender ## X-squared = 1047.7, df = 1, p-value &lt; 2.2e-16 Como se indicó con anterioridad, el contraste suele partir desde la situación de igualdad, \\(H_0\\): Hay independencia, la respuesta al cuestionario no depende del sexo. \\(H_1\\): Hay dependencia, la respuesta depende del sexo. Este contraste arroja un p-valor de 0.0000  la probabilidad es ínfima, se sitúa dentro de esa región de rechazo. De este modo, fijado un umbral de 0.05 se rechaza la \\(H_0\\) y se rechaza que hay independencia, la respuesta depende del sexo del encuestado. Recordando el análisis bivariable con los intervalos de confianza anterior: Se aprecia que el intervalo de la proporción de respuestas positivas (de la media) no incluyen la media del otro grupo. Estos dos análisis están ligados. Reduciendo el número de observaciones artificialmente pueden variar los resultados. Se aprecia que las proporciones de ambos sexos están dentro del intervalo, el test de la \\(\\chi^2\\) queda: chisq.test(muestra$Response, muestra$Gender) ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: muestra$Response and muestra$Gender ## X-squared = 0.022353, df = 1, p-value = 0.8812 En este caso se obtiene un p-valor superior a ese umbral habitual de 0.05 se está situando en la región de aceptación de la \\(H_0\\) por lo que la respuesta no depende del sexo del encuestado. Estos ejemplos están forzados pero es importante que el científico de datos no saque conclusiones erróneas en proporciones pequeñas, un modelo de aprendizaje automático no es tan sensible a esta situación y puede ser más difícil controlarla. Por otro lado, ¿es distinta la media de la antigüedad para la respuesta de los encuestados? En el capítulo anterior se vio este gráfico. train %&gt;% ggplot(aes(x=Vintage, group=as.factor(Response), fill=as.factor(Response))) + geom_density(alpha = 0.3) Visualmente no se aprecian diferentes medias pero ese resultado no tiene ninguna validez estadística. t.test(x = train[train$Response==1,]$Vintage, y = train[train$Response==0,]$Vintage, alternative = &quot;two.sided&quot;, mu = 0, var.equal = TRUE, conf.level = 0.95) ## ## Two Sample t-test ## ## data: train[train$Response == 1, ]$Vintage and train[train$Response == 0, ]$Vintage ## t = -0.64844, df = 381107, p-value = 0.5167 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -1.0780507 0.5420556 ## sample estimates: ## mean of x mean of y ## 154.1122 154.3802 En este caso la \\(H_0\\) en condiciones de igualdad es la media de la antigüedad de los clientes que responden positivamente a la encuesta es igual a la media de la antigüedad de los clientes que responden negativamente y la \\(H_1\\) la contraria. El contraste tiene una probabilidad de 0.5 por lo que fijado un umbral de 0.05 este está muy por debajo así que no es posible rechazar la hipótesis nula, con estos datos la antigüedad como cliente no está influyendo en la respuesta al cuestionario. De hecho, se ha pedido el intervalo de confianza y se observa que el 0 estaría dentro de ese intervalo. También cabe reseñar que el contraste de diferencia de medias requiere una gran cantidad de supuestos, además el t-test está muy influido por el número de observaciones. Al igual que sucedía con el muestreo no es necesario que el científico de datos conozca todos los tipos de contrastes y de supuestos necesarios para llevar a cabo el contraste, pero ha de saber plantear una hipótesis, definir correctamente esa hipótesis, determinar que registros se emplean para contrastar esa hipótesis y como han de estar dispuestos esos registros para poder realizar el contraste. No se trata de resolver un problema mediante estadística clásica, se trata de que el científico de datos tenga esa dialéctica estadística que le va a permitir mejorar en sus procesos de modelización. "],["regresion-lineal.html", "Capítulo 13 Regresión lineal 13.1 Modelo de regresión lineal simple 13.2 El coeficiente de determinación o \\(R^2\\) 13.3 Transformaciones de variables 13.4 Tramificación de variables en modelos lineales 13.5 Factores en modelos de regresión 13.6 Modelo de regresión lineal múltiple 13.7 Métodos de selección de variables 13.8 El principio de parsimonia", " Capítulo 13 Regresión lineal En el capítulo 11 dedicado al análisis bivariable se indicó que el inicio de la relación entre dos variables era la correlación, pues la regresión lineal es el principio de la modelización estadística. Evidentemente no es lo mismo pero establecer una analogía entre ambos conceptos permite entender los objetivos de la regresión lineal: En este enlace de la recomendada web de Joaquín Amat se trata con mayor detenimiento esta relación. Como se indica en la figura ahora es una variable la que afecta a otra y es necesario crear una recta de regresión que exprese como se modifica una variable dependiente en función de otra variable independiente o regresora, si sólo hay una variable independiente se trata de un modelo de regresión lineal simple, si hay más de una variable es un modelo de regresión lineal múltiple. 13.1 Modelo de regresión lineal simple La variación de una variable afecta a otra según una función lineal por lo que será necesario crear esa función, calcular los parámetros más adecuados para esa función, decidir si esos parámetros se adecuan o no y medir si el modelo es correcto. Es decir, para plantear un modelo de regresión lineal simple es necesario seguir los siguientes pasos: Escribir el modelo matemático Estimación de los parámetros del modelo Inferencia sobre los parámetros del modelo Diagnóstico del modelo Es el modelo más sencillo ya que gráficamente se puede intuir como va a ser esa relación lineal. En este caso, no es posible seguir el ejemplo de trabajo que sirve de hilo conductor del ensayo y por ello es necesario emplear otros datos. # install.packages(&quot;skimr&quot;) library(skimr) library(tidyverse) cost_living &lt;- read.csv(&quot;./data/Cost_of_living_index.csv&quot;) skim(cost_living) Table 13.1: Data summary Name cost_living Number of rows 536 Number of columns 8 _______________________ Column type frequency: character 1 numeric 7 ________________________ Group variables None Variable type: character skim_variable n_missing complete_rate min max empty n_unique whitespace City 0 1 10 37 0 536 0 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist Rank 0 1 268.50 154.87 1.00 134.75 268.50 402.25 536.00 &lt;U+2587&gt;&lt;U+2587&gt;&lt;U+2587&gt;&lt;U+2587&gt;&lt;U+2587&gt; Cost.of.Living.Index 0 1 57.19 19.98 19.26 39.37 61.97 70.86 137.56 &lt;U+2587&gt;&lt;U+2587&gt;&lt;U+2587&gt;&lt;U+2581&gt;&lt;U+2581&gt; Rent.Index 0 1 25.21 16.45 3.43 11.69 23.09 33.44 106.49 &lt;U+2587&gt;&lt;U+2586&gt;&lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2581&gt; Cost.of.Living.Plus.Rent.Index 0 1 42.05 17.46 12.36 26.65 44.45 53.09 121.21 &lt;U+2587&gt;&lt;U+2587&gt;&lt;U+2582&gt;&lt;U+2581&gt;&lt;U+2581&gt; Groceries.Index 0 1 49.76 19.32 18.01 31.50 51.57 62.76 127.35 &lt;U+2587&gt;&lt;U+2587&gt;&lt;U+2585&gt;&lt;U+2581&gt;&lt;U+2581&gt; Restaurant.Price.Index 0 1 54.68 25.09 11.93 31.55 59.41 72.34 151.77 &lt;U+2587&gt;&lt;U+2586&gt;&lt;U+2587&gt;&lt;U+2581&gt;&lt;U+2581&gt; Local.Purchasing.Power.Index 0 1 89.49 37.04 2.24 56.79 94.10 119.50 186.00 &lt;U+2582&gt;&lt;U+2587&gt;&lt;U+2587&gt;&lt;U+2587&gt;&lt;U+2581&gt; Se trata de un conjunto de datos extraído de Kaggle que dispone de un índice del coste de la vida para 536 ciudades donde Nueva York es la base que relativiza el índice, es decir, si una ciudad tiene un valor de 120 en un dato este está un 20% por encima de Nueva York. No se realiza un análisis EDA, en su lugar se emplea la librería skim para obtener los estadísticos básicos que permitan describir las variables disponibles que se definen del siguiente modo: Rank: posición de la ciudad Cost.of.Living.Index: (excluido el alquiler) es un indicador relativo de los precios de los bienes de consumo, incluidos comestibles, restaurantes, transporte y servicios públicos. El índice de costo de vida no incluye gastos de alojamiento como alquiler o hipoteca. Rent.Index: es una estimación de los precios de alquiler de apartamentos en la ciudad en comparación con la ciudad de Nueva York. Cost.of.Livin.Plus.Rent.Index: El índice del costo de vida más el alquiler es una estimación de los precios de los bienes de consumo, incluido el alquiler, en comparación con la ciudad de Nueva York. Groceries.Index: El índice de comestibles es una estimación de los precios de los comestibles en la ciudad en comparación con la ciudad de Nueva York. Para el cálculo se usan pesos de artículos en la sección Mercados para cada ciudad. Restaurant.Price.Index: El índice de restaurantes es una comparación de precios de comidas y bebidas en restaurantes y bares en comparación con la ciudad de Nueva York. Local.Purchasing.Power.Index: muestra el poder adquisitivo relativo en la compra de bienes y servicios en una ciudad dada por el salario promedio en esa ciudad. Si el poder adquisitivo doméstico es 40, esto significa que los habitantes de esa ciudad con el salario promedio pueden permitirse comprar en promedio un 60% menos de bienes y servicios que los residentes de la ciudad de Nueva York con un salario promedio. En este ejercicio se pretende crear un modelo de regresión lineal simple que permita estimar el indicador del costo de la vida en función del precio del alquiler. Siguiendo los pasos necesarios para realizar el modelo se tiene: Modelo matemático. El modelo será \\(Y = \\beta_0 + \\beta_1X + \\epsilon\\) Esta fórmula que recuerda a la ecuación punto pendiente de una recta es el principio de la modelización estadística y tiene en pocos componentes todo lo necesario para comenzar a entender como funciona. Se desea estimar el valor de \\(Y\\) que es la variable dependiente costo de la vida por ciuidad, para estimar ese valor se crea una recta de regresión que empieza, que corta el eje desde un punto inicial \\(\\beta_0\\) y tiene una pendiente \\(\\beta_1\\) que modifica linealmente \\(X\\), la variable independiente pero no es posible que el modelo describa perfectamente la variable dependiente y por ello aparece el término error \\(\\epsilon\\). En el ejemplo de trabajo la función sería Cost.of.Living.Index = \\(\\beta_0\\) + \\(\\beta_1\\)*Rent.Index + \\(\\epsilon\\) Este modelo matemático implica que la variable dependiente se modifica en función de una variable independiente de forma aditiva, recordando temas anteriores, ¿que distribución se modificaba de forma aditiva? La distribución normal, la variable Cost.of.Living.Index ha de distribuirse normalmente. Para comprobar si una variable sigue una distribución normal se puede emplear el gráfico de densidad: cost_living %&gt;% ggplot(aes(x = Cost.of.Living.Index)) + geom_density() No se distribuye normalmente y para corroborarlo se disponen de gráficos QQ que compara los cuantiles de la distribucion normal frente a los cuantiles de la distribución de una variable. qqnorm(cost_living$Cost.of.Living.Index) qqline(cost_living$Cost.of.Living.Index) Más que evidente que no se distribuye normalmente ya que los muchos puntos de la distribución están alejados de la recta que marca los cuantiles teóricos de la distribución normal. Entonces, ¿no es posible realizar un modelo lineal?. Se calcula el coeficiente de correlación lineal entre las variables: cor(cost_living$Cost.of.Living.Index, cost_living$Rent.Index) ## [1] 0.8133299 ¿Con un coeficiente de correlación lineal superior a 0.8 no va a ser posible crear un modelo de regresión lineal? Si es posible, porque el científico de datos busca separar el azar de lo estadísticamente significativo, en su trabajo diario no va a realizar modelos teóricos ideales. Estimación de los parámetros del modelo. Los parámetros son esos elementos \\(\\beta\\) presentes en la definición del modelo. Esta labor se realiza mediante mínimos cuadrados que es el proceso de modelización estadística más sencillo y para entender como funciona se parte del gráfico de pares de puntos [Rent.Index , Cost.of.Living.Index] cost_living %&gt;% ggplot(aes(x = Rent.Index, y = Cost.of.Living.Index)) + geom_point() El método de mínimos cuadrados traza una función lineal que minimiza la distancia de todos los puntos presentes en los datos a esa función. No se entra en los matices algebraicos para la obtención de la recta de regresión ya que en R está implementado el método mediante la función lm. modelo.1 &lt;- lm(data = cost_living, formula = Cost.of.Living.Index ~ Rent.Index) Esta función es importante para conocer como se realizan los modelos en R. Evidentemente es necesario indicar los datos de entrada pero también es necesario indicar la fórmula, de ahí la importancia de conocer como será el modelo matemático. Las fórmulas siempre son de la forma variable dependiente ~ variable/s independientes, en este caso es el modelo más sencillo posible Cost.of.Living.Index ~ Rent.Index pero se puede complicar y permitir crear modelos más complejos. Para describir el modelo se emplea la función summary sobre el objeto modelo.1 creado con la función lm summary(modelo.1) ## ## Call: ## lm(formula = Cost.of.Living.Index ~ Rent.Index, data = cost_living) ## ## Residuals: ## Min 1Q Median 3Q Max ## -40.602 -7.710 -0.634 7.629 49.026 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 32.28338 0.92034 35.08 &lt;2e-16 *** ## Rent.Index 0.98788 0.03058 32.30 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 11.64 on 534 degrees of freedom ## Multiple R-squared: 0.6615, Adjusted R-squared: 0.6609 ## F-statistic: 1044 on 1 and 534 DF, p-value: &lt; 2.2e-16 Esta salida es relevante. Contiene información sobre la fórmula, los residuos y los coeficientes del modelo generado, en este caso, los parámetros estimados crear una función de regresión: \\[Cost.of.Living.Index = 32.28 + 0.99·Rent.Index + \\epsilon\\] ¿Estos parámetros son adecuados? Inferencia sobre los parámetros. En el apartado de la inferencia se parte con el test F de regresión que está en la última línea del summary, de hecho siempre se comenzará con esa última línea. Esa prueba F parte de la hipótesis nula de igualdad de medias y se obtiene un p-valor de 0 por lo que se puede rechazar la hipótesis nula, las medias son distintas, se puede dar un modelo de regresión lineal. Una vez comprobada la posibilidad de que exista un modelo de regresión la estimación de los parámetros tienen asociados una prueba t cuya \\(H_0\\) es \\(\\beta_i=0\\), es decir, el parámetro no aporta nada al modelo. Como es un modelo aditivo cuanto más próximo a 0 sea ese parámetro \\(\\beta\\) menos aporta, en el caso concreto que está ilustrando este apartado se tiene que el (intercepto), el \\(\\beta_0\\), tiene un p-valor asociado al test de 0, por lo que se rechaza la hipótesis de el parámetro no aporta al modelo, igual sucede con el parámetro asociado a Rent.Index, el \\(\\beta_1\\). Ambos parámetros están aportando al modelo pero, además, hay otro elemento en la salida que tiene importancia, el Adjusted R-squared el \\(R^2\\) que es una medida sobre la calidad del modelo que se verá más adelante. Diagnóstico del modelo. Además del \\(R^2\\) es necesario validar y diagnóstica si se cumplen todas las hipótesis del modelo lineal: Linealidad. Para estudiar esta situación en el modelo de regresión lineal simple puede servir el gráfico de puntos visto con anterioridad. En este primer ejemplo se va a emplear directamente la recta de regresión creada con el modelo. Para representar gráficamente esa recta es necesario predecir, saber que valores está arojando la recta de regresión y para ello en R está la función predict sobre el objeto modelo con la variable que participa en el modelo. estimacion.modelo.1 &lt;- predict(object=modelo.1, data=cost_living$Rent.Index) estimacion.modelo.1 &lt;- data.frame(prediccion_Cost.of.Living = estimacion.modelo.1) estimacion.modelo.1$Rent.Index = cost_living$Rent.Index head(estimacion.modelo.1) ## prediccion_Cost.of.Living Rent.Index ## 1 134.06419 103.03 ## 2 94.14415 62.62 ## 3 77.86396 46.14 ## 4 82.02291 50.35 ## 5 71.02786 39.22 ## 6 100.60485 69.16 Esta tarea de generar los datos estimados por la función matemática es escorear unos datos, es decir, escorear es obtener las estimaciones del modelo para unos datos. En el ejmeplo es aplicar la función \\(Y = 32.28 + 0.99·Rent.Index\\) a una serie de datos que permita crear un scoring o una variable predicha. En este caso se han escoreado los propios datos participantes en el modelo y permiten visualizar la recta de regresión en los gráficos de dispersión. cost_living %&gt;% ggplot(aes(x = Rent.Index, y = Cost.of.Living.Index)) + geom_point() + geom_line(data = estimacion.modelo.1, aes(x=Rent.Index, y=prediccion_Cost.of.Living), color=&quot;red&quot;) + ggtitle(&quot;Estudio de la linealidad&quot;) ¿Una recta describe esta nube de puntos? No lo parece, será necesario buscar una manera de salvar esa no linealidad. Con una sola variable independiente es sencillo comprobar la linealidad, si se tienen más variables no será tan sencillo y por eso son fundamentales los dos siguientes supuestos que se basan en los residuos del modelo de regresión. Los residuos son la distancia entre esa recta de regresión y el dato real, son la diferencia entre lo obtenido por el modelo y lo observado. Si esa distancia no es normal y si no hay independencia entre los residuos es que el modelo lineal no está describiendo el comportamiento. Por lo que los otros supuestos a tener en cuenta son: Homocedasticidad. La varianza de los residuos ha de ser 0. Normalidad de residuos. Los residuos producidos por el modelo se distribuyen normalmente, nada afecta en mayor medida a un residuo. Independencia de residuos. No existe correlación entre los residuos producidos por el modelo. Para diagnosticar los residuos se tienen los gráficos de diagnóstico de los residuos: par(mfrow = c(2, 2)) plot(modelo.1) En estos gráficos los datos deben estar en el entorno de esas líneas discontinuas sin que exista un patrón específico, no se entra en mayor profundidad porque es evidente que no se cumple, esta es una situación que suele dar lugar cuando se estudian teóricamente estos modelos. La variable dependiente no sigue una distribución normal y no se cumplen los supuestos, no hay modelo. Esta impostura teórica hace que el científico de datos huya de los modelos lineales. Pero el coeficiente de correlación es 0.8, el \\(R^2\\) es 0.66 y los parámetros son significativos, hay modelo, lo que sucede es que no está recogiendo el total del efecto lineal de la variable dependiente, el modelo es claramente mejorable. 13.2 El coeficiente de determinación o \\(R^2\\) El \\(R^2\\) ha salido en varias ocasiones en el apartado anterior, es necesario conocer como funciona y las limitaciones que tiene a la hora de medir la capacidad predictiva del modelo. El coeficiente de determinación o \\(R^2\\) es una medida de la varianza de la la variable dependiente que recoge la recta de regresión. Es un valor que va desde 0 a 1 donde 0 indica que el modelo es incapaz de medir la variabilidad de la variable dependiente y 1 significa que está recogiendo la totalidad de la variabilidad. Evidentemente, cuanto más próximo a 1 sea ese coeficiente más varianza recoge el modelo, mejor será ese modelo. El \\(R^2\\) mide lo alejadas que están las observaciones de una recta, no indica que no exista relación lineal, nubes de puntos con mucha varianza pueden arrojar coeficientes de determinación menores para rectas de regresión adecuadas y eso no implica que el modelo sea malo. El siguiente código está sacado del blog de Carlos Gil, riguroso divulgador de temas estadísticos. n &lt;- 3000 x &lt;- rnorm(n) foo &lt;- function(sigma){ y &lt;- 4 - .2 * x + rnorm(n, 0, sigma) modelo &lt;- lm(y ~ x) datos &lt;- data.frame( x = x, y = y, sigma = sigma, r.squared = summary(modelo)$r.squared) } res &lt;- do.call( rbind, lapply(seq(0, 2, length.out = 20), foo)) ggplot(res, aes(x = x, y = y)) + geom_point(alpha = .2) + geom_smooth(method = &quot;lm&quot;, col = &quot;red&quot;) + facet_wrap(~sigma) ## `geom_smooth()` using formula &#39;y ~ x&#39; r.squared &lt;- unique(res[, c(&quot;sigma&quot;, &quot;r.squared&quot;)]) plot(r.squared$sigma, r.squared$r.squared, type = &quot;l&quot;, xlab = &quot;sigma&quot;, ylab = &quot;r squared&quot;, main = &quot;r cuadrado según el error\\nirreductible del modelo&quot;) Para datos análogos el \\(R^2\\) se reduce en función de la varianza de la nube de puntos. Un \\(R^2\\) bajo no implica un mal modelo de regresión, puede implicar que la variable dependiente tenga una gran varianza. Sin embargo, un \\(R^2\\) alto si implica que el modelo es aceptable, ¿umbrales para establecer que es alto? Dependerá del analista y el problema. En el summary del modelo se tiene el Multiple R-squared y el Adjusted R-squared. El primero es el \\(R^2\\) y el segundo es el \\(R^2\\) ajustado por el número de variables presentes en el modelo. Se acostumbra a usar el ajustado por el número de variables del modelo. Se van a parecer mucho, sobre todo si se aplica el principio de parsimonia a los modelos que tendrá un apartado posterior. 13.3 Transformaciones de variables Una variable se puede transformar para mejorar un modelo de regresión lineal, se puede transformar tanto la variable respuesta como la variable independiente. Qué es transformar una variable, se ilustran ejemplos. library(gridExtra) df &lt;- data.frame(x=seq(1:1000), y=seq(1:1000)) p1 &lt;- df %&gt;% ggplot(aes(x = x, y = y)) + geom_line() + ggtitle(&quot;Sin transformación&quot;) p2 &lt;- df %&gt;% ggplot(aes(x = x, y = log(y))) + geom_line() + ggtitle(&quot;Transformación logarítmica&quot;) p3 &lt;- df %&gt;% ggplot(aes(x = x, y = sqrt(y))) + geom_line() + ggtitle(&quot;Transformación raiz&quot;) p4 &lt;- df %&gt;% ggplot(aes(x = x, y = asin(y/max(y)))) + geom_line() + ggtitle(&quot;Transformación arcoseno&quot;) grid.arrange(p1,p2,p3,p4) remove(p1, p2, p3, p4) Un dato lineal, si se transforma ya no es lineal, el científico de datos debe saber que un modelo lineal es una función lineal de su respuesta, pero no es lineal frente a sus parámetros. Puede recoger situaciones no lineales y no es necesario emplear complejos algoritmos para aislar esos comportamientos sin linealidad. Viendo los gráficos anteriores y aplicando una transformación al ejemplo de trabajo. cost_living$raiz_rent.index = sqrt(cost_living$Rent.Index) modelo.2 &lt;- lm(cost_living, formula = Cost.of.Living.Index ~ raiz_rent.index) summary(modelo.2) ## ## Call: ## lm(formula = Cost.of.Living.Index ~ raiz_rent.index, data = cost_living) ## ## Residuals: ## Min 1Q Median 3Q Max ## -31.849 -6.408 -0.871 5.294 47.585 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.1298 1.3964 3.674 0.000263 *** ## raiz_rent.index 10.9199 0.2781 39.265 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 10.15 on 534 degrees of freedom ## Multiple R-squared: 0.7427, Adjusted R-squared: 0.7423 ## F-statistic: 1542 on 1 and 534 DF, p-value: &lt; 2.2e-16 La prueba F indica que hay modelo, el \\(R^2\\) ahora se sitúa en 0.74 mejorando el dato anterior y ambos parámetros son significativos. Se realiza el scoring para pintar la recta en la nube de puntos. estimacion.modelo.2 &lt;- predict(object=modelo.2, data=cost_living$raiz_rent.index) estimacion.modelo.2 &lt;- data.frame(prediccion_Cost.of.Living = estimacion.modelo.2) estimacion.modelo.2$Rent.Index = cost_living$Rent.Index cost_living %&gt;% ggplot(aes(x = Rent.Index, y = Cost.of.Living.Index)) + geom_point() + geom_line(data = estimacion.modelo.2, aes(x=Rent.Index, y=prediccion_Cost.of.Living), color=&quot;red&quot;) + ggtitle(&quot;Estudio de la linealidad&quot;) Se aprecia que la transformación recoge ese comportamiento sin linealidad, ¿lo recoge por completo? Para ello se dispone del estudio de los residuos. par(mfrow = c(2, 2)) plot(modelo.1) El primer gráfico recoge los residuos frente al ajuste, en estimaciones superiores a un índice de 80, -20% con respecto a NYC, hay un patrón que el modelo lineal no recoge. Lo corrobora el siguiente gráfico que estudia la normalidad de los residuos, falla en ambos extremos de la estimación pero más en estimaciones superiores. El gráfido de scale - location estudia la homocedasticidad, los residuos estudentizados deberían situarse sobre una línea central para asumir igualdad de varianza y esto no sucede. El último gráfico permite estudiar si hay residuos que estén influyendo sobre los resultados del modelo, se identifican la observación 12 y 14: cost_living %&gt;% filter(row_number() %in% c(12,14)) ## Rank City Cost.of.Living.Index Rent.Index Cost.of.Living.Plus.Rent.Index ## 1 12 New York, NY, United States 100.00 100.00 100.00 ## 2 14 San Francisco, CA, United States 96.88 106.49 101.43 ## Groceries.Index Restaurant.Price.Index Local.Purchasing.Power.Index raiz_rent.index ## 1 100.00 100.00 100.00 10.0000 ## 2 101.93 94.58 125.95 10.3194 La propia Nueva York y San Francisco con un precio disparatado de los alquileres están afectando al modelo. Teóricamente el modelo no sirve porque no se cumplen las hipótesis, pero no es un mal modelo, el problema es que hay ciertas situaciones que no recoge. Pero el modelo no se puede descartar da igual lo que diga la teoría, el científico de datos tiene que separar la señal del ruido y es evidente que una simple función matemática está aislando el funcionamiento de la variable en estudio. 13.4 Tramificación de variables en modelos lineales Además de transformar una variable también es posible tramificarla para recoger mejor el comportamiento de una variable que dependa de ella. A lo largo de todo el ensayo se ha hecho mención a la importancia que tiene esta labor y los modelos lineales no son una excepción. A continuación se realiza ese ejercicio. cost_living &lt;- cost_living %&gt;% mutate(fr_Rent.Index = case_when( Rent.Index &lt;= 15 ~ &quot;1. &lt;=15&quot;, Rent.Index &lt;= 30 ~ &quot;2. 16-30&quot;, Rent.Index &lt;= 45 ~ &quot;3. 31-45&quot;, Rent.Index &lt;= 60 ~ &quot;4. 46-60&quot;, TRUE ~ &quot;5. mas de 60&quot;)) modelo.3 &lt;- lm(cost_living, formula = Cost.of.Living.Index ~ fr_Rent.Index) summary(modelo.3) ## ## Call: ## lm(formula = Cost.of.Living.Index ~ fr_Rent.Index, data = cost_living) ## ## Residuals: ## Min 1Q Median 3Q Max ## -37.544 -7.215 0.157 5.723 48.804 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 36.0118 0.8379 42.98 &lt;2e-16 *** ## fr_Rent.Index2. 16-30 24.7727 1.1784 21.02 &lt;2e-16 *** ## fr_Rent.Index3. 31-45 34.8626 1.3226 26.36 &lt;2e-16 *** ## fr_Rent.Index4. 46-60 45.3337 2.2439 20.20 &lt;2e-16 *** ## fr_Rent.Index5. mas de 60 52.7442 2.3934 22.04 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 11.21 on 531 degrees of freedom ## Multiple R-squared: 0.6877, Adjusted R-squared: 0.6854 ## F-statistic: 292.4 on 4 and 531 DF, p-value: &lt; 2.2e-16 De un modo muy rápido se ha trameado la variable respuesta en 5 tramos y el modelo ha generado 4 parámetros más el término independiente con un \\(R^2\\) de 0.68 que mejora incluso al que se obtenía con el modelo inicial. Se realiza el scoring de modelo para ver como es el modelo resultante sobre la nube de puntos. estimacion.modelo.3 &lt;- predict(object=modelo.3, data=cost_living$fr_Rent.Index) estimacion.modelo.3 &lt;- data.frame(prediccion_Cost.of.Living = estimacion.modelo.3) estimacion.modelo.3$Rent.Index = cost_living$Rent.Index cost_living %&gt;% ggplot(aes(x = Rent.Index, y = Cost.of.Living.Index)) + geom_point() + geom_line(data = estimacion.modelo.3, aes(x=Rent.Index, y=prediccion_Cost.of.Living), color=&quot;red&quot;) + ggtitle(&quot;Estudio del modelo tramificado&quot;) La mera tramificación de la variable, convertir una variable numérica en un factor, está salvando la linealidad pero se está trabajando con más de un parámetro, concretamente con 4 más el término independiente. Ya no se tiene una regresión linea simple ahora se tiene una regresión lineal múltiple y un parámetro ha pasado a crear 4, pero, ¿por qué la salida de R ofrece 4 parámetros cuando se ha tramificado la variable en 5 partes? Porque 4 parámetros son suficientes. El modelo planteado tendría la siguiente forma: \\(Y = \\beta_0 + \\beta_1*Rent.Index = 1. &lt;=15 + \\beta_2*Rent.Index = 2. 16-30 + \\beta_3*Rent.Index = 3. 31-45 + \\beta_4*Rent.Index = 4. 46-60 + \\beta_5*Rent.Index = 5. mas de 60\\) Donde cada \\(X_i\\) es una variable que toma valores 0 y 1 en función del nivel del factor que tiene cada observación. Pero la salida de R es: modelo.3$coefficients ## (Intercept) fr_Rent.Index2. 16-30 fr_Rent.Index3. 31-45 fr_Rent.Index4. 46-60 ## 36.01184 24.77269 34.86257 45.33367 ## fr_Rent.Index5. mas de 60 ## 52.74416 ¿Dónde está el nivel fr_Rent.Index1. &lt;=15? En realidad no hace falta, porque los modelos lineales que incluyen variables divididas en categorías crean variables dummy, es decir, si la observación pertenece a esa categoría toma un 1 en caso contrario 0. De ese modo, si la ciudad del conjunto de datos tiene un Rent.Index de 18 estaría en la categoría 2. 16-30 y el scoring (la predicción) para ese valor sería \\(Y=36+24.77*1=60.77\\) porque pertenece a la categoría 2 luego se multiplica por su parámetro, si pertence a la categoría 1 fr_Rent.Index1. &lt;=15 que no tiene parámetro entonces se le aplica el término independiente 36 (como aparece en el gráfico anterior). Como se ha esbozado con anterioridad, al transformar la variable a tramos, una variable en un modelo de regresión clásico es capaz de recoger efectos no lineales, pero está sacrificando algo: sencillez. Un modelo, cuantos más parámetros tenga más aumenta su complejidad y esto no es siempre positivo como se verá en capítulos posteriores. 13.5 Factores en modelos de regresión Este apartado es análogo a lo anteriormente tratado, pero se insiste en ello para que el científico de datos interprete correctamente los parámetros de un modelo de regresión. ¿Es distinto el valor del índice de costo en ciudades de EEUU, España y el resto del mundo? cost_living &lt;- cost_living %&gt;% mutate(pais = case_when( grepl(&#39;United States&#39;, City)&gt;0 ~ &#39;USA&#39;, grepl(&#39;Spain&#39;, City)&gt;0 ~ &#39;España&#39;, TRUE ~ &#39;Resto&#39;)) cost_living %&gt;% ggplot(aes(x=Cost.of.Living.Index, fill=pais, color=pais, group=pais)) + geom_density(alpha=0.3) Se crea la variable empleando la función grepl que pertenece a las funciones de las regular expressions y que sirven para la manipulación de texto, en este caso busca la existencia de patrones en cadenas de texto. Se aprecian comportamientos distintos para las distribuciones, ¿dónde se sitúan las medias? cost_living %&gt;% group_by(pais) %&gt;% summarise(media_indice = mean(Cost.of.Living.Index)) ## # A tibble: 3 x 2 ## pais media_indice ## &lt;chr&gt; &lt;dbl&gt; ## 1 España 55.5 ## 2 Resto 53.6 ## 3 USA 70.9 Con los datos disponibles, ¿son distintas las medias? ¿A qué recuerda esta cuestión? El modelo lineal ayuda a resolver estos análisis y además, los parámetros dicen mucho acerca de las variables. lm(data=cost_living, Cost.of.Living.Index~pais) ## ## Call: ## lm(formula = Cost.of.Living.Index ~ pais, data = cost_living) ## ## Coefficients: ## (Intercept) paisResto paisUSA ## 55.48 -1.89 15.42 Como se observa el término independiente es exactamente la media del índice para España, la primera en el orden lexicográfico, si a ese término independiente se le resta 1.89 se tiene la media del Resto del mundo y si a la media de España se le suma 15.42 se tiene la media del índice para los EEUU. Eso es un parámetro de la regresión lineal para el nivel de un factor y el test permite determinar si es significativo en el modelo, es decir, si esas medias son diferentes. summary(lm(data=cost_living, Cost.of.Living.Index~pais)) ## ## Call: ## lm(formula = Cost.of.Living.Index ~ pais, data = cost_living) ## ## Residuals: ## Min 1Q Median 3Q Max ## -34.332 -14.222 -2.211 13.831 83.968 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 55.482 5.416 10.245 &lt; 2e-16 *** ## paisResto -1.890 5.494 -0.344 0.73098 ## paisUSA 15.425 5.703 2.705 0.00706 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 18.76 on 533 degrees of freedom ## Multiple R-squared: 0.1221, Adjusted R-squared: 0.1188 ## F-statistic: 37.07 on 2 and 533 DF, p-value: 8.471e-16 Como se intuía la agrupación de paises del Resto del mundo no es un parámetro significativo, sin embargo, las ciudades de EEUU si tienen una media distinta estableciendo el p-valor habitual. Evidentemente el \\(R^2\\) es muy bajo, pero este ejercicio también sirve al científico de datos para crear nuevas variables a partir de las disponibles y no ser un mero ejecutor de funciones informáticas. 13.6 Modelo de regresión lineal múltiple Ya se vio en el capítulo anterior que el modelo de regresión lineal múltiple es \\(Y = \\beta_0 + X_1\\beta_0 + X_2\\beta_2 + ... + X_i\\beta_i + \\epsilon\\) con las mismas consideraciones teóricas que tiene el modelo de regresión simple: Escribir el modelo matemático Estimación de los parámetros del modelo Inferencia sobre los parámetros del modelo Diagnóstico del modelo Pero hay que añadir una nueva, la no relación lineal entre las variables independientes. Cuando esto no se produce, es decir, hay relación lineal entre las variables independientes, entonces se tiene multicolinealidad. Esto es debido a la propia solución algebraica del modelo lineal múltiple, matricialmente se define como \\(Y=\\beta X + \\epsilon\\) la estimación de los parámetros es \\(\\beta = [X^tX]^-1\\) si existe alguna relación lineal entre alguna de las variables independientes \\(X\\) entonces \\([X^tX]=0\\) y una división por 0 es un problema. Por los motivos antes expuestos uno de los primeros pasos a la hora de hacer un modelo de regresión lineal múltiple será estudiar las correlaciones. Continuando con el ejemplo anterior se plantea el mismo modelo de regresión lineal pero se van a añadir nuevas variables entre las disponibles por lo que ahora la variable Cost.of.Living.Index irá en función del resto de indicadores disponibles. El primer paso será estudiar gráficamente la relación lineal de la variable dependiente frente a las variables regresoras: grafico_puntos &lt;- function(varib){ cost_living %&gt;% ggplot(aes_string(x = varib, y = &#39;Cost.of.Living.Index&#39;)) + geom_point() + ggtitle(paste0(&quot;Índice frente a &quot;,varib))} p1 &lt;- grafico_puntos(&quot;Rent.Index&quot;) p2 &lt;- grafico_puntos(&quot;Cost.of.Living.Plus.Rent.Index&quot;) p3 &lt;- grafico_puntos(&quot;Groceries.Index&quot;) p4 &lt;- grafico_puntos(&quot;Restaurant.Price.Index&quot;) p5 &lt;- grafico_puntos(&quot;Local.Purchasing.Power.Index&quot;) grid.arrange(p1,p2,p3,p4,p5) Estos gráficos ya anticipan problemas. El índice está muy relacionado con todas las variables que se van a emplear en la regresión, no parece mala noticia, sin embargo, esa relación es muy parecida para todas las variables por lo que es imprescindible analizar si existe correlación entre las variables independientes. Para estudiar las correlaciones se dispone del coeficiente de correlación al que se hizo mención en el capítulo 11. Pero, se va a presentar una visualización que permite estudiar la correlación entre todas las variables que van a participar en el estudio, el gráfico de correlaciones. library(corrplot) ## corrplot 0.92 loaded matriz_correlaciones &lt;- cor(cost_living[,3:8]) corrplot(matriz_correlaciones, title = &quot;Gráfico de correlaciones&quot;, method = &quot;square&quot;, addCoef.col = &quot;white&quot;) Este gráfico se obtiene con la librería corrplot y sólo es necesario crear previamente la matriz de correlaciones con todas las variables. Se observa que la variable Cost.of.Living.Index tiene una correlación muy alta con muchas de las variables que van a explicar su comportamiento pero es que estas variables entre sí también tienen una alta correlación. Esto ya da pistas sobre la posible existencia de la multicolinealidad. Además, se va a prescindir de la variable Cost.of.Living.Plus.Rent.Index poruqe es el propio índice más Rent.Index y puede distorsionar el modelo. Con estas advertencias y consideraciones, se plantea el modelo: modelo.4 &lt;- lm(data=cost_living, formula=Cost.of.Living.Index ~ Rent.Index + Groceries.Index + Restaurant.Price.Index + Local.Purchasing.Power.Index) summary(modelo.4) ## ## Call: ## lm(formula = Cost.of.Living.Index ~ Rent.Index + Groceries.Index + ## Restaurant.Price.Index + Local.Purchasing.Power.Index, data = cost_living) ## ## Residuals: ## Min 1Q Median 3Q Max ## -7.0406 -2.1791 -0.1758 1.7269 11.6330 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 8.719625 0.409760 21.280 &lt; 2e-16 *** ## Rent.Index 0.006692 0.014075 0.475 0.63466 ## Groceries.Index 0.564519 0.014620 38.613 &lt; 2e-16 *** ## Restaurant.Price.Index 0.395024 0.011109 35.558 &lt; 2e-16 *** ## Local.Purchasing.Power.Index -0.015555 0.005021 -3.098 0.00205 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.023 on 531 degrees of freedom ## Multiple R-squared: 0.9773, Adjusted R-squared: 0.9771 ## F-statistic: 5711 on 4 and 531 DF, p-value: &lt; 2.2e-16 Se dispone de un modelo con un excepcional \\(R^2\\) donde la variable Rent.Index es la única que no supera el test de \\(\\beta_i=0\\) algo que lo indica el propio valor del parámetro, muy próximo a 0. El resto de variables si superan el test. Con estas cosideraciones es necesario replantear el modelo: modelo.5 &lt;- lm(data=cost_living, formula=Cost.of.Living.Index ~ Groceries.Index + Restaurant.Price.Index + Local.Purchasing.Power.Index) summary(modelo.5) ## ## Call: ## lm(formula = Cost.of.Living.Index ~ Groceries.Index + Restaurant.Price.Index + ## Local.Purchasing.Power.Index, data = cost_living) ## ## Residuals: ## Min 1Q Median 3Q Max ## -7.0382 -2.0809 -0.1563 1.7731 11.8113 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 8.654063 0.385580 22.444 &lt; 2e-16 *** ## Groceries.Index 0.567363 0.013330 42.562 &lt; 2e-16 *** ## Restaurant.Price.Index 0.396296 0.010774 36.782 &lt; 2e-16 *** ## Local.Purchasing.Power.Index -0.015296 0.004987 -3.067 0.00227 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.021 on 532 degrees of freedom ## Multiple R-squared: 0.9773, Adjusted R-squared: 0.9771 ## F-statistic: 7626 on 3 and 532 DF, p-value: &lt; 2.2e-16 Ya se dispone de un modelo con todos los parámetros significativos, aunque Local.Purchasing.Power.Index no sería significativa si se fija un umbral más bajo de 0.002 ya que quedaría fuera de la región de aceptación. Con el primer modelo y teniendo en cuenta el anterior estudio de la correlación se torna necesario analizar la posible presencia de multicolinealidad. Hay diversos métodos para realizar esta tarea y se opta por ilustrar el ejemplo con el método VIF (Variance Inflation Factor). Si hay multicolinealidad \\([X^tX]=0\\) está hinflando la varianza, ¿cuánto hinfla la varianza una variable dentro del modelo? Para determinar como está afectando se va a utilizar la librería de R car library(car) vif(modelo.5) ## Groceries.Index Restaurant.Price.Index Local.Purchasing.Power.Index ## 3.887894 4.283852 2.000066 La función vif calcula cuanto está hinflando la varianza del modelo cada variable, valores por encima de 8 indican un problema, valores por encima de 4 indican la necesidad de analizar las variables en el modelo. En este modelo sólo Restaurant.Price.Index está causando problemas, está en manos del científico de datos eliminar la variable del modelo o transformarla para evitar problemas pero, como siempre, tiene que argumentar su eliminación. En este caso se opta por dejar el modelo.5 como modelo definitivo y por último es necesario estudiar el comportamiento de los residuos. par(mfrow = c(2, 2)) plot(modelo.5) Desde el punto de vista teórico se dispone de un modelo aceptable, hay linealidad, pero los residuos pierden normalidad y pierden homogeneidad de varianza en valores altos. Se pueden identificar los registros, las ciudades que están causando problemas en el modelo. estimacion.modelo.5 &lt;- predict(object=modelo.5, data=cost_living) estimacion.modelo.5 &lt;- cost_living %&gt;% mutate(estimacion = estimacion.modelo.5) estimacion.modelo.5 %&gt;% filter(estimacion&gt;110) %&gt;% select(City, Cost.of.Living.Index, estimacion, Groceries.Index, Restaurant.Price.Index, Local.Purchasing.Power.Index) ## City Cost.of.Living.Index estimacion Groceries.Index Restaurant.Price.Index ## 1 Hamilton, Bermuda 137.56 138.8588 126.56 151.77 ## 2 Zurich, Switzerland 128.65 129.1149 127.35 127.14 ## 3 Basel, Switzerland 126.89 125.9851 120.44 129.10 ## 4 Lausanne, Switzerland 119.62 121.3159 116.35 122.83 ## 5 Bern, Switzerland 118.42 117.3921 114.54 114.86 ## 6 Geneva, Switzerland 118.33 117.6710 112.08 119.78 ## 7 Reykjavik, Iceland 112.57 113.7897 98.53 127.87 ## 8 Lugano, Switzerland 111.88 113.9356 104.85 120.62 ## 9 Stavanger, Norway 111.00 114.7029 95.97 134.55 ## Local.Purchasing.Power.Index ## 1 114.19 ## 2 142.39 ## 3 141.48 ## 4 132.58 ## 5 115.48 ## 6 133.47 ## 7 94.21 ## 8 131.26 ## 9 112.62 Muchas ciudades suizas están causando problemas en el modelo, parece que la estimación siempre está por encima probablemente debido al alto coste de la alimentación. En este caso el análisis de los residuos está ofreciendo un comportamiento interesante en los datos y por este motivo el científico de datos debe estudiar esta diferencia entre lo estimado y lo real porque las observaciones que no ajustan correctamente también pueden ofrecer información al análisis. 13.7 Métodos de selección de variables En el apartado anterior se han introducido los modelos de regresión con múltiples variables, la selección de éstas se ha llevado a cabo en base a los criterios del analista. Se recomienda que el científico de datos participe en todo el proceso de modelización pero existen situaciones en las que se disponen de multitud de variables y ese análisis pormenorizado puede convertirse en ardua tarea. Por ello, es necesario conocer los métodos automáticos de selección de variables. En los modelos de regresión se plantean 3 formas de seleccionar variables, el método fordward, método backward y método stepwise. En este ensayo se van a ilustrar los tres métodos basando la capacidad predictiva de cada método en el critero de información de Akaike conocido como AIC. Al igual que el \\(R^2\\) el AIC es una medida de lo correcto que es el ajuste pero está ponderado por el número de parámetros del modelo, sin entrar en aspectos teóricos cuanto menor es el AIC mejor es el modelo y si una variable no disminuye el AIC, no lo mejora sustancialmente, esta variable será prescindible. 13.7.1 Método fordward Se parte del modelo más sencillo posible y a éste se le irán introduciendo variables que vayan mejorando el AIC. La fórmula del modelo más sencillo posible será: inicio &lt;- formula(Cost.of.Living.Index~1) La fórmula del modelo más completo posible será: final &lt;- formula(Cost.of.Living.Index ~ Rent.Index + Groceries.Index + Restaurant.Price.Index + Local.Purchasing.Power.Index) Partiendo del inicio paso a paso se llegará al modelo seleccionado mediante la dirección fordward. Para realizar esta tarea se emplea la librería MASS. library(MASS) modelo_mas_sencillo &lt;- lm(inicio , data = cost_living) modelo_forward&lt;- stepAIC(modelo_mas_sencillo, direction = &quot;forward&quot;, trace = T, scope = final) ## Start: AIC=3211.62 ## Cost.of.Living.Index ~ 1 ## ## Df Sum of Sq RSS AIC ## + Groceries.Index 1 195537 18141 1891.7 ## + Restaurant.Price.Index 1 192141 21537 1983.7 ## + Rent.Index 1 141349 72329 2633.0 ## + Local.Purchasing.Power.Index 1 98756 114921 2881.2 ## &lt;none&gt; 213677 3211.6 ## ## Step: AIC=1891.68 ## Cost.of.Living.Index ~ Groceries.Index ## ## Df Sum of Sq RSS AIC ## + Restaurant.Price.Index 1 13199.0 4941.9 1196.7 ## + Rent.Index 1 1130.8 17010.2 1859.2 ## + Local.Purchasing.Power.Index 1 935.6 17205.4 1865.3 ## &lt;none&gt; 18140.9 1891.7 ## ## Step: AIC=1196.66 ## Cost.of.Living.Index ~ Groceries.Index + Restaurant.Price.Index ## ## Df Sum of Sq RSS AIC ## + Local.Purchasing.Power.Index 1 85.853 4856.1 1189.3 ## &lt;none&gt; 4941.9 1196.7 ## + Rent.Index 1 0.180 4941.7 1198.6 ## ## Step: AIC=1189.26 ## Cost.of.Living.Index ~ Groceries.Index + Restaurant.Price.Index + ## Local.Purchasing.Power.Index ## ## Df Sum of Sq RSS AIC ## &lt;none&gt; 4856.1 1189.3 ## + Rent.Index 1 2.0664 4854.0 1191.0 Con trace = T se indica que se puedan ver los pasos seguidos en el proceso, inicialmente entra la variable Groceries.Index, seguida de Restaurant.Price.Index que reduce lo suficiente el AIC y por último Local.Purchasing.Power.Index con una reducción del AIC mucho menor que la anterior pero el modelo cada vez era más completo. El modelo resultante es análogo al planteado de forma manual con anterioridad. summary(modelo_forward) ## ## Call: ## lm(formula = Cost.of.Living.Index ~ Groceries.Index + Restaurant.Price.Index + ## Local.Purchasing.Power.Index, data = cost_living) ## ## Residuals: ## Min 1Q Median 3Q Max ## -7.0382 -2.0809 -0.1563 1.7731 11.8113 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 8.654063 0.385580 22.444 &lt; 2e-16 *** ## Groceries.Index 0.567363 0.013330 42.562 &lt; 2e-16 *** ## Restaurant.Price.Index 0.396296 0.010774 36.782 &lt; 2e-16 *** ## Local.Purchasing.Power.Index -0.015296 0.004987 -3.067 0.00227 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.021 on 532 degrees of freedom ## Multiple R-squared: 0.9773, Adjusted R-squared: 0.9771 ## F-statistic: 7626 on 3 and 532 DF, p-value: &lt; 2.2e-16 13.7.2 Método backward El sentido contrario al método fordward, se parte del modelo completo y se determina que variable es candidata a salir porque el AIC del modelo no se ve alterado. modelo_completo &lt;- lm(data=cost_living, formula = final) modelo_backward&lt;- stepAIC(modelo_completo, direction = &quot;backward&quot;, trace = T, scope = final) ## Start: AIC=1191.04 ## Cost.of.Living.Index ~ Rent.Index + Groceries.Index + Restaurant.Price.Index + ## Local.Purchasing.Power.Index ## ## Df Sum of Sq RSS AIC ## - Rent.Index 1 2.1 4856.1 1189.3 ## &lt;none&gt; 4854.0 1191.0 ## - Local.Purchasing.Power.Index 1 87.7 4941.7 1198.6 ## - Restaurant.Price.Index 1 11557.9 16411.9 1842.0 ## - Groceries.Index 1 13629.4 18483.5 1905.7 ## ## Step: AIC=1189.26 ## Cost.of.Living.Index ~ Groceries.Index + Restaurant.Price.Index + ## Local.Purchasing.Power.Index ## ## Df Sum of Sq RSS AIC ## &lt;none&gt; 4856.1 1189.3 ## - Local.Purchasing.Power.Index 1 85.9 4941.9 1196.7 ## - Restaurant.Price.Index 1 12349.3 17205.4 1865.3 ## - Groceries.Index 1 16535.5 21391.5 1982.0 Se aprecia que el AIC sin la variable Rent.Index se queda en 1189.3 por lo que es candidata a ser eliminada. En el segundo paso ninguna variable es candidata a salir por lo que se paran las iteraciones y se crea un modelo final igual a los obtenidos con anterioridad. 13.7.3 Método stepwise Este método de selección de variables es una combinación de los dos anteriores, se parte del modelo y se evalúa que variable es candidata a salir y de las variables eliminadas se vuelve a evaluar si es candidata a entrar en el modelo. modelo_combinado &lt;- stepAIC(modelo_completo, trace=T, direction=&quot;both&quot;, scope=final) ## Start: AIC=1191.04 ## Cost.of.Living.Index ~ Rent.Index + Groceries.Index + Restaurant.Price.Index + ## Local.Purchasing.Power.Index ## ## Df Sum of Sq RSS AIC ## - Rent.Index 1 2.1 4856.1 1189.3 ## &lt;none&gt; 4854.0 1191.0 ## - Local.Purchasing.Power.Index 1 87.7 4941.7 1198.6 ## - Restaurant.Price.Index 1 11557.9 16411.9 1842.0 ## - Groceries.Index 1 13629.4 18483.5 1905.7 ## ## Step: AIC=1189.26 ## Cost.of.Living.Index ~ Groceries.Index + Restaurant.Price.Index + ## Local.Purchasing.Power.Index ## ## Df Sum of Sq RSS AIC ## &lt;none&gt; 4856.1 1189.3 ## + Rent.Index 1 2.1 4854.0 1191.0 ## - Local.Purchasing.Power.Index 1 85.9 4941.9 1196.7 ## - Restaurant.Price.Index 1 12349.3 17205.4 1865.3 ## - Groceries.Index 1 16535.5 21391.5 1982.0 El primer paso es igual al empleado en el modelo backward saliendo Rent.Index pero no hay más candidatas a salir o a entrar por lo que se llega al mismo resultado. Se recomienda que el científico de datos genere sus propias herramientas para la selección argumentada de variables, pero es necesario conocer estos métodos porque se presentarán situaciones en las que se tenga cientos de variables regresoras y una selección automática de variables puede ser el primer paso para elegir las variables presentes en el modelo. En cualquier caso, se recomienda no incluir un gran número de variables en los modelos de regresión. 13.8 El principio de parsimonia El principio de parsimonia en los modelos de regresión consiste en buscar modelos con el menor número posible de parámetros ya que la presencia de múltiples parámetros puede hacer que existan relaciones lineales debidas al azar. Para ilustrar esta situación se realiza una simulación. set.seed(12) df &lt;- data.frame(dependiente = rnorm(1000,100,10)) #Bucle para añadir 300 variables for (i in 1:300){ x &lt;- data.frame(rnorm(1000,sample(1:100,1),sample(1:50,1))) nombre = paste(&quot;x&quot;,i,sep=&quot;&quot;) names(x) = nombre df=cbind(df,x) remove(x)} #1.000 observaciones y 300 variables modelo_aleatorio=lm(dependiente~. ,data=df) summary(modelo_aleatorio) ## ## Call: ## lm(formula = dependiente ~ ., data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -22.9556 -5.0359 0.0116 5.1136 26.6006 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.127e+02 6.000e+01 1.879 0.06063 . ## x1 1.617e-01 1.140e-01 1.418 0.15658 ## x2 2.533e-02 1.599e-02 1.585 0.11352 ## x3 1.157e-02 7.660e-03 1.510 0.13155 ## x4 -6.777e-03 1.165e-02 -0.582 0.56081 ## x5 3.924e-03 1.014e-02 0.387 0.69885 ## x6 2.436e-03 1.492e-02 0.163 0.87038 ## x7 5.526e-03 7.496e-03 0.737 0.46127 ## x8 7.716e-04 9.193e-03 0.084 0.93313 ## x9 -1.173e-03 1.031e-02 -0.114 0.90945 ## x10 1.371e-02 1.017e-02 1.348 0.17824 ## x11 -1.603e-02 1.409e-02 -1.138 0.25570 ## x12 -1.208e-02 8.324e-03 -1.451 0.14724 ## x13 1.703e-02 9.204e-03 1.851 0.06462 . ## x14 -1.989e-02 1.337e-02 -1.488 0.13710 ## x15 -9.550e-04 7.052e-03 -0.135 0.89231 ## x16 -2.693e-02 2.593e-02 -1.039 0.29932 ## x17 4.742e-02 1.185e-01 0.400 0.68915 ## x18 -5.601e-03 1.587e-02 -0.353 0.72427 ## x19 2.195e-03 1.097e-02 0.200 0.84141 ## x20 3.440e-02 2.699e-02 1.275 0.20282 ## x21 3.899e-03 1.224e-02 0.319 0.75006 ## x22 5.159e-02 8.891e-02 0.580 0.56190 ## x23 5.733e-03 1.753e-02 0.327 0.74370 ## x24 6.425e-02 6.016e-02 1.068 0.28587 ## x25 -5.300e-03 7.852e-03 -0.675 0.49992 ## x26 -7.158e-04 7.187e-03 -0.100 0.92069 ## x27 -9.946e-03 1.042e-02 -0.954 0.34024 ## x28 -2.756e-02 1.807e-02 -1.525 0.12761 ## x29 -2.350e-03 8.929e-03 -0.263 0.79247 ## x30 -2.583e-03 1.011e-02 -0.256 0.79834 ## x31 9.562e-03 1.251e-02 0.765 0.44482 ## x32 -8.080e-03 7.726e-03 -1.046 0.29601 ## x33 1.161e-01 3.527e-01 0.329 0.74208 ## x34 6.276e-03 1.063e-02 0.590 0.55508 ## x35 -5.357e-03 6.736e-03 -0.795 0.42674 ## x36 -4.659e-03 1.800e-02 -0.259 0.79585 ## x37 -1.339e-01 8.949e-02 -1.497 0.13497 ## x38 -3.937e-02 1.972e-02 -1.996 0.04631 * ## x39 1.781e-02 1.277e-02 1.395 0.16354 ## x40 1.066e-03 8.341e-03 0.128 0.89830 ## x41 2.876e-02 1.458e-02 1.972 0.04900 * ## x42 4.812e-02 3.828e-02 1.257 0.20912 ## x43 -2.316e-02 1.077e-02 -2.150 0.03192 * ## x44 -8.094e-04 1.177e-02 -0.069 0.94518 ## x45 3.058e-02 1.878e-02 1.628 0.10387 ## x46 -1.788e-04 6.887e-03 -0.026 0.97930 ## x47 -3.372e-02 1.566e-02 -2.154 0.03158 * ## x48 -2.085e-03 1.029e-02 -0.203 0.83951 ## x49 -3.773e-03 1.464e-02 -0.258 0.79666 ## x50 -3.924e-02 3.404e-02 -1.153 0.24950 ## x51 -3.235e-01 3.466e-01 -0.933 0.35095 ## x52 -1.816e-02 3.199e-02 -0.568 0.57049 ## x53 7.356e-02 1.731e-01 0.425 0.67090 ## x54 -6.965e-03 1.157e-02 -0.602 0.54753 ## x55 -1.442e-02 1.230e-02 -1.173 0.24138 ## x56 -7.507e-03 1.003e-02 -0.749 0.45428 ## x57 -1.827e-03 1.423e-02 -0.128 0.89790 ## x58 5.359e-03 1.543e-02 0.347 0.72842 ## x59 2.935e-02 2.190e-02 1.340 0.18065 ## x60 2.124e-02 8.941e-02 0.238 0.81227 ## x61 1.738e-02 1.171e-02 1.485 0.13799 ## x62 6.504e-04 3.968e-02 0.016 0.98693 ## x63 2.030e-02 2.706e-02 0.750 0.45342 ## x64 3.608e-03 3.242e-02 0.111 0.91140 ## x65 -9.166e-03 1.042e-02 -0.880 0.37926 ## x66 7.765e-03 1.118e-02 0.694 0.48762 ## x67 1.698e-02 6.081e-02 0.279 0.78019 ## x68 -1.167e-02 8.074e-03 -1.445 0.14891 ## x69 8.392e-03 1.330e-02 0.631 0.52818 ## x70 1.680e-02 1.594e-02 1.054 0.29206 ## x71 -6.032e-03 8.569e-03 -0.704 0.48170 ## x72 2.703e-03 2.122e-02 0.127 0.89866 ## x73 3.115e-02 1.166e-01 0.267 0.78937 ## x74 -9.760e-02 1.220e-01 -0.800 0.42401 ## x75 3.263e-02 4.182e-02 0.780 0.43555 ## x76 6.154e-03 1.127e-02 0.546 0.58521 ## x77 1.390e-01 8.613e-02 1.614 0.10704 ## x78 2.987e-03 6.852e-02 0.044 0.96524 ## x79 -5.741e-03 2.651e-02 -0.217 0.82862 ## x80 5.165e-04 9.710e-03 0.053 0.95759 ## x81 5.921e-02 5.732e-02 1.033 0.30196 ## x82 6.705e-02 3.008e-02 2.229 0.02613 * ## x83 -1.231e-02 1.914e-02 -0.643 0.52048 ## x84 5.774e-03 2.257e-02 0.256 0.79815 ## x85 2.292e-02 1.928e-02 1.189 0.23490 ## x86 -5.571e-04 7.719e-03 -0.072 0.94249 ## x87 -9.378e-02 8.936e-02 -1.049 0.29434 ## x88 -2.562e-02 4.077e-02 -0.628 0.52992 ## x89 -3.989e-02 1.512e-02 -2.638 0.00852 ** ## x90 1.253e-02 1.246e-02 1.006 0.31494 ## x91 -4.999e-04 5.217e-02 -0.010 0.99236 ## x92 2.440e-01 3.576e-01 0.682 0.49530 ## x93 4.566e-03 7.787e-03 0.586 0.55784 ## x94 9.503e-03 5.144e-02 0.185 0.85349 ## x95 -1.318e-01 1.185e-01 -1.113 0.26628 ## x96 3.203e-02 1.993e-02 1.607 0.10857 ## x97 1.063e-03 7.774e-03 0.137 0.89130 ## x98 -4.629e-04 1.500e-02 -0.031 0.97539 ## x99 9.745e-03 1.388e-02 0.702 0.48300 ## x100 1.563e-02 1.643e-02 0.952 0.34162 ## x101 8.165e-04 9.084e-03 0.090 0.92841 ## x102 4.020e-02 3.075e-02 1.307 0.19151 ## x103 2.966e-01 1.133e-01 2.618 0.00905 ** ## x104 2.039e-03 1.850e-02 0.110 0.91228 ## x105 2.170e-03 1.566e-02 0.139 0.88983 ## x106 -5.072e-03 1.074e-02 -0.472 0.63697 ## x107 8.551e-03 8.710e-03 0.982 0.32657 ## x108 -1.984e-02 1.364e-02 -1.455 0.14619 ## x109 6.124e-03 6.775e-03 0.904 0.36634 ## x110 2.733e-01 1.760e-01 1.553 0.12092 ## x111 1.094e-01 1.761e-01 0.621 0.53473 ## x112 -1.090e-02 2.959e-02 -0.368 0.71277 ## x113 1.104e-02 1.750e-02 0.631 0.52838 ## x114 -6.011e-03 7.543e-03 -0.797 0.42578 ## x115 1.063e-01 6.686e-02 1.590 0.11234 ## x116 2.041e-02 3.517e-02 0.580 0.56188 ## x117 1.119e-02 2.062e-02 0.543 0.58762 ## x118 -5.774e-03 1.350e-02 -0.428 0.66909 ## x119 7.996e-03 1.057e-02 0.756 0.44973 ## x120 -1.451e-02 7.889e-03 -1.839 0.06632 . ## x121 -1.739e-02 1.163e-02 -1.496 0.13515 ## x122 -1.323e-02 1.851e-02 -0.715 0.47490 ## x123 5.777e-03 7.882e-03 0.733 0.46385 ## x124 3.216e-01 1.188e-01 2.707 0.00695 ** ## x125 -1.227e-01 3.393e-01 -0.362 0.71776 ## x126 2.070e-03 6.890e-03 0.300 0.76389 ## x127 -1.072e-03 1.260e-02 -0.085 0.93225 ## x128 9.925e-03 2.078e-02 0.478 0.63302 ## x129 1.452e-02 9.752e-03 1.489 0.13704 ## x130 2.126e-02 1.054e-02 2.016 0.04413 * ## x131 8.401e-03 7.193e-03 1.168 0.24324 ## x132 -1.871e-02 4.281e-02 -0.437 0.66227 ## x133 -2.772e-02 1.050e-02 -2.640 0.00848 ** ## x134 -6.692e-03 1.347e-02 -0.497 0.61958 ## x135 -2.215e-03 8.706e-02 -0.025 0.97971 ## x136 -1.108e-02 7.360e-03 -1.505 0.13277 ## x137 2.064e-02 5.018e-02 0.411 0.68094 ## x138 -2.459e-02 1.572e-02 -1.564 0.11819 ## x139 3.443e-02 2.268e-02 1.518 0.12943 ## x140 5.536e-03 1.541e-02 0.359 0.71948 ## x141 8.656e-03 3.623e-02 0.239 0.81124 ## x142 8.796e-03 7.378e-03 1.192 0.23358 ## x143 5.975e-03 9.927e-03 0.602 0.54745 ## x144 -4.070e-04 7.687e-03 -0.053 0.95779 ## x145 1.985e-02 8.504e-03 2.334 0.01987 * ## x146 -1.128e-02 1.921e-02 -0.587 0.55742 ## x147 -1.433e-02 8.997e-03 -1.592 0.11177 ## x148 -4.362e-02 2.623e-02 -1.663 0.09673 . ## x149 -1.800e-03 1.561e-02 -0.115 0.90823 ## x150 -1.129e-02 8.816e-03 -1.281 0.20070 ## x151 9.841e-03 5.936e-02 0.166 0.86838 ## x152 -1.358e-03 7.726e-03 -0.176 0.86049 ## x153 9.585e-03 1.059e-02 0.905 0.36593 ## x154 6.362e-03 1.345e-02 0.473 0.63632 ## x155 7.517e-03 2.423e-02 0.310 0.75643 ## x156 -4.604e-01 1.799e-01 -2.559 0.01070 * ## x157 -2.324e-02 5.197e-02 -0.447 0.65479 ## x158 -1.147e-02 9.579e-03 -1.198 0.23140 ## x159 6.224e-03 1.501e-02 0.415 0.67853 ## x160 -1.733e-02 3.274e-02 -0.529 0.59679 ## x161 -1.334e-02 7.893e-03 -1.690 0.09153 . ## x162 -6.798e-03 1.848e-02 -0.368 0.71305 ## x163 -9.021e-03 1.403e-02 -0.643 0.52057 ## x164 4.304e-02 1.886e-02 2.282 0.02278 * ## x165 1.224e-02 2.585e-02 0.473 0.63604 ## x166 4.604e-03 1.052e-02 0.438 0.66162 ## x167 -1.394e-01 8.980e-02 -1.552 0.12104 ## x168 -9.716e-03 2.552e-02 -0.381 0.70357 ## x169 -2.387e-02 5.955e-02 -0.401 0.68869 ## x170 1.027e-02 7.272e-03 1.412 0.15826 ## x171 -5.711e-04 8.418e-03 -0.068 0.94593 ## x172 1.678e-02 1.155e-02 1.454 0.14651 ## x173 -1.182e-02 1.078e-02 -1.096 0.27325 ## x174 1.260e-02 1.770e-02 0.712 0.47690 ## x175 1.277e-03 7.903e-03 0.162 0.87171 ## x176 -4.963e-03 1.408e-02 -0.353 0.72455 ## x177 8.829e-03 8.197e-03 1.077 0.28181 ## x178 -1.005e-01 1.195e-01 -0.841 0.40046 ## x179 -2.138e-02 1.062e-02 -2.014 0.04443 * ## x180 -1.185e-01 4.881e-02 -2.429 0.01541 * ## x181 8.573e-03 9.325e-03 0.919 0.35821 ## x182 4.836e-02 1.173e-01 0.412 0.68030 ## x183 6.555e-02 4.944e-02 1.326 0.18533 ## x184 -1.086e-02 1.601e-02 -0.678 0.49770 ## x185 -1.984e-03 3.176e-02 -0.062 0.95021 ## x186 5.725e-04 7.369e-03 0.078 0.93810 ## x187 2.995e-02 3.433e-02 0.872 0.38336 ## x188 5.323e-03 8.445e-03 0.630 0.52872 ## x189 -1.876e-02 9.643e-03 -1.945 0.05220 . ## x190 -6.278e-03 1.293e-02 -0.486 0.62733 ## x191 5.416e-02 4.423e-02 1.224 0.22123 ## x192 -6.202e-03 7.709e-03 -0.804 0.42139 ## x193 -2.757e-04 1.158e-02 -0.024 0.98101 ## x194 -3.940e-02 1.710e-02 -2.304 0.02153 * ## x195 -1.317e-02 1.254e-02 -1.051 0.29383 ## x196 -4.070e-02 1.944e-02 -2.093 0.03667 * ## x197 3.400e-02 8.926e-02 0.381 0.70342 ## x198 -2.145e-03 1.492e-02 -0.144 0.88575 ## x199 3.831e-03 1.175e-02 0.326 0.74444 ## [ reached getOption(&quot;max.print&quot;) -- omitted 101 rows ] ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 9.369 on 699 degrees of freedom ## Multiple R-squared: 0.3326, Adjusted R-squared: 0.04621 ## F-statistic: 1.161 on 300 and 699 DF, p-value: 0.05947 Partiendo de datos completamente aleatorios el test F está muy próximo a 0.05 por lo que existe modelo y por si fuera poco hay variables significativas, esto sucede porque hay un gran número de parámetros y la regresión lineal no maneja bien esta situación, por este motivo, no se recomienda realizar modelos de regresión lineal con más de 10 variables independientes ya que pueden aparecer relaciones debidas al puro azar. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
